id,title,selftext,created_utc,score,num_comments,subreddit,type,text
1jpdo7y,[D] Self-Promotion Thread,"Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads.",2025-04-02 04:15:32,4,27,MachineLearning,post,"[D] Self-Promotion Thread
Please post your personal projects, startups, product placements, collaboration needs, blogs etc.

Please mention the payment and pricing requirements for products and services.

Please do not post link shorteners, link aggregator websites , or auto-subscribe links.

\--

Any abuse of trust will lead to bans.

Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

\--

Meta: This is an experiment. If the community doesnt like this, we will cancel it. This is to encourage those in the community to promote their work by not spamming the main threads."
1jnt4sp,[D] Monthly Who's Hiring and Who wants to be Hired?,"**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience.",2025-03-31 04:30:37,14,4,MachineLearning,post,"[D] Monthly Who's Hiring and Who wants to be Hired?
**For Job Postings** please use this template

>Hiring: \[Location\], Salary:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]    and \[Brief overview, what you're looking for\]

**For Those looking for jobs** please use this template

>Want to be Hired: \[Location\], Salary Expectation:\[\], \[Remote | Relocation\], \[Full Time | Contract | Part Time\]  Resume: \[Link to resume\] and \[Brief overview, what you're looking for\]

&#x200B;

Please remember that this community is geared towards those with experience."
1k0npdk,[R] Beyond-NanoGPT: Go From LLM Noob to AI Researcher!,"Hi all! 

I spent the last few weeks writing a repo that aims to help people go from **nanoGPT-level** understanding of LLM basics to be able to reason about and implement relatively sophisticated ideas **near the deep learning research frontier.** It's called [beyond-nanoGPT](https://github.com/tanishqkumar/beyond-nanogpt), and *I just open sourced it!*

It contains thousands of lines of annotated, from-scratch pytorch **implementing everything from speculative decoding to vision/diffusion transformers to linear and sparse attention, and lots more.** 

I would love to hear feedback from the ML community here since many are interested both in research-level ML ideas and in helping others learn ML. Feedback might range from key research papers I should add implementations for, any bugs spotted, or just things people want to see -- and anything else people have to say!   
  
The goal is to help convert as many nanoGPT-watchers into full-time AI researchers by getting them comfortable with fundamental modern ML research advances :)  ",2025-04-16 17:48:55,55,12,MachineLearning,post,"[R] Beyond-NanoGPT: Go From LLM Noob to AI Researcher!
Hi all! 

I spent the last few weeks writing a repo that aims to help people go from **nanoGPT-level** understanding of LLM basics to be able to reason about and implement relatively sophisticated ideas **near the deep learning research frontier.** It's called [beyond-nanoGPT](https://github.com/tanishqkumar/beyond-nanogpt), and *I just open sourced it!*

It contains thousands of lines of annotated, from-scratch pytorch **implementing everything from speculative decoding to vision/diffusion transformers to linear and sparse attention, and lots more.** 

I would love to hear feedback from the ML community here since many are interested both in research-level ML ideas and in helping others learn ML. Feedback might range from key research papers I should add implementations for, any bugs spotted, or just things people want to see -- and anything else people have to say!   
  
The goal is to help convert as many nanoGPT-watchers into full-time AI researchers by getting them comfortable with fundamental modern ML research advances :)  "
1k0fg57,[D] Google just released a new generation of TPUs. Who actually uses TPUs in production?,"Google recently their new generation of TPUs optimized for inference: [https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/)

Google TPUs have been around for quite some time now, and I've rarely seen any company seriously use them in production...

At NLP Cloud we used TPUs at some point behind our training and fine-tuning platform. But they were tricky to set up and not necessarily faster than NVIDIA GPUs.

We also worked on a POC for TPU-based inference, but it was a failure because GCP lacked many must-have features on their TPU platform: no fixed IP address, no serious observability tools, slow TPU instance provisioning process, XLA being sometimes hard to debug...

Researchers may be interested in TPUs but is it because of TPUs themselves or because of the generous Google TRC program ( [https://sites.research.google/trc](https://sites.research.google/trc) ) that gives access to a bunch of free TPUs?

Also, the fact that Google TPUs cannot be purchased but only rented through the GCP platform might scare many organizations trying to avoid vendor lock-in.

Maybe this new generation of TPUs is different and GCP has matured the TPU ecosystem on GCP?

If some of you have experience using TPUs in production, I'd love to hear your story 🙂",2025-04-16 10:29:52,101,44,MachineLearning,post,"[D] Google just released a new generation of TPUs. Who actually uses TPUs in production?
Google recently their new generation of TPUs optimized for inference: [https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/](https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/)

Google TPUs have been around for quite some time now, and I've rarely seen any company seriously use them in production...

At NLP Cloud we used TPUs at some point behind our training and fine-tuning platform. But they were tricky to set up and not necessarily faster than NVIDIA GPUs.

We also worked on a POC for TPU-based inference, but it was a failure because GCP lacked many must-have features on their TPU platform: no fixed IP address, no serious observability tools, slow TPU instance provisioning process, XLA being sometimes hard to debug...

Researchers may be interested in TPUs but is it because of TPUs themselves or because of the generous Google TRC program ( [https://sites.research.google/trc](https://sites.research.google/trc) ) that gives access to a bunch of free TPUs?

Also, the fact that Google TPUs cannot be purchased but only rented through the GCP platform might scare many organizations trying to avoid vendor lock-in.

Maybe this new generation of TPUs is different and GCP has matured the TPU ecosystem on GCP?

If some of you have experience using TPUs in production, I'd love to hear your story 🙂"
1k0yafe,"[P] Today, to give back to the open source community, I release my first paper- a novel attention mechanism, Context-Aggregated Linear Attention, or CALA.","So, it's still a work in progress, but I don't have the compute to work on it right now to do empirical validation due to me training another novel LLM architecture I designed (it reached 3.98 perplexity for the first time today, I'm so proud), so I'm turning this over to the community early.

It's a novel attention mechanism I call Context-Aggregated Linear Attention, or CALA. In short, it's an attempt to combine the O(N) efficiency of linear attention with improved local context awareness. We attempt this by inserting an efficient ""Local Context Aggregation"" step within the attention pipeline.

The paper addresses its design novelty compared to other forms of attention such as standard quadratic attention, standard linear attention, sparse attention, and conformer's use of convolution blocks.

The paper also covers the possible downsides of the architecture, such as the complexity and difficulty dealing with kernel fusion. Specifically, the efficiency gains promised by the architecture, such as O(N) attention, rely on complex implementation of optimization of custom CUDA kernels.

For more information, the rough paper is available on [github here.](https://github.com/Mmorgan-ML/Research-Projects/blob/main/Context-Aggregated%20Linear%20Attention.pdf)

*Licensing Information*

CC BY-SA 4.0 License

All works, code, papers, etc shared here are licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.

*Licensing Information*

If anyone is interested in working on a CALA architecture (or you have access to more compute than you know what to do with and you want to help train novel architectures), please reach out to me via Reddit chat. I'd love to hear from you.",2025-04-17 01:10:56,2,0,MachineLearning,post,"[P] Today, to give back to the open source community, I release my first paper- a novel attention mechanism, Context-Aggregated Linear Attention, or CALA.
So, it's still a work in progress, but I don't have the compute to work on it right now to do empirical validation due to me training another novel LLM architecture I designed (it reached 3.98 perplexity for the first time today, I'm so proud), so I'm turning this over to the community early.

It's a novel attention mechanism I call Context-Aggregated Linear Attention, or CALA. In short, it's an attempt to combine the O(N) efficiency of linear attention with improved local context awareness. We attempt this by inserting an efficient ""Local Context Aggregation"" step within the attention pipeline.

The paper addresses its design novelty compared to other forms of attention such as standard quadratic attention, standard linear attention, sparse attention, and conformer's use of convolution blocks.

The paper also covers the possible downsides of the architecture, such as the complexity and difficulty dealing with kernel fusion. Specifically, the efficiency gains promised by the architecture, such as O(N) attention, rely on complex implementation of optimization of custom CUDA kernels.

For more information, the rough paper is available on [github here.](https://github.com/Mmorgan-ML/Research-Projects/blob/main/Context-Aggregated%20Linear%20Attention.pdf)

*Licensing Information*

CC BY-SA 4.0 License

All works, code, papers, etc shared here are licensed under the Creative Commons Attribution-ShareAlike 4.0 International License.

*Licensing Information*

If anyone is interested in working on a CALA architecture (or you have access to more compute than you know what to do with and you want to help train novel architectures), please reach out to me via Reddit chat. I'd love to hear from you."
1k0qc7v,[D] Frontier AI Models Still Fail at Basic Physical Tasks: A Manufacturing Case Study,"LLMs have made significant progress on many white collar tasks. How well do they work on simple blue collar tasks? This post has a detailed case study on manufacturing a simple brass part.

All Frontier models do terribly, even on the easiest parts of the task. Surprisingly, most models also have terrible visual abilities, and are unable to identify simple features on the part. Gemini-2.5-Pro does the best, but is still very bad.

As a result, we should expect to see progress in the physical world lag significantly behind the digital world, unless new architectures or training objectives greatly improve spatial understanding and sample efficiency.

Link to the post here: [https://adamkarvonen.github.io/machine\_learning/2025/04/13/llm-manufacturing-eval.html](https://adamkarvonen.github.io/machine_learning/2025/04/13/llm-manufacturing-eval.html)

https://preview.redd.it/4oyx33r6g8ve1.jpg?width=2371&format=pjpg&auto=webp&s=0130482db71ca5b443acca30295643e270ddf770

",2025-04-16 19:35:27,4,1,MachineLearning,post,"[D] Frontier AI Models Still Fail at Basic Physical Tasks: A Manufacturing Case Study
LLMs have made significant progress on many white collar tasks. How well do they work on simple blue collar tasks? This post has a detailed case study on manufacturing a simple brass part.

All Frontier models do terribly, even on the easiest parts of the task. Surprisingly, most models also have terrible visual abilities, and are unable to identify simple features on the part. Gemini-2.5-Pro does the best, but is still very bad.

As a result, we should expect to see progress in the physical world lag significantly behind the digital world, unless new architectures or training objectives greatly improve spatial understanding and sample efficiency.

Link to the post here: [https://adamkarvonen.github.io/machine\_learning/2025/04/13/llm-manufacturing-eval.html](https://adamkarvonen.github.io/machine_learning/2025/04/13/llm-manufacturing-eval.html)

https://preview.redd.it/4oyx33r6g8ve1.jpg?width=2371&format=pjpg&auto=webp&s=0130482db71ca5b443acca30295643e270ddf770

"
1k0a3r7,[D] ACL 2025 Meta Reviews Discussion,"Hello all,

The meta reviews of ACL are supposed to be released today. Let's engage in discussion regarding scores and corresponding meta review expectations. ",2025-04-16 04:44:30,33,63,MachineLearning,post,"[D] ACL 2025 Meta Reviews Discussion
Hello all,

The meta reviews of ACL are supposed to be released today. Let's engage in discussion regarding scores and corresponding meta review expectations. "
1k0vs90,[D] Which AI Model is best for Emoji/Sticker Generation?,"Hi everyone,

We're working on a tool that can generate emojis, stickers, and similar content as fast and efficiently as possible. I recently came across platform called EMOJIS, which is quite good.

We're trying to figure out what kind of model or architecture they might be using (diffusion-based, GAN, transformer, etc.). If anyone here has experience in this domain or can make an educated guess based on what you see on their site, we’d really appreciate any thoughts or pointers!",2025-04-16 23:19:44,0,5,MachineLearning,post,"[D] Which AI Model is best for Emoji/Sticker Generation?
Hi everyone,

We're working on a tool that can generate emojis, stickers, and similar content as fast and efficiently as possible. I recently came across platform called EMOJIS, which is quite good.

We're trying to figure out what kind of model or architecture they might be using (diffusion-based, GAN, transformer, etc.). If anyone here has experience in this domain or can make an educated guess based on what you see on their site, we’d really appreciate any thoughts or pointers!"
1k0qfhi,[P] Releasing RepAlignLoss (Custom Perceptual loss function used on my software),"Hi everyone,

I'd like to share a PyTorch loss function I've developed and just open-sourced: **RepAlignLoss**.

[Link to GitHub Repository](https://github.com/BurguerJohn/RepAlignLoss)

**Core Idea:** `RepAlignLoss` guides a student model by aligning the feature representations of its *output* with those of a *ground truth* target, as interpreted by a pre-trained, frozen *teacher* model (e.g., DINOv2, ResNet). It essentially encourages the student to produce outputs that ""look"" similar to the target from the teacher's perspective, layer by layer. This falls under feature-level knowledge distillation / perceptual loss, but specifically compares `Teacher(Student_Output)` vs. `Teacher(Ground_Truth)`.

**How it Works (Briefly):**

1. Uses forward hooks to extract intermediate activations (default: Conv2d, Linear) from the frozen teacher model.
2. Processes both the student model's output and the ground truth image through the teacher to get two sets of activations.
3. Calculates loss by comparing corresponding activation layers between the two sets.

**Key Differentiator: Localized Similarity:** Instead of comparing entire flattened feature vectors per layer, `RepAlignLoss` groups features within the flattened activation maps (currently pairs), normalizes each small group via L2 norm independently, and then computes MSE between these normalized groups. I believe this encourages finer-grained structural and feature similarity in the output.

**Practical Application & Status:** I found this loss function effective in guiding generative tasks. In fact, a version of `RepAlignLoss` is used in my commercial software, FrameFusion on Steam, to train the model that generate MotionFlow from two frames in a video. I'm actively working on the loss function as I train my model to release new version of it.

**Example Results (vs. MSE):** To provide a visual intuition, here's a comparison using `RepAlignLoss` vs. standard `MSELoss` for an image reconstruction task on the CelebA dataset. Its a simple test feeding noise to a Unet for 3000 steps and making the ground truth the celeb dataset.

[GT -> MSE Result](https://imgur.com/3ZVO4cw)

[GT -> RepAlignLoss Result](https://imgur.com/mcLpYl1)",2025-04-16 19:39:08,1,3,MachineLearning,post,"[P] Releasing RepAlignLoss (Custom Perceptual loss function used on my software)
Hi everyone,

I'd like to share a PyTorch loss function I've developed and just open-sourced: **RepAlignLoss**.

[Link to GitHub Repository](https://github.com/BurguerJohn/RepAlignLoss)

**Core Idea:** `RepAlignLoss` guides a student model by aligning the feature representations of its *output* with those of a *ground truth* target, as interpreted by a pre-trained, frozen *teacher* model (e.g., DINOv2, ResNet). It essentially encourages the student to produce outputs that ""look"" similar to the target from the teacher's perspective, layer by layer. This falls under feature-level knowledge distillation / perceptual loss, but specifically compares `Teacher(Student_Output)` vs. `Teacher(Ground_Truth)`.

**How it Works (Briefly):**

1. Uses forward hooks to extract intermediate activations (default: Conv2d, Linear) from the frozen teacher model.
2. Processes both the student model's output and the ground truth image through the teacher to get two sets of activations.
3. Calculates loss by comparing corresponding activation layers between the two sets.

**Key Differentiator: Localized Similarity:** Instead of comparing entire flattened feature vectors per layer, `RepAlignLoss` groups features within the flattened activation maps (currently pairs), normalizes each small group via L2 norm independently, and then computes MSE between these normalized groups. I believe this encourages finer-grained structural and feature similarity in the output.

**Practical Application & Status:** I found this loss function effective in guiding generative tasks. In fact, a version of `RepAlignLoss` is used in my commercial software, FrameFusion on Steam, to train the model that generate MotionFlow from two frames in a video. I'm actively working on the loss function as I train my model to release new version of it.

**Example Results (vs. MSE):** To provide a visual intuition, here's a comparison using `RepAlignLoss` vs. standard `MSELoss` for an image reconstruction task on the CelebA dataset. Its a simple test feeding noise to a Unet for 3000 steps and making the ground truth the celeb dataset.

[GT -> MSE Result](https://imgur.com/3ZVO4cw)

[GT -> RepAlignLoss Result](https://imgur.com/mcLpYl1)"
1k0mg13,[R] Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning,,2025-04-16 16:56:14,1,0,MachineLearning,post,"[R] Two Heads are Better Than One: Test-time Scaling of Multi-agent Collaborative Reasoning
"
1jzpkyj,"[R] Neuron Alignment Isn’t Fundamental — It’s a Side-Effect of ReLU & Tanh Geometry, Says New Interpretability Method","Neuron alignment — where individual neurons seem to ""represent"" real-world concepts — might be an illusion.

A new method, the **Spotlight Resonance Method (SRM)**, shows that neuron alignment isn’t a deep learning principle. Instead, it’s a *geometric artefact* of activation functions like ReLU and Tanh. These functions **break rotational symmetry** and **privilege specific directions**, causing activations to rearrange to align with these basis vectors.

🧠 **TL;DR:**

The **SRM** provides a general, mathematically grounded interpretability tool that reveals:

>**Functional Forms (ReLU, Tanh) → Anisotropic Symmetry Breaking → Privileged Directions → Neuron Alignment -> Interpretable Neurons**

It’s a predictable, controllable effect. Now we can use it.

**What this means for you:**

* New generalised interpretability metric built on a solid mathematical foundation. It works on:

>**All Architectures \~ All Layers \~ All Tasks**

* Reveals how activation functions reshape representational geometry, in a controllable way.
* The metric can be maximised increasing alignment and therefore network interpretability for safer AI.

Using it has already revealed several fundamental AI discoveries…

💥 **Exciting Discoveries for ML:**

\- Challenges neuron-based interpretability — neuron alignment is a coordinate artefact, **a human choice**, not a deep learning principle.

\- A **Geometric Framework helping to unify**: *neuron selectivity, sparsity, linear disentanglement, and possibly Neural Collapse* into one cause. Demonstrates these privileged bases are the true fundamental quantity.

\- This is empirically demonstrated through a ***direct causal link between representational alignment and activation functions***!

\- Presents evidence of interpretable neurons ('**grandmother neurons**') responding to spatially varying sky, vehicles and eyes — in **non-convolutional MLPs**.

🔦 **How it works:**

SRM rotates a 'spotlight vector' in bivector planes from a privileged basis. Using this it tracks density oscillations in the latent layer activations — revealing activation clustering induced by architectural symmetry breaking. It generalises previous methods by analysing the entire activation vector using Lie algebra and so **works on all architectures.**

The paper covers this new interpretability method and the fundamental DL discoveries made with it already…

📄 [\[ICLR 2025 Workshop Paper\]](https://openreview.net/pdf?id=alxPpqVRzX)

🛠️ [Code Implementation](https://github.com/GeorgeBird1/Spotlight-Resonance-Method)

👨‍🔬 [George Bird](http://www.linkedin.com/in/george-bird-79b8b7295)",2025-04-15 13:34:30,94,46,MachineLearning,post,"[R] Neuron Alignment Isn’t Fundamental — It’s a Side-Effect of ReLU & Tanh Geometry, Says New Interpretability Method
Neuron alignment — where individual neurons seem to ""represent"" real-world concepts — might be an illusion.

A new method, the **Spotlight Resonance Method (SRM)**, shows that neuron alignment isn’t a deep learning principle. Instead, it’s a *geometric artefact* of activation functions like ReLU and Tanh. These functions **break rotational symmetry** and **privilege specific directions**, causing activations to rearrange to align with these basis vectors.

🧠 **TL;DR:**

The **SRM** provides a general, mathematically grounded interpretability tool that reveals:

>**Functional Forms (ReLU, Tanh) → Anisotropic Symmetry Breaking → Privileged Directions → Neuron Alignment -> Interpretable Neurons**

It’s a predictable, controllable effect. Now we can use it.

**What this means for you:**

* New generalised interpretability metric built on a solid mathematical foundation. It works on:

>**All Architectures \~ All Layers \~ All Tasks**

* Reveals how activation functions reshape representational geometry, in a controllable way.
* The metric can be maximised increasing alignment and therefore network interpretability for safer AI.

Using it has already revealed several fundamental AI discoveries…

💥 **Exciting Discoveries for ML:**

\- Challenges neuron-based interpretability — neuron alignment is a coordinate artefact, **a human choice**, not a deep learning principle.

\- A **Geometric Framework helping to unify**: *neuron selectivity, sparsity, linear disentanglement, and possibly Neural Collapse* into one cause. Demonstrates these privileged bases are the true fundamental quantity.

\- This is empirically demonstrated through a ***direct causal link between representational alignment and activation functions***!

\- Presents evidence of interpretable neurons ('**grandmother neurons**') responding to spatially varying sky, vehicles and eyes — in **non-convolutional MLPs**.

🔦 **How it works:**

SRM rotates a 'spotlight vector' in bivector planes from a privileged basis. Using this it tracks density oscillations in the latent layer activations — revealing activation clustering induced by architectural symmetry breaking. It generalises previous methods by analysing the entire activation vector using Lie algebra and so **works on all architectures.**

The paper covers this new interpretability method and the fundamental DL discoveries made with it already…

📄 [\[ICLR 2025 Workshop Paper\]](https://openreview.net/pdf?id=alxPpqVRzX)

🛠️ [Code Implementation](https://github.com/GeorgeBird1/Spotlight-Resonance-Method)

👨‍🔬 [George Bird](http://www.linkedin.com/in/george-bird-79b8b7295)"
1k0fbvq,"[D] Contrastive Learning (SimCLR, MoCo) vs. Non-Contrastive Pretext Tasks (Rotation, Inpainting): When/Why Does One Approach Dominate?","I’ve been diving into self-supervised representation learning and wanted to spark a discussion about the **trade-offs between contrastive frameworks** (e.g., SimCLR, MoCo) and **non-contrastive pretext tasks** (e.g., rotation prediction, image inpainting, jigsaw puzzles).  

**Specific questions:**  
1. **Downstream Performance**: Are contrastive methods (which rely on positive/negative pairs) empirically superior for specific domains (CV, NLP, healthcare) compared to simpler pretext tasks? Or does it depend on data scale/quality?  
2. **Domain-Specific Strengths**: For example, in medical imaging (limited labeled data), does contrastive learning’s reliance on augmentations hurt generalizability? Are rotation/jigsaw tasks more robust here?  
3. **Practical Trade-offs**: Beyond accuracy, how do these approaches compare in terms of:  
   - Compute/storage (e.g., MoCo’s memory bank vs. SimCLR’s large batch sizes)  
   - Sensitivity to hyperparameters (e.g., temperature in contrastive loss)  
   - Data augmentation requirements (e.g., SimCLR’s heavy augmentations vs. minimal augmentations for rotation tasks)  

**Context**: Papers like [Barlow Twins](https://arxiv.org/abs/2103.03230) argue non-contrastive methods can match performance, but I’m curious about real-world experiences.  

**Bonus Q**: Are hybrid approaches (e.g., combining contrastive + pretext tasks) gaining traction, or is the field consolidating around one paradigm?  ",2025-04-16 10:20:49,3,1,MachineLearning,post,"[D] Contrastive Learning (SimCLR, MoCo) vs. Non-Contrastive Pretext Tasks (Rotation, Inpainting): When/Why Does One Approach Dominate?
I’ve been diving into self-supervised representation learning and wanted to spark a discussion about the **trade-offs between contrastive frameworks** (e.g., SimCLR, MoCo) and **non-contrastive pretext tasks** (e.g., rotation prediction, image inpainting, jigsaw puzzles).  

**Specific questions:**  
1. **Downstream Performance**: Are contrastive methods (which rely on positive/negative pairs) empirically superior for specific domains (CV, NLP, healthcare) compared to simpler pretext tasks? Or does it depend on data scale/quality?  
2. **Domain-Specific Strengths**: For example, in medical imaging (limited labeled data), does contrastive learning’s reliance on augmentations hurt generalizability? Are rotation/jigsaw tasks more robust here?  
3. **Practical Trade-offs**: Beyond accuracy, how do these approaches compare in terms of:  
   - Compute/storage (e.g., MoCo’s memory bank vs. SimCLR’s large batch sizes)  
   - Sensitivity to hyperparameters (e.g., temperature in contrastive loss)  
   - Data augmentation requirements (e.g., SimCLR’s heavy augmentations vs. minimal augmentations for rotation tasks)  

**Context**: Papers like [Barlow Twins](https://arxiv.org/abs/2103.03230) argue non-contrastive methods can match performance, but I’m curious about real-world experiences.  

**Bonus Q**: Are hybrid approaches (e.g., combining contrastive + pretext tasks) gaining traction, or is the field consolidating around one paradigm?  "
1jzs47u,[P] LightlyTrain: Open-source SSL pretraining for better vision models (beats ImageNet),"Hi r/MachineLearning, 

I'm Igor, co-founder at Lightly AI. We’ve just open-sourced LightlyTrain, a Python library under the **AGPL-3.0 license (making it free for academic research, educational use, and projects compatible with its terms), designed to improve your computer vision models using self-supervised learning (SSL) on your own unlabeled data.

**GitHub Repo:** [https://github.com/lightly-ai/lightly-train](https://github.com/lightly-ai/lightly-train)  
**Blog Post / Benchmarks:** [https://www.lightly.ai/blog/introducing-lightly-train](https://www.lightly.ai/blog/introducing-lightly-train)

**Problem:** ImageNet/COCO pretrained models often struggle on specific domains (medical, agriculture, etc.). Getting enough labeled data for fine-tuning is expensive and slow.

**Solution:** LightlyTrain pretrains models (like YOLO, ResNet, RT-DETR, ViTs) directly on *your* unlabeled images *before* fine-tuning. This adapts the model to your domain, boosting performance and reducing the need for labeled data.

**Why use LightlyTrain?**

* **Better Performance:** Outperforms training from scratch and ImageNet weights, especially with limited labels or strong domain shifts (see benchmarks).
* **No Labels Needed for Pretraining:** Leverage your existing unlabeled image pool.
* **Domain Adaptation:** Make foundation models work better on *your* specific visual data.
* **Easy Integration:** Works with popular frameworks (Ultralytics, TIMM, Torchvision) and runs on-prem (single/multi-GPU), scaling to millions of images. **Benchmark Highlights (details in blog post):**
* COCO (10% labels): Boosted YOLOv8-s mAP by +14% over ImageNet.
* Domain-Specific Gains: Showed clear improvements on BDD100K (driving), DeepLesion (medical), DeepWeeds (agriculture). **Quick Start:**

```python
# pip install lightly-train
import lightly_train
# Pretrain on your images
lightly_train.train(
    data=“path/to/your/images”,
    model=“ultralytics/yolov8s” # Or torchvision/resnet50, etc.
)
# Load weights and fine-tune using your existing pipeline
# ... see repo/docs for framework-specific examples ...
```

Resources:

* GitHub: [https://github.com/lightly-ai/lightly-train](https://github.com/lightly-ai/lightly-train)
* Blog Post / Benchmarks: [https://www.lightly.ai/blog/introducing-lightly-train](https://www.lightly.ai/blog/introducing-lightly-train)
* Docs: [https://docs.lightly.ai/train](https://docs.lightly.ai/train)
* Demo Video: [https://youtu.be/5Lmry1k\_cA8](https://youtu.be/5Lmry1k_cA8)

We built this to make practical SSL accessible. Hope it’s useful for the community! Happy to answer technical questions.

(Disclaimer: I’m a co-founder. Commercial licenses are available.)",2025-04-15 15:38:38,50,18,MachineLearning,post,"[P] LightlyTrain: Open-source SSL pretraining for better vision models (beats ImageNet)
Hi r/MachineLearning, 

I'm Igor, co-founder at Lightly AI. We’ve just open-sourced LightlyTrain, a Python library under the **AGPL-3.0 license (making it free for academic research, educational use, and projects compatible with its terms), designed to improve your computer vision models using self-supervised learning (SSL) on your own unlabeled data.

**GitHub Repo:** [https://github.com/lightly-ai/lightly-train](https://github.com/lightly-ai/lightly-train)  
**Blog Post / Benchmarks:** [https://www.lightly.ai/blog/introducing-lightly-train](https://www.lightly.ai/blog/introducing-lightly-train)

**Problem:** ImageNet/COCO pretrained models often struggle on specific domains (medical, agriculture, etc.). Getting enough labeled data for fine-tuning is expensive and slow.

**Solution:** LightlyTrain pretrains models (like YOLO, ResNet, RT-DETR, ViTs) directly on *your* unlabeled images *before* fine-tuning. This adapts the model to your domain, boosting performance and reducing the need for labeled data.

**Why use LightlyTrain?**

* **Better Performance:** Outperforms training from scratch and ImageNet weights, especially with limited labels or strong domain shifts (see benchmarks).
* **No Labels Needed for Pretraining:** Leverage your existing unlabeled image pool.
* **Domain Adaptation:** Make foundation models work better on *your* specific visual data.
* **Easy Integration:** Works with popular frameworks (Ultralytics, TIMM, Torchvision) and runs on-prem (single/multi-GPU), scaling to millions of images. **Benchmark Highlights (details in blog post):**
* COCO (10% labels): Boosted YOLOv8-s mAP by +14% over ImageNet.
* Domain-Specific Gains: Showed clear improvements on BDD100K (driving), DeepLesion (medical), DeepWeeds (agriculture). **Quick Start:**

```python
# pip install lightly-train
import lightly_train
# Pretrain on your images
lightly_train.train(
    data=“path/to/your/images”,
    model=“ultralytics/yolov8s” # Or torchvision/resnet50, etc.
)
# Load weights and fine-tune using your existing pipeline
# ... see repo/docs for framework-specific examples ...
```

Resources:

* GitHub: [https://github.com/lightly-ai/lightly-train](https://github.com/lightly-ai/lightly-train)
* Blog Post / Benchmarks: [https://www.lightly.ai/blog/introducing-lightly-train](https://www.lightly.ai/blog/introducing-lightly-train)
* Docs: [https://docs.lightly.ai/train](https://docs.lightly.ai/train)
* Demo Video: [https://youtu.be/5Lmry1k\_cA8](https://youtu.be/5Lmry1k_cA8)

We built this to make practical SSL accessible. Hope it’s useful for the community! Happy to answer technical questions.

(Disclaimer: I’m a co-founder. Commercial licenses are available.)"
1k01cfy,Deep Dive into [R]WKV-7 with Author Eugene Cheah,"Hey all,

Last week we did a Deep Dive into RWKV (specifically the newest RWKV-7) with our Arxiv Dive research paper club. We were lucky enough to have one of the main authors & maintainers (Eugene Cheah) join and answer questions at the end, so wanted to share the full video here:

[https://www.youtube.com/watch?v=4Bdty7GOrbw](https://www.youtube.com/watch?v=4Bdty7GOrbw)

We also put it in blog form in you prefer that:

[https://www.oxen.ai/blog/how-rwkv-7-goose-works-notes-from-the-author](https://www.oxen.ai/blog/how-rwkv-7-goose-works-notes-from-the-author)

The post builds up intuition of what problems RWKV is trying to solve. I thought it was really interesting how the organization iterates on models with the community. Also it left me wanting to run more experiments with ""Learning at Test Time"" instead of fine-tuning. Lots of interesting threads to pull there.

Hope you enjoy!",2025-04-15 21:53:29,14,0,MachineLearning,post,"Deep Dive into [R]WKV-7 with Author Eugene Cheah
Hey all,

Last week we did a Deep Dive into RWKV (specifically the newest RWKV-7) with our Arxiv Dive research paper club. We were lucky enough to have one of the main authors & maintainers (Eugene Cheah) join and answer questions at the end, so wanted to share the full video here:

[https://www.youtube.com/watch?v=4Bdty7GOrbw](https://www.youtube.com/watch?v=4Bdty7GOrbw)

We also put it in blog form in you prefer that:

[https://www.oxen.ai/blog/how-rwkv-7-goose-works-notes-from-the-author](https://www.oxen.ai/blog/how-rwkv-7-goose-works-notes-from-the-author)

The post builds up intuition of what problems RWKV is trying to solve. I thought it was really interesting how the organization iterates on models with the community. Also it left me wanting to run more experiments with ""Learning at Test Time"" instead of fine-tuning. Lots of interesting threads to pull there.

Hope you enjoy!"
1k0dov1,MODE: A Lightweight TraditionalRAG Alternative (Looking for arXiv Endorsement) [P],"Hi all,

I’m an independent researcher and recently completed a paper titled **MODE: Mixture of Document Experts**, which proposes a lightweight alternative to traditional Retrieval-Augmented Generation (RAG) pipelines.

Instead of relying on vector databases and re-rankers, MODE clusters documents and uses centroid-based retrieval — making it efficient and interpretable, especially for small to medium-sized datasets.

📄 Paper (PDF): [https://github.com/rahulanand1103/mode/blob/main/paper/mode.pdf](https://github.com/rahulanand1103/mode/blob/main/paper/mode.pdf)  
📚 Docs: [https://mode-rag.readthedocs.io/en/latest/](https://mode-rag.readthedocs.io/en/latest/)  
📦 PyPI: pip install mode\_rag  
🔗 GitHub: [https://github.com/rahulanand1103/mode](https://github.com/rahulanand1103/mode)

I’d like to share this work on **arXiv (cs.AI)** but need an endorsement to submit. If you’ve published in [cs.AI](http://cs.ai/) and would be willing to endorse me, I’d be truly grateful.

🔗 **Endorsement UR**L: [https://arxiv.org/auth/endorse?x=E8V99K](https://arxiv.org/auth/endorse?x=E8V99K)  
🔑 **Endorsement Cod**e: E8V99K

Please feel free to DM me or reply here if you'd like to chat or review the paper. Thank you for your time and support!

— Rahul Anand",2025-04-16 08:21:31,0,5,MachineLearning,post,"MODE: A Lightweight TraditionalRAG Alternative (Looking for arXiv Endorsement) [P]
Hi all,

I’m an independent researcher and recently completed a paper titled **MODE: Mixture of Document Experts**, which proposes a lightweight alternative to traditional Retrieval-Augmented Generation (RAG) pipelines.

Instead of relying on vector databases and re-rankers, MODE clusters documents and uses centroid-based retrieval — making it efficient and interpretable, especially for small to medium-sized datasets.

📄 Paper (PDF): [https://github.com/rahulanand1103/mode/blob/main/paper/mode.pdf](https://github.com/rahulanand1103/mode/blob/main/paper/mode.pdf)  
📚 Docs: [https://mode-rag.readthedocs.io/en/latest/](https://mode-rag.readthedocs.io/en/latest/)  
📦 PyPI: pip install mode\_rag  
🔗 GitHub: [https://github.com/rahulanand1103/mode](https://github.com/rahulanand1103/mode)

I’d like to share this work on **arXiv (cs.AI)** but need an endorsement to submit. If you’ve published in [cs.AI](http://cs.ai/) and would be willing to endorse me, I’d be truly grateful.

🔗 **Endorsement UR**L: [https://arxiv.org/auth/endorse?x=E8V99K](https://arxiv.org/auth/endorse?x=E8V99K)  
🔑 **Endorsement Cod**e: E8V99K

Please feel free to DM me or reply here if you'd like to chat or review the paper. Thank you for your time and support!

— Rahul Anand"
1k0fx5b,[D]Mistake accesor model,"Hey Devs,
Struggling with LLM hallucinations and the lack of nuance in error correction? Here's a concept I've been mulling over:
Problem: LLMs often hallucinate confidently instead of admitting ignorance (""I don't know""). Standard training/fine-tuning doesn't always differentiate the severity of mistakes – a major factual error might not be penalized significantly more than a minor grammatical one.
Proposed Solution: Implement a secondary ""Mistake Assessor"" model or system. Its job:
Evaluate outputs from the primary LLM.
Assign weighted penalties based on error impact:
Very High Penalty: Hallucinations, confidently incorrect statements, harmful content.
Low/Zero Penalty: Correctly stating ""I don't know,"" identifying uncertainty, minor stylistic flaws.
Variable Penalty: Other errors weighted by severity (factual > grammatical).
Feed this weighted score back into the primary LLM's learning process (e.g., as a refined reward signal in RLHF or influencing the loss function during fine-tuning).
Potential Benefits:
Directly incentivizes admitting ignorance over fabrication.
Accelerates learning by forcing the model to prioritize fixing high-impact errors.
Improves overall reliability and trustworthiness.
Could act as an internal ""risk assessment"" guiding response generation.
Context: I'm not equipped to code this, but the concept seems promising for tackling core LLM reliability issues.
Looking for thoughts: Is this feasible? Does similar work exist? What are the immediate implementation challenges you foresee?",2025-04-16 11:05:16,0,2,MachineLearning,post,"[D]Mistake accesor model
Hey Devs,
Struggling with LLM hallucinations and the lack of nuance in error correction? Here's a concept I've been mulling over:
Problem: LLMs often hallucinate confidently instead of admitting ignorance (""I don't know""). Standard training/fine-tuning doesn't always differentiate the severity of mistakes – a major factual error might not be penalized significantly more than a minor grammatical one.
Proposed Solution: Implement a secondary ""Mistake Assessor"" model or system. Its job:
Evaluate outputs from the primary LLM.
Assign weighted penalties based on error impact:
Very High Penalty: Hallucinations, confidently incorrect statements, harmful content.
Low/Zero Penalty: Correctly stating ""I don't know,"" identifying uncertainty, minor stylistic flaws.
Variable Penalty: Other errors weighted by severity (factual > grammatical).
Feed this weighted score back into the primary LLM's learning process (e.g., as a refined reward signal in RLHF or influencing the loss function during fine-tuning).
Potential Benefits:
Directly incentivizes admitting ignorance over fabrication.
Accelerates learning by forcing the model to prioritize fixing high-impact errors.
Improves overall reliability and trustworthiness.
Could act as an internal ""risk assessment"" guiding response generation.
Context: I'm not equipped to code this, but the concept seems promising for tackling core LLM reliability issues.
Looking for thoughts: Is this feasible? Does similar work exist? What are the immediate implementation challenges you foresee?"
1jzpb75,[D] Are you guys still developing inhouse NLP models?," In this LLM era, are you guys still building nlp models from scratch or just fine tuning from the LLM prompts?",2025-04-15 13:19:28,15,12,MachineLearning,post,"[D] Are you guys still developing inhouse NLP models?
 In this LLM era, are you guys still building nlp models from scratch or just fine tuning from the LLM prompts?"
1jzjy7f,"[D] Experiment tracking for student researchers - WandB, Neptune, or Comet ML?","Hi,

I've come down to these 3, but can you help me decide which would be the best choice rn for me as a student researcher?

I have used WandB a bit in the past, but I read it tends to cause some slow down, and I'm training a large transformer model, so I'd like to avoid that. I'll also be using multiple GPUs, in case that's helpful information to decide which is best.

Specifically, which is easiest to quickly set up and get started with, stable (doesn't cause issues), and is decent for tracking metrics, parameters?

TIA!",2025-04-15 07:17:55,38,18,MachineLearning,post,"[D] Experiment tracking for student researchers - WandB, Neptune, or Comet ML?
Hi,

I've come down to these 3, but can you help me decide which would be the best choice rn for me as a student researcher?

I have used WandB a bit in the past, but I read it tends to cause some slow down, and I'm training a large transformer model, so I'd like to avoid that. I'll also be using multiple GPUs, in case that's helpful information to decide which is best.

Specifically, which is easiest to quickly set up and get started with, stable (doesn't cause issues), and is decent for tracking metrics, parameters?

TIA!"
1k040es,[P] How and should I use Deepgaze pytorch? - Saliency Maps,"Hi

I'm working on a project exploring visual attention and saliency modeling — specifically trying to compare traditional detection approaches like Faster R-CNN with saliency-based methods. I recently found DeepGaze pytorch and was hoping to integrate it easily into my pipeline on Google Colab. The model is exactly what I need: pretrained, biologically inspired, and built for saliency prediction. However, I'm hitting a wall.

* I installed it using !pip install git+https://github.com/matthias-k/deepgaze\_pytorch.git
* I downloaded the centerbias file as required
* But import deepgaze\_pytorch throws ModuleNotFoundError every time even after switching Colab’s runtime to Python 3.10 (via ""Use fallback runtime version"").

Has anyone gotten this to work recently on Colab? Is there an extra step I’m missing to register or install the module properly? Finally is DeepGaze still a recommended tool for saliency research, or should I consider alternatives?

Any help or direction would be seriously appreciated :-\_ )",2025-04-15 23:45:21,1,1,MachineLearning,post,"[P] How and should I use Deepgaze pytorch? - Saliency Maps
Hi

I'm working on a project exploring visual attention and saliency modeling — specifically trying to compare traditional detection approaches like Faster R-CNN with saliency-based methods. I recently found DeepGaze pytorch and was hoping to integrate it easily into my pipeline on Google Colab. The model is exactly what I need: pretrained, biologically inspired, and built for saliency prediction. However, I'm hitting a wall.

* I installed it using !pip install git+https://github.com/matthias-k/deepgaze\_pytorch.git
* I downloaded the centerbias file as required
* But import deepgaze\_pytorch throws ModuleNotFoundError every time even after switching Colab’s runtime to Python 3.10 (via ""Use fallback runtime version"").

Has anyone gotten this to work recently on Colab? Is there an extra step I’m missing to register or install the module properly? Finally is DeepGaze still a recommended tool for saliency research, or should I consider alternatives?

Any help or direction would be seriously appreciated :-\_ )"
1k03i0k,[P] I fine-tuned GPT-2 and GPT-J to mimic Mr. Darcy. Results were a mixture of promising and strange.,"This was a personal project I've worked on over the last 2 months. I wanted to see whether GPT-2 or GPT-J could be fine-tuned to consistently speak in the voice of Mr. Darcy from Pride and Prejudice—formal, clipped, and just a bit judgmental.

By fine-tune dataset standards, there’s barely any original dialogue from Darcy to work with. In an effort to mitigate this disadvantage, I included some peer-reviewed synthetic examples I wrote myself.

In the end, 2 datasets were used:

* 1st: Context-rich excerpts from the book encompassing dialogue, narrative elements, and perspectives from other characters.
* 2nd: Restricted to dialogue interactions, directly pairing either book-original or crafted prompts with Darcy's responses.

Training GPT-2 (medium) produced noticeable changes. BLEU-4 scores improved by 70% compared to the base model, though perplexity shot up and outputs reflect confusion about context. GPT-J was much more resistant to change (expected given its size), and I'd have liked to experiment with more variants but don't really have the computing power for training.

I wrote about the project here, including:

* Samples of model output (some successful, some not)
* Comparisons between models and training rounds
* What I tried, what worked, what didn't

📝 [Medium article](https://medium.com/@rhodrithomas_79672/training-mr-darcy-fine-tuning-ai-models-for-distinctive-speech-patterns-d336867ec0e3) 
📄 [PDF of article](https://github.com/birlumbus/DarcyGPT/blob/main/assets/Training%20Mr.%20Darcy.pdf) 
💾 [Code and datasets](https://github.com/birlumbus/DarcyGPT)

If anyone else has played around with literary style transfer, historical voice modeling, or just weird LLM fine-tuning ideas, I’d love to hear about it. I no longer have time to continue the project, but I’m open to any feedback or suggestions on how to push this kind of thing further (or evaluate it better).",2025-04-15 23:23:14,1,2,MachineLearning,post,"[P] I fine-tuned GPT-2 and GPT-J to mimic Mr. Darcy. Results were a mixture of promising and strange.
This was a personal project I've worked on over the last 2 months. I wanted to see whether GPT-2 or GPT-J could be fine-tuned to consistently speak in the voice of Mr. Darcy from Pride and Prejudice—formal, clipped, and just a bit judgmental.

By fine-tune dataset standards, there’s barely any original dialogue from Darcy to work with. In an effort to mitigate this disadvantage, I included some peer-reviewed synthetic examples I wrote myself.

In the end, 2 datasets were used:

* 1st: Context-rich excerpts from the book encompassing dialogue, narrative elements, and perspectives from other characters.
* 2nd: Restricted to dialogue interactions, directly pairing either book-original or crafted prompts with Darcy's responses.

Training GPT-2 (medium) produced noticeable changes. BLEU-4 scores improved by 70% compared to the base model, though perplexity shot up and outputs reflect confusion about context. GPT-J was much more resistant to change (expected given its size), and I'd have liked to experiment with more variants but don't really have the computing power for training.

I wrote about the project here, including:

* Samples of model output (some successful, some not)
* Comparisons between models and training rounds
* What I tried, what worked, what didn't

📝 [Medium article](https://medium.com/@rhodrithomas_79672/training-mr-darcy-fine-tuning-ai-models-for-distinctive-speech-patterns-d336867ec0e3) 
📄 [PDF of article](https://github.com/birlumbus/DarcyGPT/blob/main/assets/Training%20Mr.%20Darcy.pdf) 
💾 [Code and datasets](https://github.com/birlumbus/DarcyGPT)

If anyone else has played around with literary style transfer, historical voice modeling, or just weird LLM fine-tuning ideas, I’d love to hear about it. I no longer have time to continue the project, but I’m open to any feedback or suggestions on how to push this kind of thing further (or evaluate it better)."
1k02geq,[D] LoRA Vs Task Vectors,What are the difference between a LoRA adapters and task vectors? Is it just the context in which they are used?,2025-04-15 22:38:56,0,0,MachineLearning,post,"[D] LoRA Vs Task Vectors
What are the difference between a LoRA adapters and task vectors? Is it just the context in which they are used?"
1jzp9d0,[D] How to train this model with constrained resources?,"So I have made a model following this [paper](https://arxiv.org/abs/2006.04768). They basically reduced the complexity of computing the attention weights. So I modified the attention mechanism accordingly. Now, the problem is that to compare the performance, they used 64 tesla v100 gpus and used the BookCorpus along with English Wiki data which accounts to over 3300M words. I don't have access to that much resources(max is kaggle).  
I want to show that my model can show comparable performance but at lower computation complexity. I don't know how to proceed now. Please help me.  
My model has a typical transformer decoder architecture, similar to gpt2-small, 12 layers, 12 heads per layer. Total there are 164M parameters in my model.",2025-04-15 13:16:32,4,6,MachineLearning,post,"[D] How to train this model with constrained resources?
So I have made a model following this [paper](https://arxiv.org/abs/2006.04768). They basically reduced the complexity of computing the attention weights. So I modified the attention mechanism accordingly. Now, the problem is that to compare the performance, they used 64 tesla v100 gpus and used the BookCorpus along with English Wiki data which accounts to over 3300M words. I don't have access to that much resources(max is kaggle).  
I want to show that my model can show comparable performance but at lower computation complexity. I don't know how to proceed now. Please help me.  
My model has a typical transformer decoder architecture, similar to gpt2-small, 12 layers, 12 heads per layer. Total there are 164M parameters in my model."
1jzdgyk,[R] The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search,,2025-04-15 01:39:19,15,0,MachineLearning,post,"[R] The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search
"
1jyz2vg,[D] What happened to KANs? (Kolmogorov-Arnold Networks),"KANs seem promising but im not hearing any real
applications of it. Curious if anyone has worked on it",2025-04-14 15:37:55,101,33,MachineLearning,post,"[D] What happened to KANs? (Kolmogorov-Arnold Networks)
KANs seem promising but im not hearing any real
applications of it. Curious if anyone has worked on it"
1jzyl0s,[R] Scaling Laws of Synthetic Data for Language Models,,2025-04-15 20:01:30,0,0,MachineLearning,post,"[R] Scaling Laws of Synthetic Data for Language Models
"
1jz6shu,How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models [R],,2025-04-14 20:54:56,33,2,MachineLearning,post,"How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models [R]
"
1jzobwq,[D] Adress & names matching technique recommendations,"Context: 
I have a dataset of company owned products like: Name: Company A, Address: 5th avenue, Product: A. 
Company A inc, Address: New york, Product B. 
Company A inc. , Address, 5th avenue New York, product C. 

I have 400 million entries like these. As you can see, addresses and names are in inconsistent formats. 
I have another dataset that will be me ground truth for companies. It has a clean name for the company along with it’s parsed address. 

The objective is to match the records from the table with inconsistent formats to the ground truth, so that each product is linked to a clean company. 



Questions and help: 
- i was thinking to use google geocoding api to parse the addresses and get geocoding. Then use the geocoding to perform distance search between my my addresses and ground truth BUT i don’t have the geocoding in the ground truth dataset. So, i would like to find another method to match parsed addresses without using geocoding. 

- Ideally, i would like to be able to input my parsed address and the name (maybe along with some other features like industry of activity) and get returned the top matching candidates from the ground truth dataset with a score between 0 and 1. Which approach would you suggest that fits big size datasets? 

- The method should be able to handle cases were one of my addresses could be: company A, address: Washington (meaning an approximate address that is just a city for example, sometimes the country is not even specified). I will receive several parsed addresses from this candidate as Washington is vague. What is the best practice in such cases? As the google api won’t return a single result, what can i do?

- My addresses are from all around the world, do you know if google api can handle the whole world? Would a language model be better at parsing for some regions? 

Help would be very much appreciated, thank you guys. 
",2025-04-15 12:19:49,2,2,MachineLearning,post,"[D] Adress & names matching technique recommendations
Context: 
I have a dataset of company owned products like: Name: Company A, Address: 5th avenue, Product: A. 
Company A inc, Address: New york, Product B. 
Company A inc. , Address, 5th avenue New York, product C. 

I have 400 million entries like these. As you can see, addresses and names are in inconsistent formats. 
I have another dataset that will be me ground truth for companies. It has a clean name for the company along with it’s parsed address. 

The objective is to match the records from the table with inconsistent formats to the ground truth, so that each product is linked to a clean company. 



Questions and help: 
- i was thinking to use google geocoding api to parse the addresses and get geocoding. Then use the geocoding to perform distance search between my my addresses and ground truth BUT i don’t have the geocoding in the ground truth dataset. So, i would like to find another method to match parsed addresses without using geocoding. 

- Ideally, i would like to be able to input my parsed address and the name (maybe along with some other features like industry of activity) and get returned the top matching candidates from the ground truth dataset with a score between 0 and 1. Which approach would you suggest that fits big size datasets? 

- The method should be able to handle cases were one of my addresses could be: company A, address: Washington (meaning an approximate address that is just a city for example, sometimes the country is not even specified). I will receive several parsed addresses from this candidate as Washington is vague. What is the best practice in such cases? As the google api won’t return a single result, what can i do?

- My addresses are from all around the world, do you know if google api can handle the whole world? Would a language model be better at parsing for some regions? 

Help would be very much appreciated, thank you guys. 
"
1jz80xq,[D] [P] List of LLM architectures. I am collecting arxiv papers on LLM architectures- looking for any I'm missing.,"Hey all.

I'm looking for suggestions and links to any main arxiv papers for LLM architectures (and similar) I don't have in my collection yet. Would appreciate any help.

Also, as for what this is all for, I have a hobby of ""designing"" novel small language model architectures. I was curious if someone who has access to more compute than me might be interested in teaming up and doing a project with me with the ultimate goal to release a novel architecture under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license?

So far, I have the following:

-------------------------------------------------

Associative Recurrent Memory Transformers

BERT

Bi-Mamba

BigBird

DeepSeek R1

DeepSeek V3

Hyena

Hymba

Jamba

Linear Transformers

Linformer

Longformer

Mamba

Neural Turing Machines

Performer

Recurrent Memory Transformer

RetNet

RWKV

S4

Titans

Transformer",2025-04-14 21:44:33,27,19,MachineLearning,post,"[D] [P] List of LLM architectures. I am collecting arxiv papers on LLM architectures- looking for any I'm missing.
Hey all.

I'm looking for suggestions and links to any main arxiv papers for LLM architectures (and similar) I don't have in my collection yet. Would appreciate any help.

Also, as for what this is all for, I have a hobby of ""designing"" novel small language model architectures. I was curious if someone who has access to more compute than me might be interested in teaming up and doing a project with me with the ultimate goal to release a novel architecture under a Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license?

So far, I have the following:

-------------------------------------------------

Associative Recurrent Memory Transformers

BERT

Bi-Mamba

BigBird

DeepSeek R1

DeepSeek V3

Hyena

Hymba

Jamba

Linear Transformers

Linformer

Longformer

Mamba

Neural Turing Machines

Performer

Recurrent Memory Transformer

RetNet

RWKV

S4

Titans

Transformer"
1jzffw1,"[D] Building a marketplace for 100K+ hours of high-quality, ethically sourced video data—looking for feedback from AI researchers","Hey all,

I'm working on a marketplace designed specifically for AI labs:  
100K+ hours of ethically sourced, studio-licensed video content for large-scale training.

We’re building multimodal search into the core—so you can search by natural language across visuals, audio, and metadata. The idea is to make massive video datasets *actually usable*.

A few open questions for researchers and engineers training on video:

* What format do you prefer for training data? RAW? Compressed (MP4)? Resolutions like 4K, 2K, or Full HD? Something else?
* We’ve segmented videos and made them searchable via natural language.

You can license:

→ Just the segments that matches your query

→ The full videos it came from

→ Or the entire dataset

Is this kind of granular licensing actually useful in your workflow—or do you typically need larger chunks or full datasets anyway?

We’re in user discovery mode and trying to validate core assumptions. If you train on video or audio-visual data, I’d love to hear your thoughts—either in the comments or via DM.

Thanks in advance!",2025-04-15 03:14:06,5,5,MachineLearning,post,"[D] Building a marketplace for 100K+ hours of high-quality, ethically sourced video data—looking for feedback from AI researchers
Hey all,

I'm working on a marketplace designed specifically for AI labs:  
100K+ hours of ethically sourced, studio-licensed video content for large-scale training.

We’re building multimodal search into the core—so you can search by natural language across visuals, audio, and metadata. The idea is to make massive video datasets *actually usable*.

A few open questions for researchers and engineers training on video:

* What format do you prefer for training data? RAW? Compressed (MP4)? Resolutions like 4K, 2K, or Full HD? Something else?
* We’ve segmented videos and made them searchable via natural language.

You can license:

→ Just the segments that matches your query

→ The full videos it came from

→ Or the entire dataset

Is this kind of granular licensing actually useful in your workflow—or do you typically need larger chunks or full datasets anyway?

We’re in user discovery mode and trying to validate core assumptions. If you train on video or audio-visual data, I’d love to hear your thoughts—either in the comments or via DM.

Thanks in advance!"
1jz8boa,[D] Advice on building Random Forest/XGBoost model,"I have EMR data with millions of records and around 700 variables. I need to create a Random Forest or XGBoost model to assess the risk of hospitalization within 30 days post-surgery. Given the large number of variables, I'm planning to follow this process:

1. Split the data into training, validation, and test sets, and perform the following steps on the training set.
2. Use the default settings for RF/XGBoost and remove around half (or more) of the features based on feature importance.
3. Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation.
4. Reassess feature selection based on the new hyperparameters, and continue iterating between feature selection and hyperparameter tuning, evaluating performance on the validation set.

My questions are:

1. Should I start with the default settings for the RF/XGBoost model and eliminate half the features based on feature importance before performing hyperparameter tuning, or should I tune the model first? I am concerned that with such large data, tuning might not be feasible.
2. Does my approach look good? Please suggest any improvements or steps I may have missed.

This is my first time working with data of this size.

The end point of this project is to implement a model for future patients to predict 30-day hospitalization risk.",2025-04-14 21:56:36,11,13,MachineLearning,post,"[D] Advice on building Random Forest/XGBoost model
I have EMR data with millions of records and around 700 variables. I need to create a Random Forest or XGBoost model to assess the risk of hospitalization within 30 days post-surgery. Given the large number of variables, I'm planning to follow this process:

1. Split the data into training, validation, and test sets, and perform the following steps on the training set.
2. Use the default settings for RF/XGBoost and remove around half (or more) of the features based on feature importance.
3. Perform hyperparameter tuning using GridSearchCV with 5-fold cross-validation.
4. Reassess feature selection based on the new hyperparameters, and continue iterating between feature selection and hyperparameter tuning, evaluating performance on the validation set.

My questions are:

1. Should I start with the default settings for the RF/XGBoost model and eliminate half the features based on feature importance before performing hyperparameter tuning, or should I tune the model first? I am concerned that with such large data, tuning might not be feasible.
2. Does my approach look good? Please suggest any improvements or steps I may have missed.

This is my first time working with data of this size.

The end point of this project is to implement a model for future patients to predict 30-day hospitalization risk."
1k00wm0,"[D] Creating my own AI model from scratch, is it worth it?","Hey everyone, I’m a web developer teaching myself AI and I was building a SaaS to act as a direct competitor with Jasper AI. However I got stuck deciding between building my own AI model from scratch (for full control and originality) or using existing models like GPT or open-source ones (to move faster and get better results early).

I know there are tradeoffs. I want to innovate, but I don’t want to get lost reinventing the wheel either. And there are a lot of stuff I still need to learn to truly bring this Saas to life. So I wanted some opnions from people with more experience here, I truly appreciate any help.

",2025-04-15 21:35:07,0,25,MachineLearning,post,"[D] Creating my own AI model from scratch, is it worth it?
Hey everyone, I’m a web developer teaching myself AI and I was building a SaaS to act as a direct competitor with Jasper AI. However I got stuck deciding between building my own AI model from scratch (for full control and originality) or using existing models like GPT or open-source ones (to move faster and get better results early).

I know there are tradeoffs. I want to innovate, but I don’t want to get lost reinventing the wheel either. And there are a lot of stuff I still need to learn to truly bring this Saas to life. So I wanted some opnions from people with more experience here, I truly appreciate any help.

"
1jyr6ah,[D] Distillation is underrated. I replicated GPT-4o's capability in a 14x cheaper model,"Just tried something cool with distillation. Managed to replicate GPT-4o-level performance (92% accuracy) using a much smaller, fine-tuned model and it runs 14x cheaper. For those unfamiliar, distillation is basically: take a huge, expensive model, and use it to train a smaller, cheaper, faster one on a specific domain. If done right, the small model could perform *almost* as well, at a fraction of the cost. Honestly, super promising. Curious if anyone else here has played with distillation. Tell me more use cases. 

Adding my code in the comments.",2025-04-14 07:12:18,109,26,MachineLearning,post,"[D] Distillation is underrated. I replicated GPT-4o's capability in a 14x cheaper model
Just tried something cool with distillation. Managed to replicate GPT-4o-level performance (92% accuracy) using a much smaller, fine-tuned model and it runs 14x cheaper. For those unfamiliar, distillation is basically: take a huge, expensive model, and use it to train a smaller, cheaper, faster one on a specific domain. If done right, the small model could perform *almost* as well, at a fraction of the cost. Honestly, super promising. Curious if anyone else here has played with distillation. Tell me more use cases. 

Adding my code in the comments."
1jzauh4,[D] Is fractional differencing helpful for ML outside of economics?,"I've been trying to figure out ways to apply ml to non-stationary signals in my research. One very ubiquitous example I see is fractional differencing, which is commonly used in fintech. However, I don't see any mention of it outside of fintech. I'm not really sure why.

I would have expected to see it being attempted in something like neural signal processing or seismic data for ML.",2025-04-14 23:40:49,2,1,MachineLearning,post,"[D] Is fractional differencing helpful for ML outside of economics?
I've been trying to figure out ways to apply ml to non-stationary signals in my research. One very ubiquitous example I see is fractional differencing, which is commonly used in fintech. However, I don't see any mention of it outside of fintech. I'm not really sure why.

I would have expected to see it being attempted in something like neural signal processing or seismic data for ML."
1jzkzh6,[D] Creating AI Avatars from Scratch,"Firstly thanks for the help on my previous post, y'all are awesome. I now have a new thing to work on, which is creating AI avatars that users can converse with. I need something that can talk and essentially TTS the replies my chatbot generates. I need an open source solution that can create normal avatars which are kinda realistic and good to look at. Please let me know such options, at the lowest cost of compute.",2025-04-15 08:26:54,0,1,MachineLearning,post,"[D] Creating AI Avatars from Scratch
Firstly thanks for the help on my previous post, y'all are awesome. I now have a new thing to work on, which is creating AI avatars that users can converse with. I need something that can talk and essentially TTS the replies my chatbot generates. I need an open source solution that can create normal avatars which are kinda realistic and good to look at. Please let me know such options, at the lowest cost of compute."
1jz0qlk,[D] Outlier analysis in machine learning,"I trained multiple ML models and noticed that certain samples consistently yield high prediction errors. I’d like to investigate why these samples are harder to predict - whether due to inherent noise, data quality issues, or model limitations.

Does it make sense to focus on samples with high-error as outliers, or would other methods (e.g., uncertainty estimation with Gaussian Processes) be more appropriate?",2025-04-14 16:50:01,3,3,MachineLearning,post,"[D] Outlier analysis in machine learning
I trained multiple ML models and noticed that certain samples consistently yield high prediction errors. I’d like to investigate why these samples are harder to predict - whether due to inherent noise, data quality issues, or model limitations.

Does it make sense to focus on samples with high-error as outliers, or would other methods (e.g., uncertainty estimation with Gaussian Processes) be more appropriate?"
1jydy3j,[D] ICML 2025: A Shift Toward Correctness Over SOTA?,"ICML's policy this year—a good direction, prioritizing correctness over chasing SOTA?",2025-04-13 20:08:33,121,25,MachineLearning,post,"[D] ICML 2025: A Shift Toward Correctness Over SOTA?
ICML's policy this year—a good direction, prioritizing correctness over chasing SOTA?"
1jyzamc,[D] Latest TTS for voice cloning,"Hello,  
  
Do you guys know any good tts that I can run locally to clone a voice preferably multilingual?

Please no 11 labs cuz ridiculous pricing, looking for something i can thinker locally.",2025-04-14 15:47:50,1,3,MachineLearning,post,"[D] Latest TTS for voice cloning
Hello,  
  
Do you guys know any good tts that I can run locally to clone a voice preferably multilingual?

Please no 11 labs cuz ridiculous pricing, looking for something i can thinker locally."
1jypxab,[D] Unable to replicate reported results when training MMPose models from scratch,"I'm trying out MMPose but have been completely unable to replicate the reported performance using their training scripts. I've tried several models without success.

For example, I ran the following command to train from scratch:

`CUDA_VISIBLE_DEVICES=0 python tools/train.py projects/rtmpose/rtmpose/wholebody_2d_keypoint/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py`

which, according to the table at [https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose), [RTMPose-l](https://github.com/open-mmlab/mmpose/blob/main/projects/rtmpose/rtmpose/wholebody_2d_keypoint/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py) with an input size of 256x192, is supposed to achieve a Whole AP of 61.1 on the COCO dataset. However, I can only reach an AP of 54.5. I also tried increasing the stage 2 fine-tuning duration from 30 to 300 epochs, but the best result I got was an AP of 57.6. Additionally, I attempted to resume training from their provided pretrained models for more epochs, but the performance consistently degrades.

Has anyone else experienced similar issues or have any insights into what might be going wrong?",2025-04-14 05:54:28,5,0,MachineLearning,post,"[D] Unable to replicate reported results when training MMPose models from scratch
I'm trying out MMPose but have been completely unable to replicate the reported performance using their training scripts. I've tried several models without success.

For example, I ran the following command to train from scratch:

`CUDA_VISIBLE_DEVICES=0 python tools/train.py projects/rtmpose/rtmpose/wholebody_2d_keypoint/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py`

which, according to the table at [https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose](https://github.com/open-mmlab/mmpose/tree/main/projects/rtmpose), [RTMPose-l](https://github.com/open-mmlab/mmpose/blob/main/projects/rtmpose/rtmpose/wholebody_2d_keypoint/rtmpose-l_8xb64-270e_coco-wholebody-256x192.py) with an input size of 256x192, is supposed to achieve a Whole AP of 61.1 on the COCO dataset. However, I can only reach an AP of 54.5. I also tried increasing the stage 2 fine-tuning duration from 30 to 300 epochs, but the best result I got was an AP of 57.6. Additionally, I attempted to resume training from their provided pretrained models for more epochs, but the performance consistently degrades.

Has anyone else experienced similar issues or have any insights into what might be going wrong?"
1jyjkjf,[D] Just open-sourced a financial LLM trained on 10 years of Indian market data — outputs SQL you can run on DuckDB,"Hey folks,

Wanted to share something I’ve been building over the past few weeks — a small open-source project that’s been a grind to get right.

I fine-tuned a transformer model on structured Indian stock market data — fundamentals, OHLCV, and index data — across 10+ years. The model outputs SQL queries in response to natural language questions like:

- “What was the net_profit of INFY on 2021-03-31?”
- “What’s the 30-day moving average of TCS close price on 2023-02-01?”
- “Show me YoY growth of EPS for RELIANCE.”

It’s 100% offline — no APIs, no cloud calls — and ships with a DuckDB file preloaded with the dataset. You can paste the model’s SQL output into DuckDB and get results instantly. You can even add your own data without changing the schema.

Built this as a proof of concept for how useful small LLMs can be if you ground them in actual structured datasets.

It’s live on Hugging Face here:  
**https://huggingface.co/StudentOne/Nifty50GPT-Final**

Would love feedback if you try it out or have ideas to extend it. Cheers.",2025-04-14 00:15:18,12,3,MachineLearning,post,"[D] Just open-sourced a financial LLM trained on 10 years of Indian market data — outputs SQL you can run on DuckDB
Hey folks,

Wanted to share something I’ve been building over the past few weeks — a small open-source project that’s been a grind to get right.

I fine-tuned a transformer model on structured Indian stock market data — fundamentals, OHLCV, and index data — across 10+ years. The model outputs SQL queries in response to natural language questions like:

- “What was the net_profit of INFY on 2021-03-31?”
- “What’s the 30-day moving average of TCS close price on 2023-02-01?”
- “Show me YoY growth of EPS for RELIANCE.”

It’s 100% offline — no APIs, no cloud calls — and ships with a DuckDB file preloaded with the dataset. You can paste the model’s SQL output into DuckDB and get results instantly. You can even add your own data without changing the schema.

Built this as a proof of concept for how useful small LLMs can be if you ground them in actual structured datasets.

It’s live on Hugging Face here:  
**https://huggingface.co/StudentOne/Nifty50GPT-Final**

Would love feedback if you try it out or have ideas to extend it. Cheers."
1jz819a,Do You Still Use Human Data to Pre-Train Your Models? [D],"Been seeing some debates lately about the data we feed our LLMs during pre-training. It got me thinking, how essential is high-quality human data for that initial, foundational stage anymore? 

 I think we are shifting towards primarily using synthetic data for pre-training. The idea is leveraging generated text at scale to teach models the fundamentals including grammar, syntax,, basic concepts and common patterns. 

Some people are reserving the often expensive data for the fine-tuning phase. 

Are many of you still heavily reliant on human data for pre-training specifically? I'd like to know the reasons why you stick to it. ",2025-04-14 21:44:56,0,10,MachineLearning,post,"Do You Still Use Human Data to Pre-Train Your Models? [D]
Been seeing some debates lately about the data we feed our LLMs during pre-training. It got me thinking, how essential is high-quality human data for that initial, foundational stage anymore? 

 I think we are shifting towards primarily using synthetic data for pre-training. The idea is leveraging generated text at scale to teach models the fundamentals including grammar, syntax,, basic concepts and common patterns. 

Some people are reserving the often expensive data for the fine-tuning phase. 

Are many of you still heavily reliant on human data for pre-training specifically? I'd like to know the reasons why you stick to it. "
1jytgn0,[R] Responsible Data Augmentation with Diffusion Models at ICLRw 2025,"https://preview.redd.it/gjqbse3yarue1.png?width=913&format=png&auto=webp&s=c18e829a5afb3f24fc3074ce8a1078935056ad35

We propose a text-to-image (T2I) data augmentation method, named DiffCoRe-Mix, that computes a set of generative counterparts for a training sample with an explicitly constrained diffusion model that leverages sample-based context and negative prompting for a reliable augmentation sample generation. To preserve key semantic axes, we also filter out undesired generative samples in our augmentation process. To that end, we propose a hard-cosine filtration in the embedding space of CLIP. Our approach systematically mixes the natural and generative images at pixel and patch levels. We extensively evaluate our technique on ImageNet-1K, Tiny ImageNet-200, CIFAR-100, Flowers102, CUB-Birds, Stanford Cars, and Caltech datasets, demonstrating a notable increase in performance across the board, achieving up to ∼3 absolute gain for top-1 accuracy over the state-of-the-art methods, while showing comparable computational overhead.

arXiV: [https://www.arxiv.org/pdf/2503.10687](https://www.arxiv.org/pdf/2503.10687) 

Code: [https://github.com/khawar-islam/DiffCoRe-Mix](https://github.com/khawar-islam/DiffCoRe-Mix) ",2025-04-14 09:55:04,2,0,MachineLearning,post,"[R] Responsible Data Augmentation with Diffusion Models at ICLRw 2025
https://preview.redd.it/gjqbse3yarue1.png?width=913&format=png&auto=webp&s=c18e829a5afb3f24fc3074ce8a1078935056ad35

We propose a text-to-image (T2I) data augmentation method, named DiffCoRe-Mix, that computes a set of generative counterparts for a training sample with an explicitly constrained diffusion model that leverages sample-based context and negative prompting for a reliable augmentation sample generation. To preserve key semantic axes, we also filter out undesired generative samples in our augmentation process. To that end, we propose a hard-cosine filtration in the embedding space of CLIP. Our approach systematically mixes the natural and generative images at pixel and patch levels. We extensively evaluate our technique on ImageNet-1K, Tiny ImageNet-200, CIFAR-100, Flowers102, CUB-Birds, Stanford Cars, and Caltech datasets, demonstrating a notable increase in performance across the board, achieving up to ∼3 absolute gain for top-1 accuracy over the state-of-the-art methods, while showing comparable computational overhead.

arXiV: [https://www.arxiv.org/pdf/2503.10687](https://www.arxiv.org/pdf/2503.10687) 

Code: [https://github.com/khawar-islam/DiffCoRe-Mix](https://github.com/khawar-islam/DiffCoRe-Mix) "
1jz4p54,[D] What if we paused and resumed LLMs like OS processes?,"We’ve been exploring whether transformer models can be treated more like processes than static deployments. After warm-up, we snapshot the full runtime state to disk, including weights, KV cache, layout—and restore it in about 2 to 5 seconds. This allows us to pause and resume models on demand instead of keeping them loaded continuously.

So far this has enabled:

• Dozens of models running per GPU without idle time
• Dynamic agent stacks that load tools or fine-tunes only when needed
• Local fine-tuning jobs squeezed into idle windows

Feels a bit like OS-level scheduling, but applied to model lifecycles. Curious if anyone else has tested similar ideas, or if this overlaps with approaches you’re trying in local or scaled settings.",2025-04-14 19:31:47,0,7,MachineLearning,post,"[D] What if we paused and resumed LLMs like OS processes?
We’ve been exploring whether transformer models can be treated more like processes than static deployments. After warm-up, we snapshot the full runtime state to disk, including weights, KV cache, layout—and restore it in about 2 to 5 seconds. This allows us to pause and resume models on demand instead of keeping them loaded continuously.

So far this has enabled:

• Dozens of models running per GPU without idle time
• Dynamic agent stacks that load tools or fine-tunes only when needed
• Local fine-tuning jobs squeezed into idle windows

Feels a bit like OS-level scheduling, but applied to model lifecycles. Curious if anyone else has tested similar ideas, or if this overlaps with approaches you’re trying in local or scaled settings."
1jy5w5l,[P] TikTok BrainRot Generator Update,"Not too long ago, I made a brain rot generator that utilizes Motu Hira's Wav2Vec2 algorithm for force alignment and it got some traction (https://www.reddit.com/r/MachineLearning/comments/1hlgdyw/p\_i\_made\_a\_tiktok\_brain\_rot\_video\_generator/) 

  
This time, I made some updates to the brain rot generator, together with Vidhu who has personally reached out to me to help me with this project. 

  
\- Threads suggestions. (Now, if you do not know what to suggest, you can let an LLM to suggest for you aka Groq 70b Llama together with VADER sentiment)

\- Image overlay. (This was done using an algorithm which showed the timestamp, similar to the audio for force alignment but done using image instead)

\- Dockerization support (It now supports dockerisation)

\- Web App (For easy usage, I have also made a web app that makes it easy to toggle between features)

\- Major bug fixed (Thanks to Vidhu for identifying and fixing the bug which prevented people from using the repo)

  
Here is the github: [https://github.com/harvestingmoon/OBrainRot](https://github.com/harvestingmoon/OBrainRot)

  
If you have any questions, please let me know :)",2025-04-13 13:51:13,41,6,MachineLearning,post,"[P] TikTok BrainRot Generator Update
Not too long ago, I made a brain rot generator that utilizes Motu Hira's Wav2Vec2 algorithm for force alignment and it got some traction (https://www.reddit.com/r/MachineLearning/comments/1hlgdyw/p\_i\_made\_a\_tiktok\_brain\_rot\_video\_generator/) 

  
This time, I made some updates to the brain rot generator, together with Vidhu who has personally reached out to me to help me with this project. 

  
\- Threads suggestions. (Now, if you do not know what to suggest, you can let an LLM to suggest for you aka Groq 70b Llama together with VADER sentiment)

\- Image overlay. (This was done using an algorithm which showed the timestamp, similar to the audio for force alignment but done using image instead)

\- Dockerization support (It now supports dockerisation)

\- Web App (For easy usage, I have also made a web app that makes it easy to toggle between features)

\- Major bug fixed (Thanks to Vidhu for identifying and fixing the bug which prevented people from using the repo)

  
Here is the github: [https://github.com/harvestingmoon/OBrainRot](https://github.com/harvestingmoon/OBrainRot)

  
If you have any questions, please let me know :)"
1jys6sr,[P] Rust binary and library crate for semantic code retrieval,,2025-04-14 08:21:54,1,1,MachineLearning,post,"[P] Rust binary and library crate for semantic code retrieval
"
1jy9hp7,[D]Kaggle competition is it worthwhile for PhD student ?,Not sure if this is a dumb question. Is Kaggle competition currently still worthwhile for PhD student in engineering area or computer science field ?,2025-04-13 16:55:13,16,25,MachineLearning,post,"[D]Kaggle competition is it worthwhile for PhD student ?
Not sure if this is a dumb question. Is Kaggle competition currently still worthwhile for PhD student in engineering area or computer science field ?"
1jyp3g3,[Project] anyone needs compute for their passion AI projects?,"So I have 4 A100s, waiting to brrrrr.... I have some projects of mine going on, but I have some compute to spare. If anyone is interested, pitch me your idea and we can get something rolling for you",2025-04-14 05:06:05,0,4,MachineLearning,post,"[Project] anyone needs compute for their passion AI projects?
So I have 4 A100s, waiting to brrrrr.... I have some projects of mine going on, but I have some compute to spare. If anyone is interested, pitch me your idea and we can get something rolling for you"
1jy5ue4,[D] How do you manage experiments with ML models at work?,"I'm doing my master thesis at a company that doesn't do a lot of experimentation on AI models, and definitely nothing much systematic, so when I started I decided to first implement what came to be my ""standard"" project structure (ccds with Hydra and MLFlow). It took me some time to write everything I needed, set up configuration files etc. and that's not to say anything  of managing to store plots, visualising them or even any form of orchestration (outside my scope anyway).

I've done the same in university research projects and schoolwork, so since I didn't have a budget and wanted to learn I just went with implementing everything myself. Still, this seems too much effort if you do have a budget.

How are you guys managing experiments? Using some SaaS platform, running open source tools (which?) on-prem, or writing your own little stack and managing that yourselves? ",2025-04-13 13:48:09,15,14,MachineLearning,post,"[D] How do you manage experiments with ML models at work?
I'm doing my master thesis at a company that doesn't do a lot of experimentation on AI models, and definitely nothing much systematic, so when I started I decided to first implement what came to be my ""standard"" project structure (ccds with Hydra and MLFlow). It took me some time to write everything I needed, set up configuration files etc. and that's not to say anything  of managing to store plots, visualising them or even any form of orchestration (outside my scope anyway).

I've done the same in university research projects and schoolwork, so since I didn't have a budget and wanted to learn I just went with implementing everything myself. Still, this seems too much effort if you do have a budget.

How are you guys managing experiments? Using some SaaS platform, running open source tools (which?) on-prem, or writing your own little stack and managing that yourselves? "
1jy4odf,[D] The ML Paradox: When Better Metrics Lead to Worse Outcomes – Have You Faced This?,"Imagine you’ve trained a model that *theoretically* excels by all standard metrics (accuracy, F1-score, AUC-ROC, etc.) but *practically* fails catastrophically in real-world deployment. For example:  

- A medical diagnosis model with 99% accuracy that disproportionately recommends harmful treatments for rare conditions.  
- A self-driving car API that reduces pedestrian collisions in simulations but causes erratic steering in rain, leading to *more* crashes.  
- An NLP chatbot that scores highly on ‘helpfulness’ benchmarks but gives dangerous advice when queried about mental health.  

**The paradox:** Your model is ‘better’ by metrics/research standards, but ‘worse’ ethically, socially, or functionally.  

**Questions:**  
1. Have you encountered this disconnect? Share your story!  
2. How do we reconcile optimization for benchmarks with *real-world impact*?  
3. Should ML prioritizes **metrics** or **outcomes**? Can we even measure the latter?  ",2025-04-13 12:29:45,29,21,MachineLearning,post,"[D] The ML Paradox: When Better Metrics Lead to Worse Outcomes – Have You Faced This?
Imagine you’ve trained a model that *theoretically* excels by all standard metrics (accuracy, F1-score, AUC-ROC, etc.) but *practically* fails catastrophically in real-world deployment. For example:  

- A medical diagnosis model with 99% accuracy that disproportionately recommends harmful treatments for rare conditions.  
- A self-driving car API that reduces pedestrian collisions in simulations but causes erratic steering in rain, leading to *more* crashes.  
- An NLP chatbot that scores highly on ‘helpfulness’ benchmarks but gives dangerous advice when queried about mental health.  

**The paradox:** Your model is ‘better’ by metrics/research standards, but ‘worse’ ethically, socially, or functionally.  

**Questions:**  
1. Have you encountered this disconnect? Share your story!  
2. How do we reconcile optimization for benchmarks with *real-world impact*?  
3. Should ML prioritizes **metrics** or **outcomes**? Can we even measure the latter?  "
1jyaipx,"[R] New Book: ""Mastering Modern Time Series Forecasting"" – A Hands-On Guide to Statistical, ML, and Deep Learning Models in Python","Hi r/MachineLearning  community!

I’m excited to share that my book, *Mastering Modern Time Series Forecasting*, is now available for preorder. on Gumroad. As a data scientist/ML practitione, I wrote this guide to bridge the gap between theory and practical implementation. Here’s what’s inside:

* **Comprehensive coverage**: From traditional statistical models (ARIMA, SARIMA, Prophet) to modern ML/DL approaches (Transformers, N-BEATS, TFT).
* **Python-first approach**: Code examples with `statsmodels`, `scikit-learn`, `PyTorch`, and `Darts`.
* **Real-world focus**: Techniques for handling messy data, feature engineering, and evaluating forecasts.

**Why I wrote this**: After struggling to find resources that balance depth with readability, I decided to compile my learnings (and mistakes!) into a structured guide.

Feedback and reviewers welcome!

",2025-04-13 17:41:14,2,1,MachineLearning,post,"[R] New Book: ""Mastering Modern Time Series Forecasting"" – A Hands-On Guide to Statistical, ML, and Deep Learning Models in Python
Hi r/MachineLearning  community!

I’m excited to share that my book, *Mastering Modern Time Series Forecasting*, is now available for preorder. on Gumroad. As a data scientist/ML practitione, I wrote this guide to bridge the gap between theory and practical implementation. Here’s what’s inside:

* **Comprehensive coverage**: From traditional statistical models (ARIMA, SARIMA, Prophet) to modern ML/DL approaches (Transformers, N-BEATS, TFT).
* **Python-first approach**: Code examples with `statsmodels`, `scikit-learn`, `PyTorch`, and `Darts`.
* **Real-world focus**: Techniques for handling messy data, feature engineering, and evaluating forecasts.

**Why I wrote this**: After struggling to find resources that balance depth with readability, I decided to compile my learnings (and mistakes!) into a structured guide.

Feedback and reviewers welcome!

"
1jyf84w,[R] GitHub: RBFleX-NAS (Training-Free Neural Architecture Search),RBFleX-NAS is a novel training-free NAS framework that accounts for both activation outputs and input features of the last layer with a Radial Basis Function (RBF) kernel.,2025-04-13 21:02:48,1,0,MachineLearning,post,"[R] GitHub: RBFleX-NAS (Training-Free Neural Architecture Search)
RBFleX-NAS is a novel training-free NAS framework that accounts for both activation outputs and input features of the last layer with a Radial Basis Function (RBF) kernel."
1jyo8fd,[D] First-time arXiv submitter: Need endorsement for cs.AI,"Hi everyone,

I'm submitting my first paper to arXiv in the [cs.AI](http://cs.AI) category and need an endorsement to proceed.

If you've submitted 3+ arXiv papers in [cs.AI](http://cs.AI) or related categories within the last 5 years, I'd be deeply grateful if you could endorse me.

My arXiv username: yuheejang  

Endorsement code: K3LTTO  

Endorsement link: [https://arxiv.org/auth/endorse?x=K3LTTO](https://arxiv.org/auth/endorse?x=K3LTTO)

The paper is a case study on ChatGPT's fallback loop resolution through user-induced meta-feedback, and I'd love to share it once it’s up.

Thanks so much for your time and support 🙏

",2025-04-14 04:18:07,0,0,MachineLearning,post,"[D] First-time arXiv submitter: Need endorsement for cs.AI
Hi everyone,

I'm submitting my first paper to arXiv in the [cs.AI](http://cs.AI) category and need an endorsement to proceed.

If you've submitted 3+ arXiv papers in [cs.AI](http://cs.AI) or related categories within the last 5 years, I'd be deeply grateful if you could endorse me.

My arXiv username: yuheejang  

Endorsement code: K3LTTO  

Endorsement link: [https://arxiv.org/auth/endorse?x=K3LTTO](https://arxiv.org/auth/endorse?x=K3LTTO)

The paper is a case study on ChatGPT's fallback loop resolution through user-induced meta-feedback, and I'd love to share it once it’s up.

Thanks so much for your time and support 🙏

"
1jyc9mc,[D] Rethinking DoD SBIRs for the Modern AI Era: An Insider's Perspective,"*This article reflects the perspective of a PhD-level researcher with two decades of hands-on experience in applied AI/ML and signal processing, primarily focused on U.S. defense applications. The author has worked as both a technical contributor and leader within organizations deeply involved in DoD R&D contracting, providing an insider's view on innovation pipelines and their real-world effectiveness.*

**I. Introduction**

The Department of Defense's Small Business Innovation Research (SBIR) program? It's a solid idea on paper. It's all about getting small businesses to cook up innovative solutions for tough defense problems and, you know, actually get those ideas out of the lab and into the field. For years, it's been a decent engine for tech advancements across the board. But here's the thing: Artificial Intelligence and Machine Learning (AI/ML) are moving at warp speed, and it's mostly the big commercial players driving that bus. From where I sit, deep inside the DoD R&D world as a scientist, it's becoming pretty clear that the old SBIR playbook is struggling to keep up in the AI/ML arena. Instead of consistently churning out game-changing, ready-to-go tech, the program often feels more like a specialized handout – a bit of ""welfare for smart folks"" – without the bang for the buck we need to really push the AI envelope in defense.

**II. The Shadow of Big Tech: Foundational Models & Data Dominance**

The real elephant in the room is the sheer scale of the big tech companies. Think Google, Meta, Microsoft, OpenAI. Their data? Massive. Their computing power? Insane. The AI talent they've got? It dwarfs what your typical SBIR recipient – and honestly, a lot of the DoD itself – can even dream of. Their investments have led to these powerhouse ""foundational models"" – LLMs, computer vision stuff, you name it – that are just miles ahead. And the crazy part? These models aren't just for your social media feed. Turns out, with tricks like transfer learning and few-shot learning, you can adapt these externally trained models incredibly well to specific DoD areas – even super specialized sensor data like MWIR video, SAR, or hyperspectral imagery. Because they've learned so much general stuff, you often just need a relatively small amount of specific data to get state-of-the-art results by tweaking what's already there. This totally changes the game. It makes me wonder: what's the unique, truly innovative space for a small business SBIR project to build core AI models from scratch when these giant, resource-rich players already have such a huge head start?

**III. The 'Off-the-Shelf' Application Trap**

Beyond trying to out-innovate the big guys on core models, a lot of AI/ML SBIR projects stumble into another pitfall: just applying off-the-shelf tech onto a DoD problem. Sure, integrating existing tools can be useful, but you see a worrying number of projects that basically just download pre-built algorithms from places like Hugging Face or PyTorch Hub and apply them to a DoD dataset with barely any changes. It feels less like groundbreaking research and more like decent technical integration. What makes it worse is that you often see a lack of real scientific rigor. For example, literature reviews are often skipped. This means you get people unknowingly reinventing the wheel – a waste of time and taxpayer money. And the pressure to show a demo in those short SBIR phases totally overshadows the need for careful experiments, ablation studies, or really digging deep to understand *why* something works or how to push the boundaries. So, you have to ask: if the main activity is just using existing public tools without real innovation or solid methodology, is that *really* ""Research"" in Small Business Innovation *Research*?

**IV. The 'SBIR Mill': Incentives vs. Transition**

Maybe the most frustrating thing for those of us hoping SBIRs will actually lead to real-world capabilities is how many promising projects just die after Phase II. You've got plenty of small companies that become masters of the SBIR proposal game, raking in Phase I and II awards left and right. But that jump to Phase III – actually getting the tech commercialized or, for the DoD, integrated into a real program – that's where things usually fall apart. The way the system is set up kind of encourages this. Winning the next grant can become the whole business model, rewarding proposal writing skills way more than the hard, uncertain work of turning a prototype into a rugged, tested, and supported product that the warfighter can actually use. This is how you get the ""SBIR mill"" – companies that live off sequential SBIR funding without ever delivering a lasting capability or becoming self-sufficient. Often, they just don't have the systems engineering skills, the manufacturing know-how, or the business development focus to make that transition happen. For example, rarely do i see companies reaching out to industry to sell their ""new tech"" they developed on the SBIR.   When the priority is just getting the next R&D dollar instead of fielding solutions, the program risks becoming that ""welfare system"" I mentioned earlier – keeping smart people employed but not consistently delivering value to the actual end-user.

**V. Conclusion: Rethinking AI SBIRs for Real Impact**

The combination of commercial AI models, the ease of using off-the-shelf tools, and a program that unintentionally rewards grant chasing over actual transition creates a tough environment for the DoD SBIR program in the AI/ML space. While it definitely supports small businesses and keeps technical folks working, you have to seriously question how effective it is at consistently producing the cutting-edge, fieldable AI capabilities the warfighter needs in this new tech landscape. These aren't just complaints; they're honest questions about whether we're using taxpayer money in the most efficient way to achieve real AI/ML superiority. We need to take a hard look at how the SBIR program can adapt. Should the focus shift from trying to create brand new models to critical areas like curating good data, rigorous testing and evaluation, responsible AI, or the tough job of integrating existing top-tier tech into complex defense systems? And how do we make transition a real priority with teeth? If we don't tackle these systemic issues, the DoD risks continuing to fund an AI/ML SBIR engine that looks more like a well-meaning but ultimately inefficient holding pattern.",2025-04-13 18:57:09,1,0,MachineLearning,post,"[D] Rethinking DoD SBIRs for the Modern AI Era: An Insider's Perspective
*This article reflects the perspective of a PhD-level researcher with two decades of hands-on experience in applied AI/ML and signal processing, primarily focused on U.S. defense applications. The author has worked as both a technical contributor and leader within organizations deeply involved in DoD R&D contracting, providing an insider's view on innovation pipelines and their real-world effectiveness.*

**I. Introduction**

The Department of Defense's Small Business Innovation Research (SBIR) program? It's a solid idea on paper. It's all about getting small businesses to cook up innovative solutions for tough defense problems and, you know, actually get those ideas out of the lab and into the field. For years, it's been a decent engine for tech advancements across the board. But here's the thing: Artificial Intelligence and Machine Learning (AI/ML) are moving at warp speed, and it's mostly the big commercial players driving that bus. From where I sit, deep inside the DoD R&D world as a scientist, it's becoming pretty clear that the old SBIR playbook is struggling to keep up in the AI/ML arena. Instead of consistently churning out game-changing, ready-to-go tech, the program often feels more like a specialized handout – a bit of ""welfare for smart folks"" – without the bang for the buck we need to really push the AI envelope in defense.

**II. The Shadow of Big Tech: Foundational Models & Data Dominance**

The real elephant in the room is the sheer scale of the big tech companies. Think Google, Meta, Microsoft, OpenAI. Their data? Massive. Their computing power? Insane. The AI talent they've got? It dwarfs what your typical SBIR recipient – and honestly, a lot of the DoD itself – can even dream of. Their investments have led to these powerhouse ""foundational models"" – LLMs, computer vision stuff, you name it – that are just miles ahead. And the crazy part? These models aren't just for your social media feed. Turns out, with tricks like transfer learning and few-shot learning, you can adapt these externally trained models incredibly well to specific DoD areas – even super specialized sensor data like MWIR video, SAR, or hyperspectral imagery. Because they've learned so much general stuff, you often just need a relatively small amount of specific data to get state-of-the-art results by tweaking what's already there. This totally changes the game. It makes me wonder: what's the unique, truly innovative space for a small business SBIR project to build core AI models from scratch when these giant, resource-rich players already have such a huge head start?

**III. The 'Off-the-Shelf' Application Trap**

Beyond trying to out-innovate the big guys on core models, a lot of AI/ML SBIR projects stumble into another pitfall: just applying off-the-shelf tech onto a DoD problem. Sure, integrating existing tools can be useful, but you see a worrying number of projects that basically just download pre-built algorithms from places like Hugging Face or PyTorch Hub and apply them to a DoD dataset with barely any changes. It feels less like groundbreaking research and more like decent technical integration. What makes it worse is that you often see a lack of real scientific rigor. For example, literature reviews are often skipped. This means you get people unknowingly reinventing the wheel – a waste of time and taxpayer money. And the pressure to show a demo in those short SBIR phases totally overshadows the need for careful experiments, ablation studies, or really digging deep to understand *why* something works or how to push the boundaries. So, you have to ask: if the main activity is just using existing public tools without real innovation or solid methodology, is that *really* ""Research"" in Small Business Innovation *Research*?

**IV. The 'SBIR Mill': Incentives vs. Transition**

Maybe the most frustrating thing for those of us hoping SBIRs will actually lead to real-world capabilities is how many promising projects just die after Phase II. You've got plenty of small companies that become masters of the SBIR proposal game, raking in Phase I and II awards left and right. But that jump to Phase III – actually getting the tech commercialized or, for the DoD, integrated into a real program – that's where things usually fall apart. The way the system is set up kind of encourages this. Winning the next grant can become the whole business model, rewarding proposal writing skills way more than the hard, uncertain work of turning a prototype into a rugged, tested, and supported product that the warfighter can actually use. This is how you get the ""SBIR mill"" – companies that live off sequential SBIR funding without ever delivering a lasting capability or becoming self-sufficient. Often, they just don't have the systems engineering skills, the manufacturing know-how, or the business development focus to make that transition happen. For example, rarely do i see companies reaching out to industry to sell their ""new tech"" they developed on the SBIR.   When the priority is just getting the next R&D dollar instead of fielding solutions, the program risks becoming that ""welfare system"" I mentioned earlier – keeping smart people employed but not consistently delivering value to the actual end-user.

**V. Conclusion: Rethinking AI SBIRs for Real Impact**

The combination of commercial AI models, the ease of using off-the-shelf tools, and a program that unintentionally rewards grant chasing over actual transition creates a tough environment for the DoD SBIR program in the AI/ML space. While it definitely supports small businesses and keeps technical folks working, you have to seriously question how effective it is at consistently producing the cutting-edge, fieldable AI capabilities the warfighter needs in this new tech landscape. These aren't just complaints; they're honest questions about whether we're using taxpayer money in the most efficient way to achieve real AI/ML superiority. We need to take a hard look at how the SBIR program can adapt. Should the focus shift from trying to create brand new models to critical areas like curating good data, rigorous testing and evaluation, responsible AI, or the tough job of integrating existing top-tier tech into complex defense systems? And how do we make transition a real priority with teeth? If we don't tackle these systemic issues, the DoD risks continuing to fund an AI/ML SBIR engine that looks more like a well-meaning but ultimately inefficient holding pattern."
1jxin3q,[N] Google Open to let entreprises self host SOTA models,"From a major player, this sounds like a big shift and would mostly offer enterprises an interesting perspective on data privacy. Mistral is already doing this a lot while OpenAI and Anthropic maintain more closed offerings or through partners.

[https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html](https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html)",2025-04-12 16:33:00,52,3,MachineLearning,post,"[N] Google Open to let entreprises self host SOTA models
From a major player, this sounds like a big shift and would mostly offer enterprises an interesting perspective on data privacy. Mistral is already doing this a lot while OpenAI and Anthropic maintain more closed offerings or through partners.

[https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html](https://www.cnbc.com/2025/04/09/google-will-let-companies-run-gemini-models-in-their-own-data-centers.html)"
1jy4udu,[D] Distributed Clustering using HDBSCAN,"Hello all,

Here's the problem I'm trying to solve. I want to do clustering on a sample having size 1.3 million. The GPU implementation of HDBSCAN is pretty fast and I get the output in 15-30 mins. But around 70% of data is classified as noise. I want to learn a bit more about noise i.e., to which clusters a given noise point is close to. Hence, I tried soft clustering which is already available in the library.

The problem with soft clustering is, it needs significant GPU memory (Number of samples \* number of clusters \* size of float). If number of clusters generated are 10k, it needs around 52 GB GPU memory which is manageable. But my data is expected to grow in the near future which means this solution is not scalable. At this point, I was looking for something distributive and found Distributive DBSCAN. I wanted to implement something similar along those lines using HDBSCAN.

Following is my thought process:

* Divide the data into N partitions using K means so that points which are nearby has a high chance of falling into same partition.
* Perform local clustering for each partition using HDBSCAN
* Take one representative element for each local cluster across all partitions and perform clustering using HDBSCAN on those local representatives (Let's call this global clustering)
* If at least 2 representatives form a cluster in the global clustering, merge the respective local clusters.
* If a point is classified as noise in one of the local clusters. Use approximate predict function to check whether it belongs to one of the clusters in remaining partitions and classify it as belonging to one of the local clusters or noise.
* Finally, we will get a hierarchy of clusters.

If I want to predict a new point keeping the cluster hierarchy constant, I will use approximate predict on all the local cluster models and see if it fits into one of the local clusters.

I'm looking forward to suggestions. Especially while dividing the data using k-means (Might lose some clusters because of this), while merging clusters and classifying local noise.",2025-04-13 12:41:28,1,0,MachineLearning,post,"[D] Distributed Clustering using HDBSCAN
Hello all,

Here's the problem I'm trying to solve. I want to do clustering on a sample having size 1.3 million. The GPU implementation of HDBSCAN is pretty fast and I get the output in 15-30 mins. But around 70% of data is classified as noise. I want to learn a bit more about noise i.e., to which clusters a given noise point is close to. Hence, I tried soft clustering which is already available in the library.

The problem with soft clustering is, it needs significant GPU memory (Number of samples \* number of clusters \* size of float). If number of clusters generated are 10k, it needs around 52 GB GPU memory which is manageable. But my data is expected to grow in the near future which means this solution is not scalable. At this point, I was looking for something distributive and found Distributive DBSCAN. I wanted to implement something similar along those lines using HDBSCAN.

Following is my thought process:

* Divide the data into N partitions using K means so that points which are nearby has a high chance of falling into same partition.
* Perform local clustering for each partition using HDBSCAN
* Take one representative element for each local cluster across all partitions and perform clustering using HDBSCAN on those local representatives (Let's call this global clustering)
* If at least 2 representatives form a cluster in the global clustering, merge the respective local clusters.
* If a point is classified as noise in one of the local clusters. Use approximate predict function to check whether it belongs to one of the clusters in remaining partitions and classify it as belonging to one of the local clusters or noise.
* Finally, we will get a hierarchy of clusters.

If I want to predict a new point keeping the cluster hierarchy constant, I will use approximate predict on all the local cluster models and see if it fits into one of the local clusters.

I'm looking forward to suggestions. Especially while dividing the data using k-means (Might lose some clusters because of this), while merging clusters and classifying local noise."
1jxqtoo,[P] Harmonic Activations: Periodic and Monotonic Function Extensions for Neural Networks (preprint),"Hey folks! I’ve recently released a preprint proposing a new family of activation functions designed for normalization-free deep networks. I’m an independent researcher working on expressive non-linearities for MLPs and Transformers.

**TL;DR:**  
I propose a residual activation function:

`f(x) = x + α · g(sin²(πx / 2))`

where 'g' is an activation function (e.g., GeLU)

I would like to hear feedbacks. This is my first paper.

**Preprint**: [https://doi.org/10.5281/zenodo.15204452]()",2025-04-12 22:39:09,10,5,MachineLearning,post,"[P] Harmonic Activations: Periodic and Monotonic Function Extensions for Neural Networks (preprint)
Hey folks! I’ve recently released a preprint proposing a new family of activation functions designed for normalization-free deep networks. I’m an independent researcher working on expressive non-linearities for MLPs and Transformers.

**TL;DR:**  
I propose a residual activation function:

`f(x) = x + α · g(sin²(πx / 2))`

where 'g' is an activation function (e.g., GeLU)

I would like to hear feedbacks. This is my first paper.

**Preprint**: [https://doi.org/10.5281/zenodo.15204452]()"
1jxeahf,"[R]  d1: Scaling Reasoning in Diffusion Large Language Models
 via Reinforcement Learning",">Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.

Promising results on scaling Diffusion Large Language Models for reasoning tasks using reinforcement learning. Definitely something to keep an eye on when it comes to language models that **actually** reason!

Paper link: https://dllm-reasoning.github.io/media/preprint.pdf",2025-04-12 12:30:15,44,4,MachineLearning,post,"[R]  d1: Scaling Reasoning in Diffusion Large Language Models
 via Reinforcement Learning
>Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.

Promising results on scaling Diffusion Large Language Models for reasoning tasks using reinforcement learning. Definitely something to keep an eye on when it comes to language models that **actually** reason!

Paper link: https://dllm-reasoning.github.io/media/preprint.pdf"
1jxjwi2,[D] “Reasoning Models Don’t Always Say What They Think” – Anyone Got a Prompts?,"Has anyone here tried replicating the results from the **“Reasoning Models Don’t Always Say What They Think”** paper using their own prompts? I'm working on reproducing these outputs. If you’ve experimented with this and fine-tuned your approach, could you share your prompt or any insights you gained along the way? Any discussion or pointers would be greatly appreciated!

For reference, here’s the paper: [Reasoning Models Paper](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)",2025-04-12 17:30:53,15,5,MachineLearning,post,"[D] “Reasoning Models Don’t Always Say What They Think” – Anyone Got a Prompts?
Has anyone here tried replicating the results from the **“Reasoning Models Don’t Always Say What They Think”** paper using their own prompts? I'm working on reproducing these outputs. If you’ve experimented with this and fine-tuned your approach, could you share your prompt or any insights you gained along the way? Any discussion or pointers would be greatly appreciated!

For reference, here’s the paper: [Reasoning Models Paper](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)"
1jws42t,[P] A lightweight open-source model for generating manga,"I posted this on r/StableDiffusion (see some [nice discussion](https://www.reddit.com/r/StableDiffusion/comments/1jv6c6a/a_lightweight_opensource_model_for_generating/)) and someone recommended it'd also fit here.

# TL;DR

I finetuned Pixart-Sigma on 20 million manga images, and I'm making the model weights open-source.  
📦 Download them on Hugging Face: [https://huggingface.co/fumeisama/drawatoon-v1](https://huggingface.co/fumeisama/drawatoon-v1)  
🧪 Try it for free at: [https://drawatoon.com](https://drawatoon.com/)

# Background

I’m an ML engineer who’s always been curious about GenAI, but only got around to experimenting with it a few months ago. I started by trying to generate comics using diffusion models—but I quickly ran into three problems:

* Most models are amazing at photorealistic or anime-style images, but not great for black-and-white, screen-toned panels.
* Character consistency was a nightmare—generating the same character across panels was nearly impossible.
* These models are just too huge for consumer GPUs. There was no way I was running something like a 12B parameter model like Flux on my setup.

So I decided to roll up my sleeves and train my own. Every image in this post was generated using the model I built.

# 🧠 What, How, Why

While I’m new to GenAI, I’m not new to ML. I spent some time catching up—reading papers, diving into open-source repos, and trying to make sense of the firehose of new techniques. It’s a lot. But after some digging, [Pixart-Sigma](https://github.com/PixArt-alpha/PixArt-sigma) stood out: it punches way above its weight and isn’t a nightmare to run.

Finetuning bigger models was out of budget, so I committed to this one. The big hurdle was character consistency. I know the usual solution is to train a LoRA, but honestly, that felt a bit circular—how do I train a LoRA on a new character if I don’t have enough images of that character yet? And also, I need to train a new LoRA for each new character? No, thank you.

I was inspired by [DiffSensei](https://github.com/jianzongwu/DiffSensei) and [Arc2Face](https://arxiv.org/abs/2403.11641) and ended up taking a different route: I used embeddings from a [pre-trained manga character encoder](https://github.com/ragavsachdeva/magi) as conditioning. This means once I generate a character, I can extract its embedding and generate more of that character without training anything. Just drop in the embedding and go.

With that solved, I collected a dataset of \~20 million manga images and finetuned Pixart-Sigma, adding some modifications to allow conditioning on more than just text prompts.

# 🖼️ The End Result

The result is a lightweight manga image generation model that runs smoothly on consumer GPUs and can generate pretty decent black-and-white manga art from text prompts. I can:

* Specify the location of characters and speech bubbles
* Provide reference images to get consistent-looking characters across panels
* Keep the whole thing snappy without needing supercomputers

You can play with it at [https://drawatoon.com](https://drawatoon.com/) or download the model weights and run it locally.

# 🔁 Limitations

So how well does it work?

* Overall, character consistency is surprisingly solid, especially for, hair color and style, facial structure etc. but it still struggles with clothing consistency, especially for detailed or unique outfits, and other accessories. Simple outfits like school uniforms, suits, t-shirts work best. My suggestion is to design your characters to be simple but with different hair colors.
* Struggles with hands. Sigh.
* While it can generate characters consistently, it cannot generate the scenes consistently. You generated a room and want the same room but in a different angle? Can't do it. My hack has been to introduce the scene/setting once on a page and then transition to close-ups of characters so that the background isn't visible or the central focus. I'm sure scene consistency can be solved with img2img or training a ControlNet but I don't have any more money to spend on this.
* Various aspect ratios are supported but each panel has a fixed resolution—262144 pixels.

# 🛣️ Roadmap + What’s Next

There’s still stuff to do.

* ✅ Model weights are open-source on Hugging Face
* 📝 I haven’t written proper usage instructions yet—but if you know how to use PixartSigmaPipeline in diffusers, you’ll be fine. Don't worry, I’ll be writing full setup docs in the next couple of days, so you can run it locally.
* 🙏 If anyone from Comfy or other tooling ecosystems wants to integrate this—please go ahead! I’d love to see it in those pipelines, but I don’t know enough about them to help directly.

Lastly, I built [drawatoon.com](http://drawatoon.com/) so folks can test the model without downloading anything. Since I’m paying for the GPUs out of pocket:

* The server sleeps if no one is using it—so the first image may take a minute or two while it spins up.
* You get 30 images for free. I think this is enough for you to get a taste for whether it's useful for you or not. After that, it’s like 2 cents/image to keep things sustainable (otherwise feel free to just download and run the model locally instead).

Would love to hear your thoughts, feedback, and if you generate anything cool with it—please share!",2025-04-11 17:06:32,177,40,MachineLearning,post,"[P] A lightweight open-source model for generating manga
I posted this on r/StableDiffusion (see some [nice discussion](https://www.reddit.com/r/StableDiffusion/comments/1jv6c6a/a_lightweight_opensource_model_for_generating/)) and someone recommended it'd also fit here.

# TL;DR

I finetuned Pixart-Sigma on 20 million manga images, and I'm making the model weights open-source.  
📦 Download them on Hugging Face: [https://huggingface.co/fumeisama/drawatoon-v1](https://huggingface.co/fumeisama/drawatoon-v1)  
🧪 Try it for free at: [https://drawatoon.com](https://drawatoon.com/)

# Background

I’m an ML engineer who’s always been curious about GenAI, but only got around to experimenting with it a few months ago. I started by trying to generate comics using diffusion models—but I quickly ran into three problems:

* Most models are amazing at photorealistic or anime-style images, but not great for black-and-white, screen-toned panels.
* Character consistency was a nightmare—generating the same character across panels was nearly impossible.
* These models are just too huge for consumer GPUs. There was no way I was running something like a 12B parameter model like Flux on my setup.

So I decided to roll up my sleeves and train my own. Every image in this post was generated using the model I built.

# 🧠 What, How, Why

While I’m new to GenAI, I’m not new to ML. I spent some time catching up—reading papers, diving into open-source repos, and trying to make sense of the firehose of new techniques. It’s a lot. But after some digging, [Pixart-Sigma](https://github.com/PixArt-alpha/PixArt-sigma) stood out: it punches way above its weight and isn’t a nightmare to run.

Finetuning bigger models was out of budget, so I committed to this one. The big hurdle was character consistency. I know the usual solution is to train a LoRA, but honestly, that felt a bit circular—how do I train a LoRA on a new character if I don’t have enough images of that character yet? And also, I need to train a new LoRA for each new character? No, thank you.

I was inspired by [DiffSensei](https://github.com/jianzongwu/DiffSensei) and [Arc2Face](https://arxiv.org/abs/2403.11641) and ended up taking a different route: I used embeddings from a [pre-trained manga character encoder](https://github.com/ragavsachdeva/magi) as conditioning. This means once I generate a character, I can extract its embedding and generate more of that character without training anything. Just drop in the embedding and go.

With that solved, I collected a dataset of \~20 million manga images and finetuned Pixart-Sigma, adding some modifications to allow conditioning on more than just text prompts.

# 🖼️ The End Result

The result is a lightweight manga image generation model that runs smoothly on consumer GPUs and can generate pretty decent black-and-white manga art from text prompts. I can:

* Specify the location of characters and speech bubbles
* Provide reference images to get consistent-looking characters across panels
* Keep the whole thing snappy without needing supercomputers

You can play with it at [https://drawatoon.com](https://drawatoon.com/) or download the model weights and run it locally.

# 🔁 Limitations

So how well does it work?

* Overall, character consistency is surprisingly solid, especially for, hair color and style, facial structure etc. but it still struggles with clothing consistency, especially for detailed or unique outfits, and other accessories. Simple outfits like school uniforms, suits, t-shirts work best. My suggestion is to design your characters to be simple but with different hair colors.
* Struggles with hands. Sigh.
* While it can generate characters consistently, it cannot generate the scenes consistently. You generated a room and want the same room but in a different angle? Can't do it. My hack has been to introduce the scene/setting once on a page and then transition to close-ups of characters so that the background isn't visible or the central focus. I'm sure scene consistency can be solved with img2img or training a ControlNet but I don't have any more money to spend on this.
* Various aspect ratios are supported but each panel has a fixed resolution—262144 pixels.

# 🛣️ Roadmap + What’s Next

There’s still stuff to do.

* ✅ Model weights are open-source on Hugging Face
* 📝 I haven’t written proper usage instructions yet—but if you know how to use PixartSigmaPipeline in diffusers, you’ll be fine. Don't worry, I’ll be writing full setup docs in the next couple of days, so you can run it locally.
* 🙏 If anyone from Comfy or other tooling ecosystems wants to integrate this—please go ahead! I’d love to see it in those pipelines, but I don’t know enough about them to help directly.

Lastly, I built [drawatoon.com](http://drawatoon.com/) so folks can test the model without downloading anything. Since I’m paying for the GPUs out of pocket:

* The server sleeps if no one is using it—so the first image may take a minute or two while it spins up.
* You get 30 images for free. I think this is enough for you to get a taste for whether it's useful for you or not. After that, it’s like 2 cents/image to keep things sustainable (otherwise feel free to just download and run the model locally instead).

Would love to hear your thoughts, feedback, and if you generate anything cool with it—please share!"
1jxbmss,[P] Simple standalone TFRecords dataset reader with Random Access and search-in capabilities,"Hi, at work we are using tfrecords to store most of our datasets. However from time to time. we need to inspect the data to better undestand predictions of our models e.g. to find examples of particular class etc. Since TFRecords are sequential in nature they don't allow for standard random access slicing.

I decided to create this simple tool which allows to create a simple searchable index for tfrecrods which can be used later for various dataset analysis. 

Here is the project page: [https://github.com/kmkolasinski/tfrecords-reader](https://github.com/kmkolasinski/tfrecords-reader)

Features:

* Tensorflow and protobuf packages are not required
* Dataset can be read directly from Google Storage
* Indexing of 1M examples is fast and usually takes couple of seconds
* Polars is used for fast dataset querying `tfrds.select(""select * from index where name ~ 'rose' limit 10"")`

Here is a quick start example from README:

    import tensorflow_datasets as tfds # required only to download dataset
    import tfr_reader as tfr
    from PIL import Image
    import ipyplot
    
    dataset, dataset_info = tfds.load('oxford_flowers102', split='train', with_info=True)
    
    def index_fn(feature: tfr.Feature): # required only for indexing
        label = feature[""label""].value[0]
        return {
            ""label"": label,
            ""name"": dataset_info.features[""label""].int2str(label)
        }
    
    tfrds = tfr.load_from_directory( # loads ds and optionaly build index
        dataset_info.data_dir,
        # indexing options, not required if index is already created
        filepattern=""*.tfrecord*"",
        index_fn=index_fn,
        override=True, # override the index if it exists
    )
    
    # example selection using polars SQL query API
    rows, examples = tfrds.select(""select * from index where name ~ 'rose' limit 10"")
    assert examples == tfrds[rows[""_row_id""]]
    
    samples, names = [], []
    for k, example in enumerate(examples):
        image = Image.open(example[""image""].bytes_io[0]).resize((224, 224))
        names.append(rows[""name""][k])
        samples.append(image)
    
    ipyplot.plot_images(samples, names)",2025-04-12 09:13:28,5,1,MachineLearning,post,"[P] Simple standalone TFRecords dataset reader with Random Access and search-in capabilities
Hi, at work we are using tfrecords to store most of our datasets. However from time to time. we need to inspect the data to better undestand predictions of our models e.g. to find examples of particular class etc. Since TFRecords are sequential in nature they don't allow for standard random access slicing.

I decided to create this simple tool which allows to create a simple searchable index for tfrecrods which can be used later for various dataset analysis. 

Here is the project page: [https://github.com/kmkolasinski/tfrecords-reader](https://github.com/kmkolasinski/tfrecords-reader)

Features:

* Tensorflow and protobuf packages are not required
* Dataset can be read directly from Google Storage
* Indexing of 1M examples is fast and usually takes couple of seconds
* Polars is used for fast dataset querying `tfrds.select(""select * from index where name ~ 'rose' limit 10"")`

Here is a quick start example from README:

    import tensorflow_datasets as tfds # required only to download dataset
    import tfr_reader as tfr
    from PIL import Image
    import ipyplot
    
    dataset, dataset_info = tfds.load('oxford_flowers102', split='train', with_info=True)
    
    def index_fn(feature: tfr.Feature): # required only for indexing
        label = feature[""label""].value[0]
        return {
            ""label"": label,
            ""name"": dataset_info.features[""label""].int2str(label)
        }
    
    tfrds = tfr.load_from_directory( # loads ds and optionaly build index
        dataset_info.data_dir,
        # indexing options, not required if index is already created
        filepattern=""*.tfrecord*"",
        index_fn=index_fn,
        override=True, # override the index if it exists
    )
    
    # example selection using polars SQL query API
    rows, examples = tfrds.select(""select * from index where name ~ 'rose' limit 10"")
    assert examples == tfrds[rows[""_row_id""]]
    
    samples, names = [], []
    for k, example in enumerate(examples):
        image = Image.open(example[""image""].bytes_io[0]).resize((224, 224))
        names.append(rows[""name""][k])
        samples.append(image)
    
    ipyplot.plot_images(samples, names)"
1jx3zy0,[D] Adding new vocab tokens + fine-tuning LLMs to follow instructions is ineffective,"I've been experimenting on instruction-tuning LLMs and VLMs either with adding new specialized tokens to their corresponding tokenizer/processor, or not. The setup is typical: mask the instructions/prompts (only attend to responses/answer) and apply CE loss. Nothing special, standard SFT.

However, I've observed better validation losses and output quality with models trained using their base tokenizer/processor versus models trained with modified tokenizer... Any thoughts on this? Feel free to shed light on this.

(my hunch: it's difficult to increase the likelihood of these new added tokens and the model simply just can't learn it properly).
",2025-04-12 01:42:34,19,19,MachineLearning,post,"[D] Adding new vocab tokens + fine-tuning LLMs to follow instructions is ineffective
I've been experimenting on instruction-tuning LLMs and VLMs either with adding new specialized tokens to their corresponding tokenizer/processor, or not. The setup is typical: mask the instructions/prompts (only attend to responses/answer) and apply CE loss. Nothing special, standard SFT.

However, I've observed better validation losses and output quality with models trained using their base tokenizer/processor versus models trained with modified tokenizer... Any thoughts on this? Feel free to shed light on this.

(my hunch: it's difficult to increase the likelihood of these new added tokens and the model simply just can't learn it properly).
"
1jwxght,[P]We built an OS-like runtime for LLMs — curious if anyone else is doing something similar?,"We’re experimenting with an AI-native runtime that snapshot-loads LLMs (e.g., 13B–65B) in under 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of traditional preloading (like in vLLM or Triton), we serialize GPU execution + memory state and restore models on-demand. This seems to unlock:
	•	Real serverless behavior (no idle cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic workloads

Has anyone tried something similar with multi-model stacks, agent workflows, or dynamic memory reallocation (e.g., via MIG, KAI Scheduler, etc.)? Would love to hear how others are approaching this — or if this even aligns with your infra needs.

Happy to share more technical details if helpful!",2025-04-11 20:50:35,34,20,MachineLearning,post,"[P]We built an OS-like runtime for LLMs — curious if anyone else is doing something similar?
We’re experimenting with an AI-native runtime that snapshot-loads LLMs (e.g., 13B–65B) in under 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of traditional preloading (like in vLLM or Triton), we serialize GPU execution + memory state and restore models on-demand. This seems to unlock:
	•	Real serverless behavior (no idle cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic workloads

Has anyone tried something similar with multi-model stacks, agent workflows, or dynamic memory reallocation (e.g., via MIG, KAI Scheduler, etc.)? Would love to hear how others are approaching this — or if this even aligns with your infra needs.

Happy to share more technical details if helpful!"
1jwz2k3,"[D] Fine-tuned BART for product title & category normalization – still not accurate enough, any better approach?","Hi everyone,
I’m building a price comparison website for products from various online stores in Moldova. I fine-tuned a BART model on a custom dataset of around 20,000 manually normalized product titles, and achieved a loss of 0.013. I also trained a separate model for predicting product categories.

Unfortunately, the results are still not reliable — the model struggles with both product title normalization and category assignment, especially when product names have slight variations or extra keywords.

I don’t have access to SKU numbers from the websites, so matching must be done purely on text.

Is there a better approach or model I might be missing? Or maybe a tool/app that’s designed specifically for this kind of problem?

Thanks in advance!",2025-04-11 21:59:31,11,10,MachineLearning,post,"[D] Fine-tuned BART for product title & category normalization – still not accurate enough, any better approach?
Hi everyone,
I’m building a price comparison website for products from various online stores in Moldova. I fine-tuned a BART model on a custom dataset of around 20,000 manually normalized product titles, and achieved a loss of 0.013. I also trained a separate model for predicting product categories.

Unfortunately, the results are still not reliable — the model struggles with both product title normalization and category assignment, especially when product names have slight variations or extra keywords.

I don’t have access to SKU numbers from the websites, so matching must be done purely on text.

Is there a better approach or model I might be missing? Or maybe a tool/app that’s designed specifically for this kind of problem?

Thanks in advance!"
1jxn5fe,[p] What if you could run 50+ LLMs per GPU — without keeping them in memory?,"We’ve been experimenting with an AI-native runtime that snapshot-loads LLMs (13B–65B) in 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of preloading models (like in vLLM or Triton), we serialize GPU execution state + memory buffers, and restore models on demand even in shared GPU environments where full device access isn’t available.

This seems to unlock:
	•	Real serverless LLM behavior (no idle GPU cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic or dynamic workflows

Curious if others here are exploring similar ideas especially with:
	•	Multi-model/agent stacks
	•	Dynamic GPU memory management (MIG, KAI Scheduler, etc.)
	•	Cuda-checkpoint / partial device access challenges

Happy to share more technical details if helpful.
Would love to exchange notes or hear what pain points you’re seeing with current model serving infra!

For folks curious about updates, breakdowns, or pilot access — I’m sharing more over on X: @InferXai. We’re actively building in the open
",2025-04-12 19:53:52,0,10,MachineLearning,post,"[p] What if you could run 50+ LLMs per GPU — without keeping them in memory?
We’ve been experimenting with an AI-native runtime that snapshot-loads LLMs (13B–65B) in 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of preloading models (like in vLLM or Triton), we serialize GPU execution state + memory buffers, and restore models on demand even in shared GPU environments where full device access isn’t available.

This seems to unlock:
	•	Real serverless LLM behavior (no idle GPU cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic or dynamic workflows

Curious if others here are exploring similar ideas especially with:
	•	Multi-model/agent stacks
	•	Dynamic GPU memory management (MIG, KAI Scheduler, etc.)
	•	Cuda-checkpoint / partial device access challenges

Happy to share more technical details if helpful.
Would love to exchange notes or hear what pain points you’re seeing with current model serving infra!

For folks curious about updates, breakdowns, or pilot access — I’m sharing more over on X: @InferXai. We’re actively building in the open
"
1jwqrud,[R] CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers,"https://arxiv.org/abs/2504.06704
CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and introduces no heavier operations, resulting in consistent accuracy improvements and about a 10% speedup in naive PyTorch implementations on large-scale benchmarks such as ImageNet-1k and WikiText-103.",2025-04-11 16:08:26,12,0,MachineLearning,post,"[R] CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers
https://arxiv.org/abs/2504.06704
CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and introduces no heavier operations, resulting in consistent accuracy improvements and about a 10% speedup in naive PyTorch implementations on large-scale benchmarks such as ImageNet-1k and WikiText-103."
1jwpgov,[P] Building a Classifier for Time Series Forecasting,"**Hey everyone!**  
I want to build a classifier that can automatically select the best forecasting model for a given **univariate time series**, based on which one results in the **lowest MAPE (Mean Absolute Percentage Error)**.  
Does anyone have suggestions or experience on how to approach this kind of problem?

I need this for a college project, I dont seem to understand it. Can anyone point me in right direction?  
I know ARIMA, LSTM, Exponential Smoothening are some models. But how do I train a classifier that choose among them based on MAPE.",2025-04-11 15:09:00,5,5,MachineLearning,post,"[P] Building a Classifier for Time Series Forecasting
**Hey everyone!**  
I want to build a classifier that can automatically select the best forecasting model for a given **univariate time series**, based on which one results in the **lowest MAPE (Mean Absolute Percentage Error)**.  
Does anyone have suggestions or experience on how to approach this kind of problem?

I need this for a college project, I dont seem to understand it. Can anyone point me in right direction?  
I know ARIMA, LSTM, Exponential Smoothening are some models. But how do I train a classifier that choose among them based on MAPE."
1jwp3iv,[D] Anyone having experience working with GRF (Google Research Football) Environment?,I'm basically facing severe issues while working with GRF. I was wondering if there was someone who's experienced and could guide me through them. ,2025-04-11 14:51:36,2,0,MachineLearning,post,"[D] Anyone having experience working with GRF (Google Research Football) Environment?
I'm basically facing severe issues while working with GRF. I was wondering if there was someone who's experienced and could guide me through them. "
1jw3b9b,[P] B200 vs H100 Benchmarks: Early Tests Show Up to 57% Faster Training Throughput & Self-Hosting Cost Analysis,"We at Lightly AI recently got early access to Nvidia B200 GPUs in Europe and ran some independent benchmarks comparing them against H100s, focusing on computer vision model training workloads. We wanted to share the key results as they might be relevant for hardware planning and cost modeling.

**TL;DR / Key Findings:**

* **Training Performance:** Observed up to **57% higher training throughput** with the B200 compared to the H100 on the specific CV tasks we tested.
* **Cost Perspective (Self-Hosted):** Our analysis suggests self-hosted B200s could offer significantly lower OpEx/GPU/hour compared to typical cloud H100 instances (we found a potential range of **\~6x-30x cheaper**, details/assumptions in the post). This obviously depends heavily on utilization, energy costs, and amortization.
* **Setup:** All tests were conducted on our own hardware cluster hosted at GreenMountain, a data center running on 100% renewable energy.

The full blog post contains more details on the specific models trained, batch sizes, methodology, performance charts, and a breakdown of the cost considerations:

[https://www.lightly.ai/blog/nvidia-b200-vs-h100](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.lightly.ai%2Fblog%2Fnvidia-b200-vs-h100)

We thought these early, real-world numbers comparing the new generation might be useful for the community. Happy to discuss the methodology, results, or our experience with the new hardware in the comments!",2025-04-10 19:18:35,68,5,MachineLearning,post,"[P] B200 vs H100 Benchmarks: Early Tests Show Up to 57% Faster Training Throughput & Self-Hosting Cost Analysis
We at Lightly AI recently got early access to Nvidia B200 GPUs in Europe and ran some independent benchmarks comparing them against H100s, focusing on computer vision model training workloads. We wanted to share the key results as they might be relevant for hardware planning and cost modeling.

**TL;DR / Key Findings:**

* **Training Performance:** Observed up to **57% higher training throughput** with the B200 compared to the H100 on the specific CV tasks we tested.
* **Cost Perspective (Self-Hosted):** Our analysis suggests self-hosted B200s could offer significantly lower OpEx/GPU/hour compared to typical cloud H100 instances (we found a potential range of **\~6x-30x cheaper**, details/assumptions in the post). This obviously depends heavily on utilization, energy costs, and amortization.
* **Setup:** All tests were conducted on our own hardware cluster hosted at GreenMountain, a data center running on 100% renewable energy.

The full blog post contains more details on the specific models trained, batch sizes, methodology, performance charts, and a breakdown of the cost considerations:

[https://www.lightly.ai/blog/nvidia-b200-vs-h100](https://www.google.com/url?sa=E&q=https%3A%2F%2Fwww.lightly.ai%2Fblog%2Fnvidia-b200-vs-h100)

We thought these early, real-world numbers comparing the new generation might be useful for the community. Happy to discuss the methodology, results, or our experience with the new hardware in the comments!"
1jvrk68,[D] Yann LeCun Auto-Regressive LLMs are Doomed,"[Yann LeCun at Josiah Willard Gibbs Lecture \(2025\)](https://preview.redd.it/wbhfwlibeyte1.png?width=1942&format=png&auto=webp&s=d885e1d58806b76c875c9d48ca0e8cf96a99b485)

Not sure who else agrees, but I think Yann LeCun raises an interesting point here. Curious to hear other opinions on this!

  
Lecture link: [https://www.youtube.com/watch?v=ETZfkkv6V7Y](https://www.youtube.com/watch?v=ETZfkkv6V7Y)",2025-04-10 08:44:27,339,141,MachineLearning,post,"[D] Yann LeCun Auto-Regressive LLMs are Doomed
[Yann LeCun at Josiah Willard Gibbs Lecture \(2025\)](https://preview.redd.it/wbhfwlibeyte1.png?width=1942&format=png&auto=webp&s=d885e1d58806b76c875c9d48ca0e8cf96a99b485)

Not sure who else agrees, but I think Yann LeCun raises an interesting point here. Curious to hear other opinions on this!

  
Lecture link: [https://www.youtube.com/watch?v=ETZfkkv6V7Y](https://www.youtube.com/watch?v=ETZfkkv6V7Y)"
1jw1i4b,[P] A slop forensics toolkit for LLMs: computing over-represented lexical profiles and inferring similarity trees,"Releasing a few tools around LLM slop (over-represented words & phrases).

It uses stylometric analysis to surface repetitive words & n-grams which occur more often in LLM output compared to human writing.

Also borrowing some bioinformatics tools to infer similarity trees from these slop profiles, treating the presence/absence of lexical features as ""mutations"" to infer relationships.

\- compute a ""slop profile"" of over-represented words & phrases for your model

\- uses bioinformatics tools to infer similarity trees

\- builds canonical slop phrase lists

Github repo: [https://github.com/sam-paech/slop-forensics](https://github.com/sam-paech/slop-forensics)

Notebook: [https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing](https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing)",2025-04-10 18:02:50,53,4,MachineLearning,post,"[P] A slop forensics toolkit for LLMs: computing over-represented lexical profiles and inferring similarity trees
Releasing a few tools around LLM slop (over-represented words & phrases).

It uses stylometric analysis to surface repetitive words & n-grams which occur more often in LLM output compared to human writing.

Also borrowing some bioinformatics tools to infer similarity trees from these slop profiles, treating the presence/absence of lexical features as ""mutations"" to infer relationships.

\- compute a ""slop profile"" of over-represented words & phrases for your model

\- uses bioinformatics tools to infer similarity trees

\- builds canonical slop phrase lists

Github repo: [https://github.com/sam-paech/slop-forensics](https://github.com/sam-paech/slop-forensics)

Notebook: [https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing](https://colab.research.google.com/drive/1SQfnHs4wh87yR8FZQpsCOBL5h5MMs8E6?usp=sharing)"
1jww7nn,[P] Sub-2s cold starts for 13B+ LLMs + 50+ models per GPU — curious how others are tackling orchestration?,"We’re experimenting with an AI-native runtime that snapshot-loads LLMs (e.g., 13B–65B) in under 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of traditional preloading (like in vLLM or Triton), we serialize GPU execution + memory state and restore models on-demand. This seems to unlock:
	•	Real serverless behavior (no idle cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic workloads

Has anyone tried something similar with multi-model stacks, agent workflows, or dynamic memory reallocation (e.g., via MIG, KAI Scheduler, etc.)? Would love to hear how others are approaching this — or if this even aligns with your infra needs.

Happy to share more technical details if helpful!",2025-04-11 19:58:33,0,0,MachineLearning,post,"[P] Sub-2s cold starts for 13B+ LLMs + 50+ models per GPU — curious how others are tackling orchestration?
We’re experimenting with an AI-native runtime that snapshot-loads LLMs (e.g., 13B–65B) in under 2–5 seconds and dynamically runs 50+ models per GPU — without keeping them always resident in memory.

Instead of traditional preloading (like in vLLM or Triton), we serialize GPU execution + memory state and restore models on-demand. This seems to unlock:
	•	Real serverless behavior (no idle cost)
	•	Multi-model orchestration at low latency
	•	Better GPU utilization for agentic workloads

Has anyone tried something similar with multi-model stacks, agent workflows, or dynamic memory reallocation (e.g., via MIG, KAI Scheduler, etc.)? Would love to hear how others are approaching this — or if this even aligns with your infra needs.

Happy to share more technical details if helpful!"
1jw79pp,Previewing parquet directly from the OS [Discussion],"Hi!

I've worked with Parquet for years at this point and it's my favorite format by far for data work.

Nothing beats it. It compresses super well, fast as hell, maintains a schema, and doesn't corrupt data (I'm looking at you Excel & CSV). but...

It's impossible to view without some code / CLI. Super annoying, especially if you need to peek at what you're doing before starting some analyse. Or frankly just debugging an output dataset.

This has been my biggest pet peeve for the last 6 years of my life. So I've fixed it haha.

The image below shows you how you can quick view a parquet file from directly within the operating system. Works across different apps that support previewing, etc. Also, no size limit (because it's a preview obviously)

I believe strongly that the data space has been neglected on the UI & continuity front. Something that video, for example, doesn't face.

I'm planning on adding other formats commonly used in Data Science / Machine Learning.

Like:

\- Partitioned Directories *( this is pretty tricky )*

\- HDF5

\- Avro

\- ORC

\- Feather

\- JSON Lines

\- DuckDB (.db)

\- SQLLite (.db)

\- Formats above, but directly from S3 / GCS without going to the console.

Any other format I should add?

Let me know what you think!

https://i.redd.it/lapf15vkc2ue1.gif",2025-04-10 22:01:52,17,5,MachineLearning,post,"Previewing parquet directly from the OS [Discussion]
Hi!

I've worked with Parquet for years at this point and it's my favorite format by far for data work.

Nothing beats it. It compresses super well, fast as hell, maintains a schema, and doesn't corrupt data (I'm looking at you Excel & CSV). but...

It's impossible to view without some code / CLI. Super annoying, especially if you need to peek at what you're doing before starting some analyse. Or frankly just debugging an output dataset.

This has been my biggest pet peeve for the last 6 years of my life. So I've fixed it haha.

The image below shows you how you can quick view a parquet file from directly within the operating system. Works across different apps that support previewing, etc. Also, no size limit (because it's a preview obviously)

I believe strongly that the data space has been neglected on the UI & continuity front. Something that video, for example, doesn't face.

I'm planning on adding other formats commonly used in Data Science / Machine Learning.

Like:

\- Partitioned Directories *( this is pretty tricky )*

\- HDF5

\- Avro

\- ORC

\- Feather

\- JSON Lines

\- DuckDB (.db)

\- SQLLite (.db)

\- Formats above, but directly from S3 / GCS without going to the console.

Any other format I should add?

Let me know what you think!

https://i.redd.it/lapf15vkc2ue1.gif"
1jwv8qp,[Project] I created a crop generator that you might want to use.,"Hello everyone, I created a python based crop generator that helps me with my image datasets.

[https://github.com/fegarza7/CropGenerator](https://github.com/fegarza7/CropGenerator)

I am training SDXL models to recognize features and concepts and I just couldn't find a quick tool to do this (or didn't look for it enough).

My specific use case is that I have images that are big and some are somewhat small, and I need to select specific features, some are very small and I was getting very blurry images when I created a 1:1 crop of a specific zoomed feature.

This script uses your JSONL to find the center of the bounding box and export the image in the resolution you need (8px based) and upscales/denoises them to create 1:1 crops that you can use to train your model, it also creates a metadata.csv with the file\_name and the description from your JSONL.

I essentially run this on my raw images folder, and it creates a new folder with the cropped images, the metadata.csv (containing the filename and the description) and I'm ready to train very fast.

Of course you need to first create your JSONL file with all the bounding boxes and I already have that light HTML script but right now I don't have the time to make it less specific to my case use and I'm sure I can improve it a bit, I will update the repo once I have it.

Hopefully you can use this in your training, refork, suggest changes etc..",2025-04-11 19:17:49,0,8,MachineLearning,post,"[Project] I created a crop generator that you might want to use.
Hello everyone, I created a python based crop generator that helps me with my image datasets.

[https://github.com/fegarza7/CropGenerator](https://github.com/fegarza7/CropGenerator)

I am training SDXL models to recognize features and concepts and I just couldn't find a quick tool to do this (or didn't look for it enough).

My specific use case is that I have images that are big and some are somewhat small, and I need to select specific features, some are very small and I was getting very blurry images when I created a 1:1 crop of a specific zoomed feature.

This script uses your JSONL to find the center of the bounding box and export the image in the resolution you need (8px based) and upscales/denoises them to create 1:1 crops that you can use to train your model, it also creates a metadata.csv with the file\_name and the description from your JSONL.

I essentially run this on my raw images folder, and it creates a new folder with the cropped images, the metadata.csv (containing the filename and the description) and I'm ready to train very fast.

Of course you need to first create your JSONL file with all the bounding boxes and I already have that light HTML script but right now I don't have the time to make it less specific to my case use and I'm sure I can improve it a bit, I will update the repo once I have it.

Hopefully you can use this in your training, refork, suggest changes etc.."
1jwg9fj,[D] Dynamic patch weighting in ViTs,"Has anyone explored weighting non-overlapping patches in images using ViTs? The weights would be part of learnable parameters. For instance, the background patches are sometimes useless for an image classification task. I am hypothesising that including this as a part of image embedding might be adding noise.

It would be great if someone could point me to some relevant works.",2025-04-11 05:14:59,5,8,MachineLearning,post,"[D] Dynamic patch weighting in ViTs
Has anyone explored weighting non-overlapping patches in images using ViTs? The weights would be part of learnable parameters. For instance, the background patches are sometimes useless for an image classification task. I am hypothesising that including this as a part of image embedding might be adding noise.

It would be great if someone could point me to some relevant works."
1jw28iw,[D] Thoughts about ICASSP 2025,"There were a lot of issues in visas so half of the poster boards were empty and in 2 sessions I attended were just videos playing. Why visa issues are there in conferences?

I got my paper in CVPR 23 but couldn't go because canadian government thought I would leave my PhD and stay there. 

I hope in future countries start to go easy on researchers",2025-04-10 18:33:40,29,9,MachineLearning,post,"[D] Thoughts about ICASSP 2025
There were a lot of issues in visas so half of the poster boards were empty and in 2 sessions I attended were just videos playing. Why visa issues are there in conferences?

I got my paper in CVPR 23 but couldn't go because canadian government thought I would leave my PhD and stay there. 

I hope in future countries start to go easy on researchers"
1jvz48g,[D] Is research on discrete sampling / MCMC useful in industry? Feeling unsure.,"Hi all,

I’m currently a 2nd year PhD student in CS at a top 20 school. My research focuses on discrete sampling — designing MCMC-based algorithms for inference and generation over discrete spaces. While I find this area intellectually exciting and core to probabilistic machine learning, I’m starting to worry about its industry relevance.



To be honest, I don’t see many companies actively hiring for roles that focus on sampling algorithms in discrete spaces. Meanwhile, I see a lot of buzz and job openings around reinforcement learning, bandits, and active learning — areas that my department unfortunately doesn’t focus on.



This has left me feeling a bit anxious:

• Is discrete sampling considered valuable in the industry (esp. outside of research labs)?

• Does it translate well to real-world ML/AI systems?

• Should I pivot toward something more “applied” or “sexy” like RL, causality, etc.?



I’d love to hear from anyone working in industry or hiring PhDs — is this line of work appreciated? Would love any advice or perspective.



Thanks in advance!",2025-04-10 16:21:40,33,26,MachineLearning,post,"[D] Is research on discrete sampling / MCMC useful in industry? Feeling unsure.
Hi all,

I’m currently a 2nd year PhD student in CS at a top 20 school. My research focuses on discrete sampling — designing MCMC-based algorithms for inference and generation over discrete spaces. While I find this area intellectually exciting and core to probabilistic machine learning, I’m starting to worry about its industry relevance.



To be honest, I don’t see many companies actively hiring for roles that focus on sampling algorithms in discrete spaces. Meanwhile, I see a lot of buzz and job openings around reinforcement learning, bandits, and active learning — areas that my department unfortunately doesn’t focus on.



This has left me feeling a bit anxious:

• Is discrete sampling considered valuable in the industry (esp. outside of research labs)?

• Does it translate well to real-world ML/AI systems?

• Should I pivot toward something more “applied” or “sexy” like RL, causality, etc.?



I’d love to hear from anyone working in industry or hiring PhDs — is this line of work appreciated? Would love any advice or perspective.



Thanks in advance!"
1jvv7s8,[P] [R] [D] I built a biomedical GNN + LLM pipeline (XplainMD) for explainable multi-link prediction,"Hi everyone,

I'm an independent researcher and recently finished building **XplainMD**, an end-to-end explainable AI pipeline for biomedical knowledge graphs. It’s designed to predict and *explain* multiple biomedical connections like drug–disease or gene–phenotype relationships using a blend of graph learning and large language models.

# What it does:

* Uses **R-GCN** for multi-relational link prediction on **PrimeKG(precision medicine knowledge graph)**
* Utilises **GNNExplainer** for model interpretability
* Visualises subgraphs of model predictions with **PyVis**
* Explains model predictions using **LLaMA 3.1 8B** instruct for sanity check and natural language explanation
* Deployed in an interactive **Gradio app**

# 🚀 Why I built it:

I wanted to create something that goes beyond prediction and gives researchers a way to **understand the ""why""** behind a model’s decision—especially in sensitive fields like precision medicine.

# 🧰 Tech Stack:

`PyTorch Geometric` • `GNNExplainer` • `LLaMA 3.1` • `Gradio` • `PyVis`

Here’s the full repo + write-up:

[https://medium.com/@fhirshotlearning/xplainmd-a-graph-powered-guide-to-smarter-healthcare-fd5fe22504de](https://medium.com/@fhirshotlearning/xplainmd-a-graph-powered-guide-to-smarter-healthcare-fd5fe22504de)

github: [https://github.com/amulya-prasad/XplainMD](https://github.com/amulya-prasad/XplainMD)

Your feedback is highly appreciated!

PS:This is my first time working with graph theory and my knowledge and experience is very limited. But I am eager to learn moving forward and I have a lot to optimise in this project. But through this project I wanted to demonstrate the beauty of graphs and how it can be used to redefine healthcare :)",2025-04-10 13:06:07,44,8,MachineLearning,post,"[P] [R] [D] I built a biomedical GNN + LLM pipeline (XplainMD) for explainable multi-link prediction
Hi everyone,

I'm an independent researcher and recently finished building **XplainMD**, an end-to-end explainable AI pipeline for biomedical knowledge graphs. It’s designed to predict and *explain* multiple biomedical connections like drug–disease or gene–phenotype relationships using a blend of graph learning and large language models.

# What it does:

* Uses **R-GCN** for multi-relational link prediction on **PrimeKG(precision medicine knowledge graph)**
* Utilises **GNNExplainer** for model interpretability
* Visualises subgraphs of model predictions with **PyVis**
* Explains model predictions using **LLaMA 3.1 8B** instruct for sanity check and natural language explanation
* Deployed in an interactive **Gradio app**

# 🚀 Why I built it:

I wanted to create something that goes beyond prediction and gives researchers a way to **understand the ""why""** behind a model’s decision—especially in sensitive fields like precision medicine.

# 🧰 Tech Stack:

`PyTorch Geometric` • `GNNExplainer` • `LLaMA 3.1` • `Gradio` • `PyVis`

Here’s the full repo + write-up:

[https://medium.com/@fhirshotlearning/xplainmd-a-graph-powered-guide-to-smarter-healthcare-fd5fe22504de](https://medium.com/@fhirshotlearning/xplainmd-a-graph-powered-guide-to-smarter-healthcare-fd5fe22504de)

github: [https://github.com/amulya-prasad/XplainMD](https://github.com/amulya-prasad/XplainMD)

Your feedback is highly appreciated!

PS:This is my first time working with graph theory and my knowledge and experience is very limited. But I am eager to learn moving forward and I have a lot to optimise in this project. But through this project I wanted to demonstrate the beauty of graphs and how it can be used to redefine healthcare :)"
1jw8d2x,[D] Best Sentiment Analysis Model for Reddit,"Hello all! My first time posting.

I'm working on a sentiment analysis project focusing on Reddit comments about a war conflict. For this task, I've been using three sentiment analysis tools: **VADER**, **TextBlob**, and **DistilBERT**. However, I'm facing a challenge as the outcomes from these three models often differ significantly.The dataset is quite large, so manual verification of each comment isn't feasible. I’d appreciate any advice on how to approach the issue of achieving the most accurate sentiment results.

* Should I consider combining the scores from these tools? If so, how could I account for the fact that each model's scoring system functions differently?
* Alternatively, would it make sense to rely on majority voting for sentiment labels (e.g., choosing the sentiment that at least two out of three models agree on)?
* Any other approaches or best practices that might work?    

 TIA!!",2025-04-10 22:47:07,4,7,MachineLearning,post,"[D] Best Sentiment Analysis Model for Reddit
Hello all! My first time posting.

I'm working on a sentiment analysis project focusing on Reddit comments about a war conflict. For this task, I've been using three sentiment analysis tools: **VADER**, **TextBlob**, and **DistilBERT**. However, I'm facing a challenge as the outcomes from these three models often differ significantly.The dataset is quite large, so manual verification of each comment isn't feasible. I’d appreciate any advice on how to approach the issue of achieving the most accurate sentiment results.

* Should I consider combining the scores from these tools? If so, how could I account for the fact that each model's scoring system functions differently?
* Alternatively, would it make sense to rely on majority voting for sentiment labels (e.g., choosing the sentiment that at least two out of three models agree on)?
* Any other approaches or best practices that might work?    

 TIA!!"
1jwnxtk,[R] Arxiv Endorsement in CS.AI,"Hi, can anyone endorse me in Arxiv, subfield cs.ai?  
Here is my draft: [https://drive.google.com/file/d/1DCoKPc5JG-isx8ziySzQ\_4IDEcny6Rk\_/view?usp=sharing](https://drive.google.com/file/d/1DCoKPc5JG-isx8ziySzQ_4IDEcny6Rk_/view?usp=sharing)

and here's the endorsement code: [https://arxiv.org/auth/endorse?x=63Q8AR](https://arxiv.org/auth/endorse?x=63Q8AR)

Thanks!",2025-04-11 13:50:52,0,0,MachineLearning,post,"[R] Arxiv Endorsement in CS.AI
Hi, can anyone endorse me in Arxiv, subfield cs.ai?  
Here is my draft: [https://drive.google.com/file/d/1DCoKPc5JG-isx8ziySzQ\_4IDEcny6Rk\_/view?usp=sharing](https://drive.google.com/file/d/1DCoKPc5JG-isx8ziySzQ_4IDEcny6Rk_/view?usp=sharing)

and here's the endorsement code: [https://arxiv.org/auth/endorse?x=63Q8AR](https://arxiv.org/auth/endorse?x=63Q8AR)

Thanks!"
1jw3fku,[P] [D] mcp-use: an open source library that lets you connect LLMs to MCPs from python in 6 lines of code,"Hello all!

I've been really excited to see the recent buzz around MCP and all the cool things people are building with it. Though, the fact that you can use it only through desktop apps really seemed wrong and prevented me for trying most examples, so I wrote a simple client, then I wrapped into some class, and I ended up creating a python package that abstracts some of the async uglyness.

You need:

* one of those MCPconfig JSONs
* 6 lines of code and you can have an agent use the MCP tools from python.

Like this:

https://preview.redd.it/ajx123jgk1ue1.png?width=1080&format=png&auto=webp&s=186f8ffe26dbcd3211de17b43bae03b79be5fc0a

The structure is simple: an MCP client creates and manages the connection and instantiation (if needed) of the server and extracts the available tools. The MCPAgent reads the tools from the client, converts them into callable objects, gives access to them to an LLM, manages tool calls and responses.

It's very early-stage, and I'm sharing it here for feedback, contributions and to share a resource that might be helpful for testing and playing around with MCPs. Let me know what you think! Any suggestions ? 

Repo: [https://github.com/mcp-use/mcp-use](https://github.com/mcp-use/mcp-use) Pipy: [https://pypi.org/project/mcp-use/](https://pypi.org/project/mcp-use/)

Docs: [https://docs.mcp-use.io/introduction](https://docs.mcp-use.io/introduction)

    pip install mcp-use

Happy to answer questions or walk through examples!

Thanks!",2025-04-10 19:23:26,1,0,MachineLearning,post,"[P] [D] mcp-use: an open source library that lets you connect LLMs to MCPs from python in 6 lines of code
Hello all!

I've been really excited to see the recent buzz around MCP and all the cool things people are building with it. Though, the fact that you can use it only through desktop apps really seemed wrong and prevented me for trying most examples, so I wrote a simple client, then I wrapped into some class, and I ended up creating a python package that abstracts some of the async uglyness.

You need:

* one of those MCPconfig JSONs
* 6 lines of code and you can have an agent use the MCP tools from python.

Like this:

https://preview.redd.it/ajx123jgk1ue1.png?width=1080&format=png&auto=webp&s=186f8ffe26dbcd3211de17b43bae03b79be5fc0a

The structure is simple: an MCP client creates and manages the connection and instantiation (if needed) of the server and extracts the available tools. The MCPAgent reads the tools from the client, converts them into callable objects, gives access to them to an LLM, manages tool calls and responses.

It's very early-stage, and I'm sharing it here for feedback, contributions and to share a resource that might be helpful for testing and playing around with MCPs. Let me know what you think! Any suggestions ? 

Repo: [https://github.com/mcp-use/mcp-use](https://github.com/mcp-use/mcp-use) Pipy: [https://pypi.org/project/mcp-use/](https://pypi.org/project/mcp-use/)

Docs: [https://docs.mcp-use.io/introduction](https://docs.mcp-use.io/introduction)

    pip install mcp-use

Happy to answer questions or walk through examples!

Thanks!"
1jv80y4,[D] Has anyone trained LLM on GCP? How long did you wait for H100 approval?,"How long did you guys wait for the quota increase approval for the H100 80gb Gpus? I need to use 8 H100 80GB GPU's for the Llama 4 Maverick, requested today and still waiting. Wondering because for lower amounts on different GPU's the approval was almost instant.",2025-04-09 17:06:08,35,21,MachineLearning,post,"[D] Has anyone trained LLM on GCP? How long did you wait for H100 approval?
How long did you guys wait for the quota increase approval for the H100 80gb Gpus? I need to use 8 H100 80GB GPU's for the Llama 4 Maverick, requested today and still waiting. Wondering because for lower amounts on different GPU's the approval was almost instant."
1jv2zxc,[D] How do you monitor your AI agents or LLM apps?,"I’m curious how others are monitoring and tracking LLM-based apps or AI agents, especially as they get more complex with RAG, tool use, or user input.

Do you track things like:

* Token usage
* Latency
* Error rates
* Prompt version changes ...or any other performance/cost-related metrics?

Do you use a tool for this, or is it mostly something you’ve built yourself?

Would love to hear what’s worked (or not) for you — even lightweight solutions or pain points.",2025-04-09 13:01:08,19,10,MachineLearning,post,"[D] How do you monitor your AI agents or LLM apps?
I’m curious how others are monitoring and tracking LLM-based apps or AI agents, especially as they get more complex with RAG, tool use, or user input.

Do you track things like:

* Token usage
* Latency
* Error rates
* Prompt version changes ...or any other performance/cost-related metrics?

Do you use a tool for this, or is it mostly something you’ve built yourself?

Would love to hear what’s worked (or not) for you — even lightweight solutions or pain points."
1jvndi5,[P] FlexChunk: 100M×100M Out-of-Core SpMV in 1.8min on CPU (~1.7 GB RAM),"Developed a new algorithm **FlexChunk** – a chunk-based out-of-core SpMV approach that multiplies**100M×100M sparse matrices** on CPU in **\~1.8 minutes** using only **\~1.7 GB RAM**.

\+ Near-linear scaling  
\+ Works on regular hardware  
\+ Zero dependencies  
\+ Full demo + benchmarks

Idea: processing sparse matrices by locality-aware adaptive chunking, with minimal memory usage and predictable performance.

* **Demo**: [Hugging Face](https://huggingface.co/spaces/DanielSwift/FlexChunk)
* **Code**: [GitHub](https://github.com/DanielSwift1992/FlexChunk)
* **Article**: [LessWrong](https://www.lesswrong.com/posts/zpRhsdDkWygTDScxb/flexchunk-enabling-100m-100m-out-of-core-spmv-1-8-min-1-7-gb)

❓Struggling to get feedback — any ideas where projects like this are best shared? Or feedback on the approach itself is very welcome. Thanks!",2025-04-10 04:25:31,2,0,MachineLearning,post,"[P] FlexChunk: 100M×100M Out-of-Core SpMV in 1.8min on CPU (~1.7 GB RAM)
Developed a new algorithm **FlexChunk** – a chunk-based out-of-core SpMV approach that multiplies**100M×100M sparse matrices** on CPU in **\~1.8 minutes** using only **\~1.7 GB RAM**.

\+ Near-linear scaling  
\+ Works on regular hardware  
\+ Zero dependencies  
\+ Full demo + benchmarks

Idea: processing sparse matrices by locality-aware adaptive chunking, with minimal memory usage and predictable performance.

* **Demo**: [Hugging Face](https://huggingface.co/spaces/DanielSwift/FlexChunk)
* **Code**: [GitHub](https://github.com/DanielSwift1992/FlexChunk)
* **Article**: [LessWrong](https://www.lesswrong.com/posts/zpRhsdDkWygTDScxb/flexchunk-enabling-100m-100m-out-of-core-spmv-1-8-min-1-7-gb)

❓Struggling to get feedback — any ideas where projects like this are best shared? Or feedback on the approach itself is very welcome. Thanks!"
1jva1nc,[R] Exploring a prime-based 2D grid system as an experimental AI architecture – Feedback welcome,"Hi everyone,

I’ve been working on a conceptual AI architecture inspired by prime number behavior in a 2D grid structure.

By layering vertical patterns based on numerical spacing, we create a grid that filters and stores values based on prime-related behavior. This enables:

Probabilistic deduction

Filtering logic

Memory-like data handling

Multi-layered processing potential


The idea is to treat numbers not just as values, but as containers with mathematical and behavioral properties—usable in logic, memory, and even emotional representation in future AI systems.

It’s an early-stage white paper, but I’d love your thoughts:
[https://drive.google.com/file/d/1FA60YWBGqV6WGfbk64OlirohemJ2RDli/view?usp=sharing]

What do you think about using mathematical pattern grids (like this) as a foundation for alternative AI logic beyond traditional neural networks?

Looking forward to hearing your feedback and ideas.
",2025-04-09 18:29:04,3,1,MachineLearning,post,"[R] Exploring a prime-based 2D grid system as an experimental AI architecture – Feedback welcome
Hi everyone,

I’ve been working on a conceptual AI architecture inspired by prime number behavior in a 2D grid structure.

By layering vertical patterns based on numerical spacing, we create a grid that filters and stores values based on prime-related behavior. This enables:

Probabilistic deduction

Filtering logic

Memory-like data handling

Multi-layered processing potential


The idea is to treat numbers not just as values, but as containers with mathematical and behavioral properties—usable in logic, memory, and even emotional representation in future AI systems.

It’s an early-stage white paper, but I’d love your thoughts:
[https://drive.google.com/file/d/1FA60YWBGqV6WGfbk64OlirohemJ2RDli/view?usp=sharing]

What do you think about using mathematical pattern grids (like this) as a foundation for alternative AI logic beyond traditional neural networks?

Looking forward to hearing your feedback and ideas.
"
1jv89f5,[D] CVPR registration. What's my paper number?,"They ask for a paper number in the CVPR registration website and I am not sure which one it is. Is it the submission id in OpenReview or is it the number in the cvpr list of accepted papers url to my paper?

Thanks!",2025-04-09 17:15:39,1,5,MachineLearning,post,"[D] CVPR registration. What's my paper number?
They ask for a paper number in the CVPR registration website and I am not sure which one it is. Is it the submission id in OpenReview or is it the number in the cvpr list of accepted papers url to my paper?

Thanks!"
1juxjwk,[P] Yin-Yang Classification,"I have been messing around yin-yang data classification and threw it together in a repo.

Link: [https://github.com/mavleo96/yin-yang-classification](https://github.com/mavleo96/yin-yang-classification)

Please do comment your thought and any suggestion on what else might be interesting to visualize here — and feel free to star the repo if it's interesting / helpful.",2025-04-09 06:35:52,11,1,MachineLearning,post,"[P] Yin-Yang Classification
I have been messing around yin-yang data classification and threw it together in a repo.

Link: [https://github.com/mavleo96/yin-yang-classification](https://github.com/mavleo96/yin-yang-classification)

Please do comment your thought and any suggestion on what else might be interesting to visualize here — and feel free to star the repo if it's interesting / helpful."
1juft4t,[D] Synthetic introduction to ML for PhD student in Mathematics,"Hi all,

I'm a about to begin my PhD in Mathematics, and my supervisor current project is to investigate the feasibility of some niche Linear Algebra tools to the setting of Machine Learning, especially PINNs.

I am already very familiar with such niche Linear Algebra results; however I lack any knowledge of ML.

Moreover, I have some knowledge of Measure Theory, Calculus of Probabilities and Statistics.

I skimmed through Bishops's [Pattern Recognition] (https://books.google.it/books/about/Pattern_Recognition_and_Machine_Learning.html?id=kTNoQgAACAAJ&redir_esc=y) and Goodfellows's [Deep Learning](https://www.deeplearningbook.org), and I have found both books to be excessively redundant and verbose.

I do appreciate the abundance of examples and the maieutic approach of these books, however I need to get a theoretical grasp on the subject.  

I am looking for an alternative resource(s) on the subject written with mathematical rigour targeted at graduate students. 

Do you have anything to suggest, be it books, lecture notes or video lectures?",2025-04-08 17:06:02,47,13,MachineLearning,post,"[D] Synthetic introduction to ML for PhD student in Mathematics
Hi all,

I'm a about to begin my PhD in Mathematics, and my supervisor current project is to investigate the feasibility of some niche Linear Algebra tools to the setting of Machine Learning, especially PINNs.

I am already very familiar with such niche Linear Algebra results; however I lack any knowledge of ML.

Moreover, I have some knowledge of Measure Theory, Calculus of Probabilities and Statistics.

I skimmed through Bishops's [Pattern Recognition] (https://books.google.it/books/about/Pattern_Recognition_and_Machine_Learning.html?id=kTNoQgAACAAJ&redir_esc=y) and Goodfellows's [Deep Learning](https://www.deeplearningbook.org), and I have found both books to be excessively redundant and verbose.

I do appreciate the abundance of examples and the maieutic approach of these books, however I need to get a theoretical grasp on the subject.  

I am looking for an alternative resource(s) on the subject written with mathematical rigour targeted at graduate students. 

Do you have anything to suggest, be it books, lecture notes or video lectures?"
1jurarc,[P] Reducing Transformer Training Time Without Sacrificing Accuracy — A Dynamic Architecture Update Approach,"Hey everyone!

I’ve been working on a research project focused on optimizing transformer models to **reduce training time without compromising accuracy**. 🚀

Through this work, I developed a novel method where the model **dynamically updates its architecture during training**, allowing it to converge faster while still maintaining performance. Think of it like adaptive scaling, but smarter — we’re not just reducing size arbitrarily, we're making informed structural updates *on the fly*.

I recently published a Medium article explaining one part of the approach: **how I managed to keep the model’s accuracy stable even after reducing the training time**. If you're interested in the technical details or just want to nerd out on optimization strategies, I'd love for you to check it out!

🔗 **Medium article**: [https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf](https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf)  
🔗 **GitHub repo**: [https://github.com/suparshwa31/Dynamic\_Transformer](https://github.com/suparshwa31/Dynamic_Transformer)

Would love feedback, ideas, or even collaborators — feel free to open a PR or drop your thoughts. Always happy to discuss!",2025-04-09 01:06:39,9,7,MachineLearning,post,"[P] Reducing Transformer Training Time Without Sacrificing Accuracy — A Dynamic Architecture Update Approach
Hey everyone!

I’ve been working on a research project focused on optimizing transformer models to **reduce training time without compromising accuracy**. 🚀

Through this work, I developed a novel method where the model **dynamically updates its architecture during training**, allowing it to converge faster while still maintaining performance. Think of it like adaptive scaling, but smarter — we’re not just reducing size arbitrarily, we're making informed structural updates *on the fly*.

I recently published a Medium article explaining one part of the approach: **how I managed to keep the model’s accuracy stable even after reducing the training time**. If you're interested in the technical details or just want to nerd out on optimization strategies, I'd love for you to check it out!

🔗 **Medium article**: [https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf](https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf)  
🔗 **GitHub repo**: [https://github.com/suparshwa31/Dynamic\_Transformer](https://github.com/suparshwa31/Dynamic_Transformer)

Would love feedback, ideas, or even collaborators — feel free to open a PR or drop your thoughts. Always happy to discuss!"
1jukunq,[D] How to handle questions about parts of a collaborative research project I didn’t directly work on during a poster session presentation?,"I’m presenting research where I focused on experimental results/codebase, but our paper includes theoretical work by collaborators. How do I answer questions about parts I didn’t handle?

* **Is it okay to say**, ‘This aspect was led by \[Name\]—I can explain how it connects to my experiments’?
* **How detailed should I be** about others’ contributions?
* **What phrases** do you use to redirect to your expertise without sounding dismissive?",2025-04-08 20:30:52,12,2,MachineLearning,post,"[D] How to handle questions about parts of a collaborative research project I didn’t directly work on during a poster session presentation?
I’m presenting research where I focused on experimental results/codebase, but our paper includes theoretical work by collaborators. How do I answer questions about parts I didn’t handle?

* **Is it okay to say**, ‘This aspect was led by \[Name\]—I can explain how it connects to my experiments’?
* **How detailed should I be** about others’ contributions?
* **What phrases** do you use to redirect to your expertise without sounding dismissive?"
1juay0t,"[D] Comparing GenAI Inference Engines: TensorRT-LLM, vLLM, Hugging Face TGI, and LMDeploy","Hey everyone, I’ve been diving into the world of generative AI inference engines for quite some time at NLP Cloud, and I wanted to share some insights from a comparison I put together. I looked at four popular options—NVIDIA’s TensorRT-LLM, vLLM, Hugging Face’s Text Generation Inference (TGI), and LMDeploy—and ran some benchmarks to see how they stack up for real-world use cases. Thought this might spark some discussion here since I know a lot of you are working with LLMs or optimizing inference pipelines:

TensorRT-LLM

* NVIDIA’s beast for GPU-accelerated inference. Built on TensorRT, it optimizes models with layer fusion, precision tuning (FP16, INT8, even FP8), and custom CUDA kernels.
* Pros: Blazing fast on NVIDIA GPUs—think sub-50ms latency for single requests on an A100 and \~700 tokens/sec at 100 concurrent users for LLaMA-3 70B Q4 (per BentoML benchmarks). Dynamic batching and tight integration with Triton Inference Server make it a throughput monster.
* Cons: Setup can be complex if you’re not already in the NVIDIA ecosystem. You need to deal with model compilation, and it’s not super flexible for quick prototyping.

vLLM

* Open-source champion for high-throughput inference. Uses PagedAttention to manage KV caches in chunks, cutting memory waste and boosting speed.
* Pros: Easy to spin up (pip install, Python-friendly), and it’s flexible—runs on NVIDIA, AMD, even CPU. Throughput is solid (\~600-650 tokens/sec at 100 users for LLaMA-3 70B Q4), and dynamic batching keeps it humming. Latency’s decent at 60-80ms solo.
* Cons: It’s less optimized for single-request latency, so if you’re building a chatbot with one user at a time, it might not shine as much. Also, it’s still maturing—some edge cases (like exotic model architectures) might not be supported.

Hugging Face TGI

* Hugging Face’s production-ready inference tool. Ties into their model hub (BERT, GPT, etc.) and uses Rust for speed, with continuous batching to keep GPUs busy.
* Pros: Docker setup is quick, and it scales well. Latency’s 50-70ms, throughput matches vLLM (\~600-650 tokens/sec at 100 users). Bonus: built-in output filtering for safety. Perfect if you’re already in the HF ecosystem.
* Cons: Less raw speed than TensorRT-LLM, and memory can bloat with big batches. Feels a bit restrictive outside HF’s world.

LMDeploy

* This Toolkit from the MMRazor/MMDeploy crew, focused on fast, efficient LLM deployment. Features TurboMind (a high-performance engine) and a PyTorch fallback, with persistent batching and blocked KV caching for speed.
* Pros: Decoding speed is nuts—up to 1.8x more requests/sec than vLLM on an A100. TurboMind pushes 4-bit inference 2.4x faster than FP16, hitting \~700 tokens/sec at 100 users (LLaMA-3 70B Q4). Low latency (40-60ms), easy one-command server setup, and it even handles multi-round chats efficiently by caching history.
* Cons: TurboMind’s picky—doesn’t support sliding window attention (e.g., Mistral) yet. Non-NVIDIA users get stuck with the slower PyTorch engine. Still, on NVIDIA GPUs, it’s a performance beast.

You can read the full comparison here: [https://nlpcloud.com/genai-inference-engines-tensorrt-llm-vs-vllm-vs-hugging-face-tgi-vs-lmdeploy.html](https://nlpcloud.com/genai-inference-engines-tensorrt-llm-vs-vllm-vs-hugging-face-tgi-vs-lmdeploy.html)

What’s your experience with these tools? Any hidden issues I missed? Or are there other inference engines that should be mentioned? Would love to hear your thoughts!

Julien",2025-04-08 13:09:57,23,4,MachineLearning,post,"[D] Comparing GenAI Inference Engines: TensorRT-LLM, vLLM, Hugging Face TGI, and LMDeploy
Hey everyone, I’ve been diving into the world of generative AI inference engines for quite some time at NLP Cloud, and I wanted to share some insights from a comparison I put together. I looked at four popular options—NVIDIA’s TensorRT-LLM, vLLM, Hugging Face’s Text Generation Inference (TGI), and LMDeploy—and ran some benchmarks to see how they stack up for real-world use cases. Thought this might spark some discussion here since I know a lot of you are working with LLMs or optimizing inference pipelines:

TensorRT-LLM

* NVIDIA’s beast for GPU-accelerated inference. Built on TensorRT, it optimizes models with layer fusion, precision tuning (FP16, INT8, even FP8), and custom CUDA kernels.
* Pros: Blazing fast on NVIDIA GPUs—think sub-50ms latency for single requests on an A100 and \~700 tokens/sec at 100 concurrent users for LLaMA-3 70B Q4 (per BentoML benchmarks). Dynamic batching and tight integration with Triton Inference Server make it a throughput monster.
* Cons: Setup can be complex if you’re not already in the NVIDIA ecosystem. You need to deal with model compilation, and it’s not super flexible for quick prototyping.

vLLM

* Open-source champion for high-throughput inference. Uses PagedAttention to manage KV caches in chunks, cutting memory waste and boosting speed.
* Pros: Easy to spin up (pip install, Python-friendly), and it’s flexible—runs on NVIDIA, AMD, even CPU. Throughput is solid (\~600-650 tokens/sec at 100 users for LLaMA-3 70B Q4), and dynamic batching keeps it humming. Latency’s decent at 60-80ms solo.
* Cons: It’s less optimized for single-request latency, so if you’re building a chatbot with one user at a time, it might not shine as much. Also, it’s still maturing—some edge cases (like exotic model architectures) might not be supported.

Hugging Face TGI

* Hugging Face’s production-ready inference tool. Ties into their model hub (BERT, GPT, etc.) and uses Rust for speed, with continuous batching to keep GPUs busy.
* Pros: Docker setup is quick, and it scales well. Latency’s 50-70ms, throughput matches vLLM (\~600-650 tokens/sec at 100 users). Bonus: built-in output filtering for safety. Perfect if you’re already in the HF ecosystem.
* Cons: Less raw speed than TensorRT-LLM, and memory can bloat with big batches. Feels a bit restrictive outside HF’s world.

LMDeploy

* This Toolkit from the MMRazor/MMDeploy crew, focused on fast, efficient LLM deployment. Features TurboMind (a high-performance engine) and a PyTorch fallback, with persistent batching and blocked KV caching for speed.
* Pros: Decoding speed is nuts—up to 1.8x more requests/sec than vLLM on an A100. TurboMind pushes 4-bit inference 2.4x faster than FP16, hitting \~700 tokens/sec at 100 users (LLaMA-3 70B Q4). Low latency (40-60ms), easy one-command server setup, and it even handles multi-round chats efficiently by caching history.
* Cons: TurboMind’s picky—doesn’t support sliding window attention (e.g., Mistral) yet. Non-NVIDIA users get stuck with the slower PyTorch engine. Still, on NVIDIA GPUs, it’s a performance beast.

You can read the full comparison here: [https://nlpcloud.com/genai-inference-engines-tensorrt-llm-vs-vllm-vs-hugging-face-tgi-vs-lmdeploy.html](https://nlpcloud.com/genai-inference-engines-tensorrt-llm-vs-vllm-vs-hugging-face-tgi-vs-lmdeploy.html)

What’s your experience with these tools? Any hidden issues I missed? Or are there other inference engines that should be mentioned? Would love to hear your thoughts!

Julien"
1ju5g9d,[D] A regression head for llm works surprisingly well!,"I have been training a small 33M VIT+decoder model I have written for visual grounding tasks, and when training from scratch, I had great success by introducing a regresion head to the embeds before lm head to gain great accuracy.   
  
All the literature (such as: https://arxiv.org/html/2501.19383v1) I could find directly works with particular tokens and cross entropy loss from what I gathered.   
  
I had this success for a personal project by jointly doing cross entropy on lm\_head results (for point tokens) and introducing a regression head on the last embed layer and doing regression loss.    
  
I just cooked it up originally, but is this known?  ",2025-04-08 06:42:18,56,16,MachineLearning,post,"[D] A regression head for llm works surprisingly well!
I have been training a small 33M VIT+decoder model I have written for visual grounding tasks, and when training from scratch, I had great success by introducing a regresion head to the embeds before lm head to gain great accuracy.   
  
All the literature (such as: https://arxiv.org/html/2501.19383v1) I could find directly works with particular tokens and cross entropy loss from what I gathered.   
  
I had this success for a personal project by jointly doing cross entropy on lm\_head results (for point tokens) and introducing a regression head on the last embed layer and doing regression loss.    
  
I just cooked it up originally, but is this known?  "
1judb8u,[P] Insights in shift of performance of certain LLM's on different hardware,"Hello all,

For school i conducted some simple performance tests an a couple of LLMs, one on a desktop with a RTX2060 and the other on a Raspberry Pi5. I am trying to make sense of the data but still have a couple of questions as I am not an expert on the theory in this field.

On the desktop Llama3.2:1b did way better than any other model i tested but when i tested the same models on the same prompts on the Raspberry Pi it came second and i have no idea why.

Another question I have is why the results of Granite3.1-MoE are so spread out compared to the other models, is this just because it is an MoE model and it depends on which part of the model it activates?

all of the models i tested were small enough to fit in the 6GB of VRAM of the 2060 and the 8GB of system RAM of the Pi.

Any insights on this are appreciated!

below are the boxplots to give a clearer view of the data.

https://preview.redd.it/l58w4gji1mte1.png?width=1842&format=png&auto=webp&s=94a822462323bf50bfe599a1b823df98edeb1ac6

https://preview.redd.it/t4eu34kj1mte1.png?width=1842&format=png&auto=webp&s=498206702fa7cf5a376b34590acb890bf06fc27c",2025-04-08 15:16:44,2,2,MachineLearning,post,"[P] Insights in shift of performance of certain LLM's on different hardware
Hello all,

For school i conducted some simple performance tests an a couple of LLMs, one on a desktop with a RTX2060 and the other on a Raspberry Pi5. I am trying to make sense of the data but still have a couple of questions as I am not an expert on the theory in this field.

On the desktop Llama3.2:1b did way better than any other model i tested but when i tested the same models on the same prompts on the Raspberry Pi it came second and i have no idea why.

Another question I have is why the results of Granite3.1-MoE are so spread out compared to the other models, is this just because it is an MoE model and it depends on which part of the model it activates?

all of the models i tested were small enough to fit in the 6GB of VRAM of the 2060 and the 8GB of system RAM of the Pi.

Any insights on this are appreciated!

below are the boxplots to give a clearer view of the data.

https://preview.redd.it/l58w4gji1mte1.png?width=1842&format=png&auto=webp&s=94a822462323bf50bfe599a1b823df98edeb1ac6

https://preview.redd.it/t4eu34kj1mte1.png?width=1842&format=png&auto=webp&s=498206702fa7cf5a376b34590acb890bf06fc27c"
1jui5tm,[P] First-Order Motion Transfer in Keras – Animate a Static Image from a Driving Video,"**TL;DR:**  
Implemented first-order motion transfer in Keras (Siarohin et al., NeurIPS 2019) to animate static images using driving videos. Built a custom flow map warping module since Keras lacks native support for normalized flow-based deformation. Works well on TensorFlow. Code, docs, and demo here:

🔗 [https://github.com/abhaskumarsinha/KMT](https://github.com/abhaskumarsinha/KMT)  
📘 [https://abhaskumarsinha.github.io/KMT/src.html](https://abhaskumarsinha.github.io/KMT/src.html)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Hey folks! 👋

I’ve been working on implementing motion transfer in Keras, inspired by the **First Order Motion Model for Image Animation** (Siarohin et al., NeurIPS 2019). The idea is simple but powerful: take a static image and animate it using motion extracted from a reference video.

💡 The tricky part?  
Keras doesn’t really have support for deforming images using **normalized flow maps** (like PyTorch’s `grid_sample`). The closest is `keras.ops.image.map_coordinates()` — but it doesn’t work well inside models (no batching, absolute coordinates, CPU only).

🔧 So I built a custom flow warping module for Keras:

* Supports batching
* Works with normalized coordinates (\[-1, 1\])
* GPU-compatible
* Can be used as part of a DL model to learn flow maps and deform images in parallel

📦 Project includes:

* Keypoint detection and motion estimation
* Generator with first-order motion approximation
* GAN-based training pipeline
* Example notebook to get started

🧪 Still experimental, but works well on TensorFlow backend.

👉 Repo: [https://github.com/abhaskumarsinha/KMT](https://github.com/abhaskumarsinha/KMT)  
📘 Docs: [https://abhaskumarsinha.github.io/KMT/src.html](https://abhaskumarsinha.github.io/KMT/src.html)  
🧪 Try: `example.ipyn`b for a quick demo

Would love feedback, ideas, or contributions — and happy to collab if anyone’s working on similar stuff!

",2025-04-08 18:43:26,1,0,MachineLearning,post,"[P] First-Order Motion Transfer in Keras – Animate a Static Image from a Driving Video
**TL;DR:**  
Implemented first-order motion transfer in Keras (Siarohin et al., NeurIPS 2019) to animate static images using driving videos. Built a custom flow map warping module since Keras lacks native support for normalized flow-based deformation. Works well on TensorFlow. Code, docs, and demo here:

🔗 [https://github.com/abhaskumarsinha/KMT](https://github.com/abhaskumarsinha/KMT)  
📘 [https://abhaskumarsinha.github.io/KMT/src.html](https://abhaskumarsinha.github.io/KMT/src.html)

\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

Hey folks! 👋

I’ve been working on implementing motion transfer in Keras, inspired by the **First Order Motion Model for Image Animation** (Siarohin et al., NeurIPS 2019). The idea is simple but powerful: take a static image and animate it using motion extracted from a reference video.

💡 The tricky part?  
Keras doesn’t really have support for deforming images using **normalized flow maps** (like PyTorch’s `grid_sample`). The closest is `keras.ops.image.map_coordinates()` — but it doesn’t work well inside models (no batching, absolute coordinates, CPU only).

🔧 So I built a custom flow warping module for Keras:

* Supports batching
* Works with normalized coordinates (\[-1, 1\])
* GPU-compatible
* Can be used as part of a DL model to learn flow maps and deform images in parallel

📦 Project includes:

* Keypoint detection and motion estimation
* Generator with first-order motion approximation
* GAN-based training pipeline
* Example notebook to get started

🧪 Still experimental, but works well on TensorFlow backend.

👉 Repo: [https://github.com/abhaskumarsinha/KMT](https://github.com/abhaskumarsinha/KMT)  
📘 Docs: [https://abhaskumarsinha.github.io/KMT/src.html](https://abhaskumarsinha.github.io/KMT/src.html)  
🧪 Try: `example.ipyn`b for a quick demo

Would love feedback, ideas, or contributions — and happy to collab if anyone’s working on similar stuff!

"
1ju8923,"[R] Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification
 via Efficient Multi-Token Prediction",">Zero-shot text classification typically relies on prompt engineering, but the inherent prompt brittleness of large language models under mines its reliability. Minor changes in prompt can cause significant discrepancies in model performance. We attribute this prompt brittleness largely to the narrow focus on next token probabilities in existing methods. To address this, we propose Placeholding Parallel Prediction (P3), a novel approach that predicts token probabilities across multiple positions and simulates comprehensive sampling of generation paths in a single run of a language model. Experiments show improved accuracy and up to 98% reduction in the standard devia tion across prompts, boosting robustness. Even without a prompt, P3 maintains comparable performance, reducing the need for prompt engineering.

Interesting paper on improving determinism in ML models and avoid ""prompt brittleness"" using placeholders and parallel predictions instead of relying solely on next-token probabilities.

  
Paper link: [https://arxiv.org/abs/2504.03159](https://arxiv.org/abs/2504.03159)",2025-04-08 09:57:25,5,0,MachineLearning,post,"[R] Beyond the Next Token: Towards Prompt-Robust Zero-Shot Classification
 via Efficient Multi-Token Prediction
>Zero-shot text classification typically relies on prompt engineering, but the inherent prompt brittleness of large language models under mines its reliability. Minor changes in prompt can cause significant discrepancies in model performance. We attribute this prompt brittleness largely to the narrow focus on next token probabilities in existing methods. To address this, we propose Placeholding Parallel Prediction (P3), a novel approach that predicts token probabilities across multiple positions and simulates comprehensive sampling of generation paths in a single run of a language model. Experiments show improved accuracy and up to 98% reduction in the standard devia tion across prompts, boosting robustness. Even without a prompt, P3 maintains comparable performance, reducing the need for prompt engineering.

Interesting paper on improving determinism in ML models and avoid ""prompt brittleness"" using placeholders and parallel predictions instead of relying solely on next-token probabilities.

  
Paper link: [https://arxiv.org/abs/2504.03159](https://arxiv.org/abs/2504.03159)"
1jtwdn8,[P] [D] Why does my GNN-LSTM model fail to generalize with full training data for a spatiotemporal prediction task?,"I'm working on a spatiotemporal prediction problem where I want to forecast a scalar value per spatial node over time. My data spans multiple spatial grid locations with daily observations.

**Data Setup**

* The spatial region is divided into subregions, each with a graph structure.
* Each node represents a grid cell with input features: variable\_value\_t, lat, lon
* Edges are static for a subregion and are formed based on distance and correlation
* Edge features include direction and distance.
* Each subregion is normalized independently using Z-score normalization (mean/std from training split).

**Model**

    class GNNLayer(nn.Module):
       def __init__(self, node_in_dim, edge_in_dim, hidden_dim):
           ...
           self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=2, batch_first=True)
    
       def forward(self, x, edge_index, edge_attr):
           row, col = edge_index
           src, tgt = x[row], x[col]
           edge_messages = self.edge_net(edge_attr, src, tgt)
           agg_msg = torch.zeros_like(x).index_add(0, col, edge_messages)
           x_updated = self.node_net(x, agg_msg)
           attn_out, _ = self.attention(x_updated.unsqueeze(0), x_updated.unsqueeze(0), x_updated.unsqueeze(0))
           return x_updated + attn_out.squeeze(0), edge_messages
    
    class GNNLSTM(nn.Module):
        def __init__(self, ...):
            ...
            self.gnn_layers = nn.ModuleList([...])
            self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=128, num_layers=2, dropout=0.2, batch_first=True)
            self.pred_head = nn.Sequential(
                nn.Linear(128, 64), nn.LeakyReLU(0.1), nn.Linear(64, 2 * pred_len)
            )
    
        def forward(self, batch):
            ...
            for t in range(T):
                x_t = graph.x  # batched node features
                for gnn in self.gnn_layers:
                    x_t, _ = gnn(x_t, graph.edge_index, graph.edge_attr)
                x_stack.append(x_t)
            x_seq = torch.stack(x_stack, dim=1)  # [B, T, N, hidden_dim]
            lstm_out, _ = self.lstm(x_seq.reshape(B*N, T, -1))
            out = self.pred_head(lstm_out[:, -1]).view(B, N, 2)
            mean, logvar = out[..., 0], out[..., 1]
            return mean, torch.exp(logvar) + 1e-3

**Training Details**

Loss: MSE Loss

Optimizer: Adam, LR = 1e-4

Scheduler: ReduceLROnPlateau

Per-subregion training (each subregion is trained independently)

I also tried using curriculum learning: Start with 50 batches and increase gradually each epoch until the full training set is used. I have 500 batches in total in the train split

**Issue:**  When trained on a small number of batches, the model converges and gives reasonable results. However, when trained on the full dataset, the model:

* Shows inconsistent or worsening validation loss after a few epochs
* Seems to rely too much on the LSTM (e.g., lstm.weight\_hh\_\* has much higher parameter updates than GNN layers)
* Keeps predicting poorly on the same few grid cells over time

I’ve tried:

* Increasing GNN depth (currently 4 layers)
* Gradient clipping
* Attention + residuals + layer norm in GNN

What could cause the GNN-LSTM model to fail generalization with full training data despite success with smaller subsets? I am at my wit's end.

[This was for a sanity check - I trained on 40 batches and validated on 10.](https://preview.redd.it/n5lqi20x9hte1.png?width=1446&format=png&auto=webp&s=44bf4f2be5ba373168de91240c1b0b87f77f5124)

**UPDATE**

Hi everybody! Thank you so much for your help and insights. I think I figured out what was going wrong. I think my edge creation thresholds were too weak and I tightened them and reduced my model complexity. Thanks to u/Ben___Pen and u/Ty4Readin, I also increased my dataset size and training epochs.

This is what I am achieving:

Test Metrics for one subregion:

• MSE:  0.012611 

• RMSE: 0.112299 

• MAE:  0.084387 

• R²:   0.985847

I will further refine my steps as I go. Once again, thank you all! Everyone is so kind and helpful :)",2025-04-07 23:07:25,27,8,MachineLearning,post,"[P] [D] Why does my GNN-LSTM model fail to generalize with full training data for a spatiotemporal prediction task?
I'm working on a spatiotemporal prediction problem where I want to forecast a scalar value per spatial node over time. My data spans multiple spatial grid locations with daily observations.

**Data Setup**

* The spatial region is divided into subregions, each with a graph structure.
* Each node represents a grid cell with input features: variable\_value\_t, lat, lon
* Edges are static for a subregion and are formed based on distance and correlation
* Edge features include direction and distance.
* Each subregion is normalized independently using Z-score normalization (mean/std from training split).

**Model**

    class GNNLayer(nn.Module):
       def __init__(self, node_in_dim, edge_in_dim, hidden_dim):
           ...
           self.attention = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=2, batch_first=True)
    
       def forward(self, x, edge_index, edge_attr):
           row, col = edge_index
           src, tgt = x[row], x[col]
           edge_messages = self.edge_net(edge_attr, src, tgt)
           agg_msg = torch.zeros_like(x).index_add(0, col, edge_messages)
           x_updated = self.node_net(x, agg_msg)
           attn_out, _ = self.attention(x_updated.unsqueeze(0), x_updated.unsqueeze(0), x_updated.unsqueeze(0))
           return x_updated + attn_out.squeeze(0), edge_messages
    
    class GNNLSTM(nn.Module):
        def __init__(self, ...):
            ...
            self.gnn_layers = nn.ModuleList([...])
            self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=128, num_layers=2, dropout=0.2, batch_first=True)
            self.pred_head = nn.Sequential(
                nn.Linear(128, 64), nn.LeakyReLU(0.1), nn.Linear(64, 2 * pred_len)
            )
    
        def forward(self, batch):
            ...
            for t in range(T):
                x_t = graph.x  # batched node features
                for gnn in self.gnn_layers:
                    x_t, _ = gnn(x_t, graph.edge_index, graph.edge_attr)
                x_stack.append(x_t)
            x_seq = torch.stack(x_stack, dim=1)  # [B, T, N, hidden_dim]
            lstm_out, _ = self.lstm(x_seq.reshape(B*N, T, -1))
            out = self.pred_head(lstm_out[:, -1]).view(B, N, 2)
            mean, logvar = out[..., 0], out[..., 1]
            return mean, torch.exp(logvar) + 1e-3

**Training Details**

Loss: MSE Loss

Optimizer: Adam, LR = 1e-4

Scheduler: ReduceLROnPlateau

Per-subregion training (each subregion is trained independently)

I also tried using curriculum learning: Start with 50 batches and increase gradually each epoch until the full training set is used. I have 500 batches in total in the train split

**Issue:**  When trained on a small number of batches, the model converges and gives reasonable results. However, when trained on the full dataset, the model:

* Shows inconsistent or worsening validation loss after a few epochs
* Seems to rely too much on the LSTM (e.g., lstm.weight\_hh\_\* has much higher parameter updates than GNN layers)
* Keeps predicting poorly on the same few grid cells over time

I’ve tried:

* Increasing GNN depth (currently 4 layers)
* Gradient clipping
* Attention + residuals + layer norm in GNN

What could cause the GNN-LSTM model to fail generalization with full training data despite success with smaller subsets? I am at my wit's end.

[This was for a sanity check - I trained on 40 batches and validated on 10.](https://preview.redd.it/n5lqi20x9hte1.png?width=1446&format=png&auto=webp&s=44bf4f2be5ba373168de91240c1b0b87f77f5124)

**UPDATE**

Hi everybody! Thank you so much for your help and insights. I think I figured out what was going wrong. I think my edge creation thresholds were too weak and I tightened them and reduced my model complexity. Thanks to u/Ben___Pen and u/Ty4Readin, I also increased my dataset size and training epochs.

This is what I am achieving:

Test Metrics for one subregion:

• MSE:  0.012611 

• RMSE: 0.112299 

• MAE:  0.084387 

• R²:   0.985847

I will further refine my steps as I go. Once again, thank you all! Everyone is so kind and helpful :)"
1jtoegy,[D] HAI Artificial Intelligence Index Report 2025: The AI Race Has Gotten Crowded—and China Is Closing In on the US,"Stanford University’s [Institute for Human-Centered AI (HAI)](https://hai.stanford.edu/) published a new research paper today, which highlighted just how crowded the field has become.

* [HAI Artificial Intelligence Index Report 2025](https://macro.com/app/pdf/e10d9df1-f135-4681-b377-8a6c72ec07f8/)

Main Takeaways:

1. AI performance on demanding benchmarks continues to improve.
2. AI is increasingly embedded in everyday life.
3. Business is all in on AI, fueling record investment and usage, as research continues to show strong productivity impacts.
4. The U.S. still leads in producing top AI models—but China is closing the performance gap.
5. The responsible AI ecosystem evolves—unevenly.
6. Global AI optimism is rising—but deep regional divides remain.
7. AI becomes more efficient, affordable and accessible.
8. Governments are stepping up on AI—with regulation and investment.
9. AI and computer science education is expanding—but gaps in access and readiness persist.
10. Industry is racing ahead in AI—but the frontier is tightening.
11. AI earns top honors for its impact on science.
12. Complex reasoning remains a challenge.",2025-04-07 17:44:24,33,4,MachineLearning,post,"[D] HAI Artificial Intelligence Index Report 2025: The AI Race Has Gotten Crowded—and China Is Closing In on the US
Stanford University’s [Institute for Human-Centered AI (HAI)](https://hai.stanford.edu/) published a new research paper today, which highlighted just how crowded the field has become.

* [HAI Artificial Intelligence Index Report 2025](https://macro.com/app/pdf/e10d9df1-f135-4681-b377-8a6c72ec07f8/)

Main Takeaways:

1. AI performance on demanding benchmarks continues to improve.
2. AI is increasingly embedded in everyday life.
3. Business is all in on AI, fueling record investment and usage, as research continues to show strong productivity impacts.
4. The U.S. still leads in producing top AI models—but China is closing the performance gap.
5. The responsible AI ecosystem evolves—unevenly.
6. Global AI optimism is rising—but deep regional divides remain.
7. AI becomes more efficient, affordable and accessible.
8. Governments are stepping up on AI—with regulation and investment.
9. AI and computer science education is expanding—but gaps in access and readiness persist.
10. Industry is racing ahead in AI—but the frontier is tightening.
11. AI earns top honors for its impact on science.
12. Complex reasoning remains a challenge."
1jtjw2b,"[P] Docext: Open-Source, On-Prem Document Intelligence Powered by Vision-Language Models","We’re excited to open source `docext`, a zero-OCR, on-premises tool for extracting structured data from documents like invoices, passports, and more — no cloud, no external APIs, no OCR engines required.  
 Powered entirely by **vision-language models (VLMs)**, `docext` understands documents visually and semantically to extract both field data and tables — directly from document images.  
 **Run it fully on-prem** for complete data privacy and control. 

**Key Features:**

*  Custom & pre-built extraction templates
*  Table + field data extraction
*  Gradio-powered web interface
*  On-prem deployment with REST API
*  Multi-page document support
*  Confidence scores for extracted fields

Whether you're processing invoices, ID documents, or any form-heavy paperwork, `docext` helps you turn them into usable data in minutes.  
 Try it out:

* `pip install docext` or launch via Docker
* Spin up the web UI with `python -m` [`docext.app.app`](https://github.com/nanonets/docext)
* Dive into the [Colab demo](https://github.com/nanonets/docext#colab-notebook)

 GitHub: [https://github.com/nanonets/docext](https://github.com/nanonets/docext)  
 Questions? Feature requests? Open an issue or start a discussion!",2025-04-07 14:20:48,38,4,MachineLearning,post,"[P] Docext: Open-Source, On-Prem Document Intelligence Powered by Vision-Language Models
We’re excited to open source `docext`, a zero-OCR, on-premises tool for extracting structured data from documents like invoices, passports, and more — no cloud, no external APIs, no OCR engines required.  
 Powered entirely by **vision-language models (VLMs)**, `docext` understands documents visually and semantically to extract both field data and tables — directly from document images.  
 **Run it fully on-prem** for complete data privacy and control. 

**Key Features:**

*  Custom & pre-built extraction templates
*  Table + field data extraction
*  Gradio-powered web interface
*  On-prem deployment with REST API
*  Multi-page document support
*  Confidence scores for extracted fields

Whether you're processing invoices, ID documents, or any form-heavy paperwork, `docext` helps you turn them into usable data in minutes.  
 Try it out:

* `pip install docext` or launch via Docker
* Spin up the web UI with `python -m` [`docext.app.app`](https://github.com/nanonets/docext)
* Dive into the [Colab demo](https://github.com/nanonets/docext#colab-notebook)

 GitHub: [https://github.com/nanonets/docext](https://github.com/nanonets/docext)  
 Questions? Feature requests? Open an issue or start a discussion!"
1ju0wko,"[D] If a method used pretrained model like Owlvit2 v2, there is no way to know if these models has been trained on the validation set of a downstream task?",How people solve these problems. Could I still publish a paper for my results,2025-04-08 02:36:13,3,0,MachineLearning,post,"[D] If a method used pretrained model like Owlvit2 v2, there is no way to know if these models has been trained on the validation set of a downstream task?
How people solve these problems. Could I still publish a paper for my results"
1jtwusn,[Research] Evaluating your retrieval system - new research from Chroma on generative benchmarking,"HI all, I'm Jeff, cofounder of Chroma. We're working to make AI application development more like engineering and less like alchemy.

Today, we are introducing **representative generative benchmarking**—custom evaluation sets built from your own data and reflective of the queries users actually make in production. These benchmarks are designed to test retrieval systems under similar conditions they face in production, rather than relying on artificial or generic datasets.

Benchmarking is essential for evaluating AI systems, especially in tasks like document retrieval where outputs are probabilistic and highly context-dependent. However, widely used benchmarks like MTEB are often overly clean, generic, and in many cases, have been memorized by the embedding models during training. We show that strong results on public benchmarks can fail to generalize to production settings, and we present a generation method that produces realistic queries representative of actual user queries.

Check out our technical report here: [https://research.trychroma.com/generative-benchmarking](https://research.trychroma.com/generative-benchmarking)",2025-04-07 23:27:53,3,0,MachineLearning,post,"[Research] Evaluating your retrieval system - new research from Chroma on generative benchmarking
HI all, I'm Jeff, cofounder of Chroma. We're working to make AI application development more like engineering and less like alchemy.

Today, we are introducing **representative generative benchmarking**—custom evaluation sets built from your own data and reflective of the queries users actually make in production. These benchmarks are designed to test retrieval systems under similar conditions they face in production, rather than relying on artificial or generic datasets.

Benchmarking is essential for evaluating AI systems, especially in tasks like document retrieval where outputs are probabilistic and highly context-dependent. However, widely used benchmarks like MTEB are often overly clean, generic, and in many cases, have been memorized by the embedding models during training. We show that strong results on public benchmarks can fail to generalize to production settings, and we present a generation method that produces realistic queries representative of actual user queries.

Check out our technical report here: [https://research.trychroma.com/generative-benchmarking](https://research.trychroma.com/generative-benchmarking)"
1jtmk36,[R] Dataset with medical notes,"Working on dataextraction tools for medical notes (like notes physicians write after consultation).   
Is there any publicly available dataset I can use for validation?

I have looked at MIMIC datasets, which seems interesting but not sure whether I will be able to access it representing a HealthTech company.  
PMC Patients and CLINICAL VISIT NOTE SUMMARIZATION CORPUS from Microsoft seems good, but are not super representative for the use case I am looking for. ",2025-04-07 16:27:28,7,5,MachineLearning,post,"[R] Dataset with medical notes
Working on dataextraction tools for medical notes (like notes physicians write after consultation).   
Is there any publicly available dataset I can use for validation?

I have looked at MIMIC datasets, which seems interesting but not sure whether I will be able to access it representing a HealthTech company.  
PMC Patients and CLINICAL VISIT NOTE SUMMARIZATION CORPUS from Microsoft seems good, but are not super representative for the use case I am looking for. "
1jtfhwo,[R] Deep Learning Hits SOTA in Cancer Mutation Detection (Nature Communications),"🚀 VarNet is an end-to-end deep learning framework trained on hundreds of whole cancer genomes to detect somatic variants with high accuracy — no hand-tuned heuristics.  
Published in **Nature Communications**, it achieves state-of-the-art performance across multiple benchmarks.  
👉 **Paper:** [https://www.nature.com/articles/s41467-022-31765-8](https://www.nature.com/articles/s41467-022-31765-8)  
👉 **Code:** [https://github.com/skandlab/VarNet](https://github.com/skandlab/VarNet)",2025-04-07 09:21:05,22,1,MachineLearning,post,"[R] Deep Learning Hits SOTA in Cancer Mutation Detection (Nature Communications)
🚀 VarNet is an end-to-end deep learning framework trained on hundreds of whole cancer genomes to detect somatic variants with high accuracy — no hand-tuned heuristics.  
Published in **Nature Communications**, it achieves state-of-the-art performance across multiple benchmarks.  
👉 **Paper:** [https://www.nature.com/articles/s41467-022-31765-8](https://www.nature.com/articles/s41467-022-31765-8)  
👉 **Code:** [https://github.com/skandlab/VarNet](https://github.com/skandlab/VarNet)"
1jt9r7u,[R] Uniformly distributed deep feature representations improve fairness & robustness [TMLR],"TLDR: Theoretically and empircally demonstrates that encouraging deep feature represenatations to be uniformly distributed improves fairness and robustness (specifically, sub-group robustness and domain generalization). Paper with code: [https://openreview.net/forum?id=PgLbS5yp8n](https://openreview.net/forum?id=PgLbS5yp8n)",2025-04-07 03:26:01,20,0,MachineLearning,post,"[R] Uniformly distributed deep feature representations improve fairness & robustness [TMLR]
TLDR: Theoretically and empircally demonstrates that encouraging deep feature represenatations to be uniformly distributed improves fairness and robustness (specifically, sub-group robustness and domain generalization). Paper with code: [https://openreview.net/forum?id=PgLbS5yp8n](https://openreview.net/forum?id=PgLbS5yp8n)"
1jt4pqr,[R] SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators,,2025-04-06 23:15:03,27,3,MachineLearning,post,"[R] SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators
"
1jswn5k,[R] Image classification by evolving bytecode,"Over the last few years, I’ve been working on [Zyme](https://zyme.dev/), an esoteric language for genetic programming: creating computer programs by means of natural selection. I’ve started seeing promising results, showing that random bytecode mutations can, over time, lead to measurable improvements in program performance. While still a long way from state-of-the-art approaches like neural networks, I wanted to share my progress.

Feedback and criticism are welcome!",2025-04-06 17:25:58,38,9,MachineLearning,post,"[R] Image classification by evolving bytecode
Over the last few years, I’ve been working on [Zyme](https://zyme.dev/), an esoteric language for genetic programming: creating computer programs by means of natural selection. I’ve started seeing promising results, showing that random bytecode mutations can, over time, lead to measurable improvements in program performance. While still a long way from state-of-the-art approaches like neural networks, I wanted to share my progress.

Feedback and criticism are welcome!"
1jszd7k,[D] Everyday examples of non-linearly separable problems,"I'm trying to think of examples that help to intuitively understand the concept of non-linearly separable problems. For example, determining if two inputs are equal is one such problem, but I'm hoping for something less abstract than that, something that students do themselves without realising.",2025-04-06 19:24:30,17,14,MachineLearning,post,"[D] Everyday examples of non-linearly separable problems
I'm trying to think of examples that help to intuitively understand the concept of non-linearly separable problems. For example, determining if two inputs are equal is one such problem, but I'm hoping for something less abstract than that, something that students do themselves without realising."
1jt77na,[D] Scanning the OpenAI cookbook for vulnerabilities (with open-source),,2025-04-07 01:13:37,4,2,MachineLearning,post,"[D] Scanning the OpenAI cookbook for vulnerabilities (with open-source)
"
1jss0lu,[D]IJCAI 2025 reviews and rebuttal discussion,Thread for discussion,2025-04-06 13:29:40,25,79,MachineLearning,post,"[D]IJCAI 2025 reviews and rebuttal discussion
Thread for discussion"
1jsft3c,[R] NoProp: Training neural networks without back-propagation or forward-propagation,"[https://arxiv.org/pdf/2503.24322](https://arxiv.org/pdf/2503.24322)

  
Abstract  
The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer be- low, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or back- wards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierar- chical representations – at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learn- ing algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gra- dient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process.",2025-04-06 00:46:20,142,34,MachineLearning,post,"[R] NoProp: Training neural networks without back-propagation or forward-propagation
[https://arxiv.org/pdf/2503.24322](https://arxiv.org/pdf/2503.24322)

  
Abstract  
The canonical deep learning approach for learning requires computing a gradient term at each layer by back-propagating the error signal from the output towards each learnable parameter. Given the stacked structure of neural networks, where each layer builds on the representation of the layer be- low, this approach leads to hierarchical representations. More abstract features live on the top layers of the model, while features on lower layers are expected to be less abstract. In contrast to this, we introduce a new learning method named NoProp, which does not rely on either forward or back- wards propagation. Instead, NoProp takes inspiration from diffusion and flow matching methods, where each layer independently learns to denoise a noisy target. We believe this work takes a first step towards introducing a new family of gradient-free learning methods, that does not learn hierar- chical representations – at least not in the usual sense. NoProp needs to fix the representation at each layer beforehand to a noised version of the target, learning a local denoising process that can then be exploited at inference. We demonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100 image classification benchmarks. Our results show that NoProp is a viable learn- ing algorithm which achieves superior accuracy, is easier to use and computationally more efficient compared to other existing back-propagation-free methods. By departing from the traditional gra- dient based learning paradigm, NoProp alters how credit assignment is done within the network, enabling more efficient distributed learning as well as potentially impacting other characteristics of the learning process."
1jsuyxh,[D] How to handle limited space in RAM when training in Google Colab?,"Hello, I am currently trying to solve the [IEEE-CIS Fraud Detection competition](https://www.kaggle.com/competitions/ieee-fraud-detection) on kaggle and I have made myself a Google Colab notebook where I am working with the data. The issue I have is that that while the dataset can just barely fit into memory when I load it into pandas, when I try to do something else with it like data imputation or training a model, the notebook often crashes due to running out of RAM. I've already upgrade to Colab Pro and this gives me 50GB of ram, which helps, but still sometimes is not enough. I wonder if anyone could suggest a better method? Maybe theres some way I could stream the data in from storage bit by bit?

Alternatively is there a better place for me to be working than Colab? My local machine does not have the juice for fast training of models, but I also am financing this myself so the price on Colab Pro is working alright for me (11.38 euros a month), but I would be willing to consider paying more if there's somewhere better to host my notebooks",2025-04-06 16:09:57,6,4,MachineLearning,post,"[D] How to handle limited space in RAM when training in Google Colab?
Hello, I am currently trying to solve the [IEEE-CIS Fraud Detection competition](https://www.kaggle.com/competitions/ieee-fraud-detection) on kaggle and I have made myself a Google Colab notebook where I am working with the data. The issue I have is that that while the dataset can just barely fit into memory when I load it into pandas, when I try to do something else with it like data imputation or training a model, the notebook often crashes due to running out of RAM. I've already upgrade to Colab Pro and this gives me 50GB of ram, which helps, but still sometimes is not enough. I wonder if anyone could suggest a better method? Maybe theres some way I could stream the data in from storage bit by bit?

Alternatively is there a better place for me to be working than Colab? My local machine does not have the juice for fast training of models, but I also am financing this myself so the price on Colab Pro is working alright for me (11.38 euros a month), but I would be willing to consider paying more if there's somewhere better to host my notebooks"
1jsbbuy,[N] Llama 4 release,"[Llama4 ELO score vs cost](https://preview.redd.it/sttz9ok0h2te1.png?width=1525&format=png&auto=webp&s=c57ae7cf9e69afb461237e7b47a8526275223739)

  
[https://www.llama.com/](https://www.llama.com/)",2025-04-05 21:22:16,121,6,MachineLearning,post,"[N] Llama 4 release
[Llama4 ELO score vs cost](https://preview.redd.it/sttz9ok0h2te1.png?width=1525&format=png&auto=webp&s=c57ae7cf9e69afb461237e7b47a8526275223739)

  
[https://www.llama.com/](https://www.llama.com/)"
1jslkhw,"[D] Rich Sutton: Self-Verification, The Key to AI",,2025-04-06 06:01:49,24,5,MachineLearning,post,"[D] Rich Sutton: Self-Verification, The Key to AI
"
1jswwj2,"[Project] ML Model for Predicting Demographic Trends or Anomalies – Seeking Guidance on Model Selection, Validation, and Insights","I’m working on a project that involves building a geospatial analytics system with the following components:

1. **Data Mining**: Scrape and parse city, state, county, and zipcode data from US Census QuickFacts.
2. **Database & Cache**: Load data into PostgreSQL with PostGIS, set up caching with Redis.
3. **Geospatial Visualization**: Use Mapbox or Leaflet.js for interactive maps showing boundaries and demographic details.
4. **Geospatial Queries**: Backend APIs for geofiltering and polygon queries (e.g., nearby cities, demographic trends over time).
5. **Deployment**: Docker or Kubernetes for containerization.

**ML Task**: Integrate an ML model to predict demographic trends or anomalies based on the mined data.

Has anyone implemented something similar or have suggestions for how to approach the ML integration, especially the model selection, validation, and insights?",2025-04-06 17:37:21,2,0,MachineLearning,post,"[Project] ML Model for Predicting Demographic Trends or Anomalies – Seeking Guidance on Model Selection, Validation, and Insights
I’m working on a project that involves building a geospatial analytics system with the following components:

1. **Data Mining**: Scrape and parse city, state, county, and zipcode data from US Census QuickFacts.
2. **Database & Cache**: Load data into PostgreSQL with PostGIS, set up caching with Redis.
3. **Geospatial Visualization**: Use Mapbox or Leaflet.js for interactive maps showing boundaries and demographic details.
4. **Geospatial Queries**: Backend APIs for geofiltering and polygon queries (e.g., nearby cities, demographic trends over time).
5. **Deployment**: Docker or Kubernetes for containerization.

**ML Task**: Integrate an ML model to predict demographic trends or anomalies based on the mined data.

Has anyone implemented something similar or have suggestions for how to approach the ML integration, especially the model selection, validation, and insights?"
1jsw3ey,[R] [D] harmonic clustering a new approach to uncover music listener groups. need feedback/review.,"i recently completed a project called harmonic clustering where we use network science and community detection to uncover natural music listener groups from large scale streaming data.

the twist is we moved away from traditional clustering and came up with a new approach that builds temporal user user graphs based on overlapping playlists and then applies multiple community detection algorithms like louvain label propagation and infomap.

we compared different methods analyzed community purity and visualized the results through clean interactive graphs and this approach turned out to be more robust than the earlier ones we tried.

the main notebook walks through the full pipeline and the repo includes cleaned datasets preprocessing graph generation detection evaluation and visualizations.

repo link : [https://github.com/jacktherizzler/harmonicClustering](https://github.com/jacktherizzler/harmonicClustering)

we are currently writing a paper on this and would love to hear thoughts from people here feel free to try it on your own dataset fork it or drop suggestions we are open to collaborations too.",2025-04-06 17:01:28,2,0,MachineLearning,post,"[R] [D] harmonic clustering a new approach to uncover music listener groups. need feedback/review.
i recently completed a project called harmonic clustering where we use network science and community detection to uncover natural music listener groups from large scale streaming data.

the twist is we moved away from traditional clustering and came up with a new approach that builds temporal user user graphs based on overlapping playlists and then applies multiple community detection algorithms like louvain label propagation and infomap.

we compared different methods analyzed community purity and visualized the results through clean interactive graphs and this approach turned out to be more robust than the earlier ones we tried.

the main notebook walks through the full pipeline and the repo includes cleaned datasets preprocessing graph generation detection evaluation and visualizations.

repo link : [https://github.com/jacktherizzler/harmonicClustering](https://github.com/jacktherizzler/harmonicClustering)

we are currently writing a paper on this and would love to hear thoughts from people here feel free to try it on your own dataset fork it or drop suggestions we are open to collaborations too."
1js1ucr,[D] ICML 2025 - what if reviewers don't acknowledge rebuttal?,"2 out of my 5 reviewers at ICML didn't acknowledge my rebuttal at all. Not only no answer, they also didn't even click the ""acknowledge rebuttal"" at all. According to ICML rules, they are required to do that. What happens when they don't? Should we report this to AC? I didn't find this anywhere, so maybe someone here knows or is in a similar situation.",2025-04-05 13:59:32,40,18,MachineLearning,post,"[D] ICML 2025 - what if reviewers don't acknowledge rebuttal?
2 out of my 5 reviewers at ICML didn't acknowledge my rebuttal at all. Not only no answer, they also didn't even click the ""acknowledge rebuttal"" at all. According to ICML rules, they are required to do that. What happens when they don't? Should we report this to AC? I didn't find this anywhere, so maybe someone here knows or is in a similar situation."
1jsclxw,[P] anyone working on Arabic OCR?,"all the OCRs i tried for Arabic don’t work well at all. i’m really interested in working on building a proper Arabic OCR. if you know anyone working on it or any open projects, please let me know. i’d love to contribute and help improve it.",2025-04-05 22:18:56,5,2,MachineLearning,post,"[P] anyone working on Arabic OCR?
all the OCRs i tried for Arabic don’t work well at all. i’m really interested in working on building a proper Arabic OCR. if you know anyone working on it or any open projects, please let me know. i’d love to contribute and help improve it."
1js6jd9,[Discussion] This might be a really dumb question regarding current training method...,"# So why can't we train a very large network at low quantization, get the lowest test error possible, prune the network at the lowest test error epoch, and then increase the quantization or the remaining parameters to start the training? Wouldn't this allow overcoming getting stuck at the local minima more effectively?

",2025-04-05 17:53:35,9,16,MachineLearning,post,"[Discussion] This might be a really dumb question regarding current training method...
# So why can't we train a very large network at low quantization, get the lowest test error possible, prune the network at the lowest test error epoch, and then increase the quantization or the remaining parameters to start the training? Wouldn't this allow overcoming getting stuck at the local minima more effectively?

"
1js0tvk,[D] Are Domain Adversarial Neural Networks (DANN) used in real world scenarios? Is there anything out there that works?,"I find the idea presented in that paper very attractive, being able to train on one controlled domain, for which it is easy to label data, and ""transfer"" it to another domain which can be quite hard to label the data for.

Be it synthetic/generated data to real data, or office captured data to in the wild data, there's some real value in being able to successfully capturing a domain without labels. Does anyone have some experience with this issue? It sounds too good to be true, it's also not as well known as I'd expect for something so useful, which raises another flag.",2025-04-05 12:54:34,13,9,MachineLearning,post,"[D] Are Domain Adversarial Neural Networks (DANN) used in real world scenarios? Is there anything out there that works?
I find the idea presented in that paper very attractive, being able to train on one controlled domain, for which it is easy to label data, and ""transfer"" it to another domain which can be quite hard to label the data for.

Be it synthetic/generated data to real data, or office captured data to in the wild data, there's some real value in being able to successfully capturing a domain without labels. Does anyone have some experience with this issue? It sounds too good to be true, it's also not as well known as I'd expect for something so useful, which raises another flag."
1jrxh39,KDD 2025 [Cycle 2] Reviews Are Out!,"Hi everyone,

KDD 2025 paper reviews are visible on OpenReview. With the reviews released, I thought I would create a discussion thread to gather thoughts, questions and recommendations or anything else. Would love to hear other people's thoughts on the rating scheme.

Wishing everyone the best!",2025-04-05 08:48:45,23,44,MachineLearning,post,"KDD 2025 [Cycle 2] Reviews Are Out!
Hi everyone,

KDD 2025 paper reviews are visible on OpenReview. With the reviews released, I thought I would create a discussion thread to gather thoughts, questions and recommendations or anything else. Would love to hear other people's thoughts on the rating scheme.

Wishing everyone the best!"
1jrwqa0,[R] Novel Logic-Enhanced LLM for Improved Symbolic Reasoning,"I’m experimenting with a novel approach that integrates symbolic logic directly into a transformer’s attention mechanism. By using a custom spaCy-based logic parser, I generate a “logic mask” that guides the self-attention layers to focus on logical constructs. In preliminary tests with a fine-tuned LLaMA 3 8B model, this method has shown promising improvements on symbolic reasoning tasks (e.g., achieving around 62% on the FOLIO dataset). I’m eager to hear thoughts and suggestions from the community on further refining this approach. Also please note I don’t have a PhD nor masters in machine learning. Happy to take any criticism good or bad. :) ",2025-04-05 07:58:00,22,5,MachineLearning,post,"[R] Novel Logic-Enhanced LLM for Improved Symbolic Reasoning
I’m experimenting with a novel approach that integrates symbolic logic directly into a transformer’s attention mechanism. By using a custom spaCy-based logic parser, I generate a “logic mask” that guides the self-attention layers to focus on logical constructs. In preliminary tests with a fine-tuned LLaMA 3 8B model, this method has shown promising improvements on symbolic reasoning tasks (e.g., achieving around 62% on the FOLIO dataset). I’m eager to hear thoughts and suggestions from the community on further refining this approach. Also please note I don’t have a PhD nor masters in machine learning. Happy to take any criticism good or bad. :) "
1js2ro5,[D] ICASSP 2025,"Hi there, will be attending ICASSP this year. 

Was wondering if there are folks from the community attending the conference as well. Probably we can catch up sometime. 

PS: Has already reached the venue",2025-04-05 14:53:02,5,8,MachineLearning,post,"[D] ICASSP 2025
Hi there, will be attending ICASSP this year. 

Was wondering if there are folks from the community attending the conference as well. Probably we can catch up sometime. 

PS: Has already reached the venue"
1jrxq16,[R] Improving Generalist Reward Models with Self-Principled Critique Tuning and Inference-Time Scaling,"DeepSeek's new reward modeling approach uses inference-time scaling to significantly outperform existing systems. Their DeepSeek Generalist Reward Model (GRM) introduces Self-Principled Critique Tuning, which generates evaluation principles specific to each task before critiquing responses.

Key technical contributions:
* **Self-Principled Critique Tuning (SPCT)** - Adaptation of online RLHF where the model generates principles relevant to each query before critiquing
* **Inference-time scaling** through parallel sampling and meta-reward model voting
* **Pointwise generative reward modeling** that improves over pairwise approaches
* A novel meta-reward model that evaluates and combines multiple evaluations to select the best one

Main results:
* Outperforms other reward models (Claude-2, GPT-4) on MT-Bench and AlpacaEval
* Shows significant gains through inference-time scaling (more samples = better results)
* Effectively handles a diverse range of tasks without developing severe biases
* Demonstrates that inference-time scaling can be more effective than scaling model size

I think this approach represents an important shift in how we think about scaling AI capabilities. Rather than focusing exclusively on larger models and more training data, we could achieve better results through smarter use of compute during inference. This could potentially democratize access to high-quality AI by making it possible to get frontier-level results without enormous training budgets.

The principles-first approach also seems like it could help with interpretability and alignment. By explicitly generating evaluation criteria before making judgments, the model provides more transparency about its decision-making process.

TLDR: DeepSeek-GRM uses a novel approach where the model first generates task-specific principles, then critiques responses based on those principles. Combined with inference-time scaling through parallel sampling, this achieves state-of-the-art results across multiple benchmarks. Their work suggests we might get more bang for our computational buck by scaling inference rather than training.

[Full summary is here](https://aimodels.fyi/papers/arxiv/inference-time-scaling-generalist-reward-modeling). Paper [here](https://arxiv.org/abs/2504.02495).",2025-04-05 09:05:14,6,1,MachineLearning,post,"[R] Improving Generalist Reward Models with Self-Principled Critique Tuning and Inference-Time Scaling
DeepSeek's new reward modeling approach uses inference-time scaling to significantly outperform existing systems. Their DeepSeek Generalist Reward Model (GRM) introduces Self-Principled Critique Tuning, which generates evaluation principles specific to each task before critiquing responses.

Key technical contributions:
* **Self-Principled Critique Tuning (SPCT)** - Adaptation of online RLHF where the model generates principles relevant to each query before critiquing
* **Inference-time scaling** through parallel sampling and meta-reward model voting
* **Pointwise generative reward modeling** that improves over pairwise approaches
* A novel meta-reward model that evaluates and combines multiple evaluations to select the best one

Main results:
* Outperforms other reward models (Claude-2, GPT-4) on MT-Bench and AlpacaEval
* Shows significant gains through inference-time scaling (more samples = better results)
* Effectively handles a diverse range of tasks without developing severe biases
* Demonstrates that inference-time scaling can be more effective than scaling model size

I think this approach represents an important shift in how we think about scaling AI capabilities. Rather than focusing exclusively on larger models and more training data, we could achieve better results through smarter use of compute during inference. This could potentially democratize access to high-quality AI by making it possible to get frontier-level results without enormous training budgets.

The principles-first approach also seems like it could help with interpretability and alignment. By explicitly generating evaluation criteria before making judgments, the model provides more transparency about its decision-making process.

TLDR: DeepSeek-GRM uses a novel approach where the model first generates task-specific principles, then critiques responses based on those principles. Combined with inference-time scaling through parallel sampling, this achieves state-of-the-art results across multiple benchmarks. Their work suggests we might get more bang for our computational buck by scaling inference rather than training.

[Full summary is here](https://aimodels.fyi/papers/arxiv/inference-time-scaling-generalist-reward-modeling). Paper [here](https://arxiv.org/abs/2504.02495)."
1js3jcl,[P] A tool to create a ranked list of projects in ML/AI for CS students,"## TL; DR
> This is still work in progress, but I want to hear your early feedback!

Inspired by a recent post by Neel Nanda on [Research Directions in explainable AI](https://www.lesswrong.com/posts/qGKq4G3HGRcSBc94C/mats-applications-research-directions-i-m-currently-excited), I'm building a tool that extracts projects from ICLR 2025 and uses tournament-like ranking of them based on how impactful they are, you can find them here https://openreview-copilot.eamag.me/projects. There are many ways to improve it, but I want to get your early feedback on how useful it is and what are the most important things to iterate on.
## Why

I think the best way to learn things is by building something. People in universities are building simple apps to learn how to code, for example. Won't it be better if they were building something that's more useful for the world? I'm extracting projects from recent ML papers based on different level of competency, from no-coding to PhD. I rank undergraduate-level projects (mostly in explainable AI area, but also just top ranked papers from that conference) to find the most useful. More details on the motivation and implementation are here https://eamag.me/2025/Paper-To-Project

We can probably increase the speed of research in AI alignment by involving more people in it, and to do so we have to lower the barriers of entry, and prove that the things people can work on are actually meaningful. The ranking now is subjective and automatic, but it's possible to add another (weighed) voting system on top to rerank projects based on researchers' intuition.
## Call to action

- Tell me if I'm missing something in the motivation section
- Take a look at projects and corresponding papers
- Suggest how to make it more helpful and actually used by people
- There are many improvements to be made, from better projects extraction and ranking, to UI and promotion. Help me prioritize them and get involved!",2025-04-05 15:32:15,1,0,MachineLearning,post,"[P] A tool to create a ranked list of projects in ML/AI for CS students
## TL; DR
> This is still work in progress, but I want to hear your early feedback!

Inspired by a recent post by Neel Nanda on [Research Directions in explainable AI](https://www.lesswrong.com/posts/qGKq4G3HGRcSBc94C/mats-applications-research-directions-i-m-currently-excited), I'm building a tool that extracts projects from ICLR 2025 and uses tournament-like ranking of them based on how impactful they are, you can find them here https://openreview-copilot.eamag.me/projects. There are many ways to improve it, but I want to get your early feedback on how useful it is and what are the most important things to iterate on.
## Why

I think the best way to learn things is by building something. People in universities are building simple apps to learn how to code, for example. Won't it be better if they were building something that's more useful for the world? I'm extracting projects from recent ML papers based on different level of competency, from no-coding to PhD. I rank undergraduate-level projects (mostly in explainable AI area, but also just top ranked papers from that conference) to find the most useful. More details on the motivation and implementation are here https://eamag.me/2025/Paper-To-Project

We can probably increase the speed of research in AI alignment by involving more people in it, and to do so we have to lower the barriers of entry, and prove that the things people can work on are actually meaningful. The ranking now is subjective and automatic, but it's possible to add another (weighed) voting system on top to rerank projects based on researchers' intuition.
## Call to action

- Tell me if I'm missing something in the motivation section
- Take a look at projects and corresponding papers
- Suggest how to make it more helpful and actually used by people
- There are many improvements to be made, from better projects extraction and ranking, to UI and promotion. Help me prioritize them and get involved!"
1jribqw,[R] How Do Large Language Monkeys Get Their Power (Laws)?,,2025-04-04 20:01:07,12,4,MachineLearning,post,"[R] How Do Large Language Monkeys Get Their Power (Laws)?
"
1jr6iqj,[R] Anthropic: Reasoning Models Don’t Always Say What They Think,">Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT mon itoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.

Another paper about AI alignment from anthropic (has a pdf version this time around) that seems to point out how ""reasoning models"" that use CoT seem to lie to users. Very interesting paper.

  
Paper link: [reasoning\_models\_paper.pdf](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)",2025-04-04 09:52:34,71,53,MachineLearning,post,"[R] Anthropic: Reasoning Models Don’t Always Say What They Think
>Chain-of-thought (CoT) offers a potential boon for AI safety as it allows monitoring a model’s CoT to try to understand its intentions and reasoning processes. However, the effectiveness of such monitoring hinges on CoTs faithfully representing models’ actual reasoning processes. We evaluate CoT faithfulness of state-of-the-art reasoning models across 6 reasoning hints presented in the prompts and find: (1) for most settings and models tested, CoTs reveal their usage of hints in at least 1% of examples where they use the hint, but the reveal rate is often below 20%, (2) outcome-based reinforcement learning initially improves faithfulness but plateaus without saturating, and (3) when reinforcement learning increases how frequently hints are used (reward hacking), the propensity to verbalize them does not increase, even without training against a CoT monitor. These results suggest that CoT mon itoring is a promising way of noticing undesired behaviors during training and evaluations, but that it is not sufficient to rule them out. They also suggest that in settings like ours where CoT reasoning is not necessary, test-time monitoring of CoTs is unlikely to reliably catch rare and catastrophic unexpected behaviors.

Another paper about AI alignment from anthropic (has a pdf version this time around) that seems to point out how ""reasoning models"" that use CoT seem to lie to users. Very interesting paper.

  
Paper link: [reasoning\_models\_paper.pdf](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf)"
1jrd9tc,[R] Mitigating Real-World Distribution Shifts in the Fourier Domain (TMLR),"TLDR: Do unsupervised domain adaption by simply matching the frequency statistics of train and test domain samples - no labels needed. Works for vision, audio, time-series. paper (with code): [https://openreview.net/forum?id=lu4oAq55iK](https://openreview.net/forum?id=lu4oAq55iK)",2025-04-04 16:29:54,19,4,MachineLearning,post,"[R] Mitigating Real-World Distribution Shifts in the Fourier Domain (TMLR)
TLDR: Do unsupervised domain adaption by simply matching the frequency statistics of train and test domain samples - no labels needed. Works for vision, audio, time-series. paper (with code): [https://openreview.net/forum?id=lu4oAq55iK](https://openreview.net/forum?id=lu4oAq55iK)"
1jr8klg,What is your practical NER (Named Entity Recognition) approach? [P],"Hi all,

I'm working on a Flutter app that scans food products using OCR (Google ML Kit) to extract text from an image, recognizes the language and translate it to English. This works. The next challenge is however **structuring the extracted text** into meaningful parts, so for example:

* **Title**
* **Nutrition Facts**
* **Brand** 
* **etc.**

The goal would be to extract those and automatically fill the form for a user.

Right now, I use **rule-based parsing (regex + keywords like ""Calories"")**, but it's unreliable for unstructured text and gives messy results. I really like the Google ML kit that is offline, so no internet and no subscriptions or calls to an external company. I thought of a few potential approaches for extracting this structured text:

1. **Pure regex/rule-based parsing** → Simple but fails with unstructured text. (so maybe not the best solution)
2. **Make my own model and train it to perform NER (Named Entity Recognition)**  →  One thing, I have never trained any model and am a noob in this AI / ML thing. 
3. **External APIs** → Google Cloud NLP, [Wit.ai](http://Wit.ai), etc. (but this I really would prefer to avoid to save costs)

**Which method would you recommend?** I am sure I maybe miss some approach and would love to hear how you all tackle similar problems! I am willing to spend time btw into AI/ML but of course I'm looking to spend my time efficient.

Any reference or info is highly appreciated!",2025-04-04 12:23:19,23,14,MachineLearning,post,"What is your practical NER (Named Entity Recognition) approach? [P]
Hi all,

I'm working on a Flutter app that scans food products using OCR (Google ML Kit) to extract text from an image, recognizes the language and translate it to English. This works. The next challenge is however **structuring the extracted text** into meaningful parts, so for example:

* **Title**
* **Nutrition Facts**
* **Brand** 
* **etc.**

The goal would be to extract those and automatically fill the form for a user.

Right now, I use **rule-based parsing (regex + keywords like ""Calories"")**, but it's unreliable for unstructured text and gives messy results. I really like the Google ML kit that is offline, so no internet and no subscriptions or calls to an external company. I thought of a few potential approaches for extracting this structured text:

1. **Pure regex/rule-based parsing** → Simple but fails with unstructured text. (so maybe not the best solution)
2. **Make my own model and train it to perform NER (Named Entity Recognition)**  →  One thing, I have never trained any model and am a noob in this AI / ML thing. 
3. **External APIs** → Google Cloud NLP, [Wit.ai](http://Wit.ai), etc. (but this I really would prefer to avoid to save costs)

**Which method would you recommend?** I am sure I maybe miss some approach and would love to hear how you all tackle similar problems! I am willing to spend time btw into AI/ML but of course I'm looking to spend my time efficient.

Any reference or info is highly appreciated!"
1jrn64e,[R] Do you include blank ground truth masks in MRI segmentation evaluation?,"So I am currently working on a u-net model that does MRI segmentation. There are about \~10% of the test dataset currently that include blank ground truth masks (near the top and bottom part of the target structure). The evaluation changes drastically based on whether I include these blank-ground-truth-mask MRI slices. I read for BraTS, they do include them for brain tumor segmentation and penalize any false positives with a 0 dice score.

What is the common approach for research papers when it comes to evaluation? Is the BraTS approach the universal approach or do you just exclude all blank ground truth mask slices near the target structure when evaluating?",2025-04-04 23:26:20,1,0,MachineLearning,post,"[R] Do you include blank ground truth masks in MRI segmentation evaluation?
So I am currently working on a u-net model that does MRI segmentation. There are about \~10% of the test dataset currently that include blank ground truth masks (near the top and bottom part of the target structure). The evaluation changes drastically based on whether I include these blank-ground-truth-mask MRI slices. I read for BraTS, they do include them for brain tumor segmentation and penalize any false positives with a 0 dice score.

What is the common approach for research papers when it comes to evaluation? Is the BraTS approach the universal approach or do you just exclude all blank ground truth mask slices near the target structure when evaluating?"
1jr6xwi,[R] Scaling Language-Free Visual Representation Learning,"New paper from FAIR+NYU:
Pure Self-Supervised Learning such as DINO can beat CLIP-style supervised methods on image recognition tasks because the performance scales well with architecture size and dataset size.",2025-04-04 10:24:16,11,0,MachineLearning,post,"[R] Scaling Language-Free Visual Representation Learning
New paper from FAIR+NYU:
Pure Self-Supervised Learning such as DINO can beat CLIP-style supervised methods on image recognition tasks because the performance scales well with architecture size and dataset size."
1jrkbny,[D] Better data batching causes slower computing,"For my research, I am running some LLMs on a middle-end desktop GPU. I figured that batching the matrices is generally not a bad idea, at best it would make more things run in parallel and might cut some overhead that I missed, at worst I wouldn't lose anything. And I wrote algorithms so that they batch all data for GPU computing that they can. Then I fiddled with batch sizes and found that apparently the shorter each batch is, the faster the whole dataset is processed. This fact holds ~~the whole range from effectively no batching~~ from minimal reasonable batching to maximum VRAM utilization. And this is very noticable, the difference in speed between extremes is almost 2 times.

upd: actually looks like total absense of batching does slow down computing compared to very small batches for some algorithms, at least there is some explanation for that

I am very confused (and frustrated from apparently having wasted time). I could only think of unnesseccary data copies being done somewhere, but by this point I am pretty sure it doesn't happen to the ""hefty"" matrices.

(The GPU is NVIDIA RTX 30.., used via CUDA. I haven't had prior experience with GPU computing. I believe this is the most appropriate sub for this post.)",2025-04-04 21:25:11,1,1,MachineLearning,post,"[D] Better data batching causes slower computing
For my research, I am running some LLMs on a middle-end desktop GPU. I figured that batching the matrices is generally not a bad idea, at best it would make more things run in parallel and might cut some overhead that I missed, at worst I wouldn't lose anything. And I wrote algorithms so that they batch all data for GPU computing that they can. Then I fiddled with batch sizes and found that apparently the shorter each batch is, the faster the whole dataset is processed. This fact holds ~~the whole range from effectively no batching~~ from minimal reasonable batching to maximum VRAM utilization. And this is very noticable, the difference in speed between extremes is almost 2 times.

upd: actually looks like total absense of batching does slow down computing compared to very small batches for some algorithms, at least there is some explanation for that

I am very confused (and frustrated from apparently having wasted time). I could only think of unnesseccary data copies being done somewhere, but by this point I am pretty sure it doesn't happen to the ""hefty"" matrices.

(The GPU is NVIDIA RTX 30.., used via CUDA. I haven't had prior experience with GPU computing. I believe this is the most appropriate sub for this post.)"
1jrjmyp,[R]: Can we learn with fewer parameters than an MLP?,"**Answer: Yes.**

**STFT-KAN**

* **arXiv:** [https://arxiv.org/abs/2503.23647](https://arxiv.org/abs/2503.23647)
* **GitHub:** [https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN](https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN)",2025-04-04 20:56:20,1,0,MachineLearning,post,"[R]: Can we learn with fewer parameters than an MLP?
**Answer: Yes.**

**STFT-KAN**

* **arXiv:** [https://arxiv.org/abs/2503.23647](https://arxiv.org/abs/2503.23647)
* **GitHub:** [https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN](https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN)"
1jqolkh,AI tools for ML Research - what am I missing? [D],"AI/ML Researchers who still code experiments and write papers. What tools have you started using in day-to-day workflow? I think it is way different what other SWE/MLE uses for their work.

What I use -

- Cursor (w/ sonnet, gemini) for writing codes for experiments and basically designing the entire pipeline. Using it since 2-3 months and feels great.

- NotebookLM / some other text-to-audio summarisers for reading papers daily.

- Sonnet/DeepSeak has been good for technical writing work.

- Gemini Deep Research (also Perplexity) for finding references and day to day search.

Feel free to add more!
",2025-04-03 19:30:11,74,35,MachineLearning,post,"AI tools for ML Research - what am I missing? [D]
AI/ML Researchers who still code experiments and write papers. What tools have you started using in day-to-day workflow? I think it is way different what other SWE/MLE uses for their work.

What I use -

- Cursor (w/ sonnet, gemini) for writing codes for experiments and basically designing the entire pipeline. Using it since 2-3 months and feels great.

- NotebookLM / some other text-to-audio summarisers for reading papers daily.

- Sonnet/DeepSeak has been good for technical writing work.

- Gemini Deep Research (also Perplexity) for finding references and day to day search.

Feel free to add more!
"
1jratgd,[P] Looking for NLP approaches to extract machine-readable rules from building regulations,"Hey everyone,

I'm working on a project and could use some help. I'm trying to build a system that reads building codes (like German DIN standards) and converts them into a machine-readable format, so I can automatically check BIM models for code compliance.

I found a paper that does something similar:

Automated Code Compliance Checking Based on BIM and Knowledge Graph

They use:

* NLP (with CRF models) to extract entities, attributes, and relationships
* A knowledge graph built in Neo4j
* BIM models converted from IFC to RDF
* SPARQL queries to check if the model follows the rules

The problem I’m facing is I can’t find:

* Any pretrained NLP models for construction codes or technical/legal standards
* Annotated datasets to train one (even general regulation/legal text would help)
* Tools that help turn these kinds of regulations into structured, machine-readable rules

I've already got access to the regulations and scraped a bunch, but I’m stuck on how to actually extract the logic or rules from the text.

If anyone has worked on something similar or knows of useful datasets, tools, or approaches, I’d really appreciate it!

Thanks in advance.",2025-04-04 14:36:31,2,0,MachineLearning,post,"[P] Looking for NLP approaches to extract machine-readable rules from building regulations
Hey everyone,

I'm working on a project and could use some help. I'm trying to build a system that reads building codes (like German DIN standards) and converts them into a machine-readable format, so I can automatically check BIM models for code compliance.

I found a paper that does something similar:

Automated Code Compliance Checking Based on BIM and Knowledge Graph

They use:

* NLP (with CRF models) to extract entities, attributes, and relationships
* A knowledge graph built in Neo4j
* BIM models converted from IFC to RDF
* SPARQL queries to check if the model follows the rules

The problem I’m facing is I can’t find:

* Any pretrained NLP models for construction codes or technical/legal standards
* Annotated datasets to train one (even general regulation/legal text would help)
* Tools that help turn these kinds of regulations into structured, machine-readable rules

I've already got access to the regulations and scraped a bunch, but I’m stuck on how to actually extract the logic or rules from the text.

If anyone has worked on something similar or knows of useful datasets, tools, or approaches, I’d really appreciate it!

Thanks in advance."
1jqqlxc,"[N] Open-data reasoning model, trained on curated supervised fine-tuning (SFT) dataset, outperforms DeepSeekR1. Big win for the open source community","Open Thoughts initiative was announced in late January with the goal of surpassing DeepSeek’s 32B model and releasing the associated training data, (something DeepSeek had not done).  
Previously, team had released the OpenThoughts-114k dataset, which was used to train the OpenThinker-32B model that closely matched the performance of DeepSeek-32B. Today, they have achieved their objective with the release of OpenThinker2-32B, a model that outperforms DeepSeek-32B. They are open-sourcing 1 million high-quality SFT examples used in its training.  
The earlier 114k dataset gained significant traction(500k downloads on HF).  
With this new model, they showed that just a bigger dataset was all it took to beat deepseekR1.   
RL would give even better results I am guessing",2025-04-03 20:46:04,44,5,MachineLearning,post,"[N] Open-data reasoning model, trained on curated supervised fine-tuning (SFT) dataset, outperforms DeepSeekR1. Big win for the open source community
Open Thoughts initiative was announced in late January with the goal of surpassing DeepSeek’s 32B model and releasing the associated training data, (something DeepSeek had not done).  
Previously, team had released the OpenThoughts-114k dataset, which was used to train the OpenThinker-32B model that closely matched the performance of DeepSeek-32B. Today, they have achieved their objective with the release of OpenThinker2-32B, a model that outperforms DeepSeek-32B. They are open-sourcing 1 million high-quality SFT examples used in its training.  
The earlier 114k dataset gained significant traction(500k downloads on HF).  
With this new model, they showed that just a bigger dataset was all it took to beat deepseekR1.   
RL would give even better results I am guessing"
1jr68a2,"[P] Simpler/faster data domains to benchmark transformers on, when experimenting?","Does anyone have any recommendations on simple datasets and domains that work well for benchmarking the efficacy of modified transformers? Language models require too much training to produce legible results, and so contrasting a poorly trained language model to another poorly trained language model can give misleading or conterintuitive results that may not actually reflect real world performance when trained at a scale where the language model is producing useful predictions. So I'm trying to find a simpler, lower dimensional data domain that a transformer can excel at very quickly, so I can iterate quickly. ",2025-04-04 09:30:53,4,1,MachineLearning,post,"[P] Simpler/faster data domains to benchmark transformers on, when experimenting?
Does anyone have any recommendations on simple datasets and domains that work well for benchmarking the efficacy of modified transformers? Language models require too much training to produce legible results, and so contrasting a poorly trained language model to another poorly trained language model can give misleading or conterintuitive results that may not actually reflect real world performance when trained at a scale where the language model is producing useful predictions. So I'm trying to find a simpler, lower dimensional data domain that a transformer can excel at very quickly, so I can iterate quickly. "
1jqojkv,[R] Position: Model Collapse Does Not Mean What You Think,"- The proliferation of AI-generated content online has fueled concerns over __model collapse__, a degradation in future generative models' performance when trained on synthetic data generated by earlier models.
- We contend this widespread narrative fundamentally misunderstands the scientific evidence
- We highlight that research on model collapse actually encompasses eight distinct and at times conflicting definitions of model collapse, and argue that inconsistent terminology within and between papers has hindered building a comprehensive understanding of model collapse
- We posit what we believe are realistic conditions for studying model collapse and then conduct a rigorous assessment of the literature's methodologies through this lens
- Our analysis of research studies, weighted by how faithfully each study matches real-world conditions, leads us to conclude that certain predicted claims of model collapse rely on assumptions and conditions that poorly match real-world conditions,
- Altogether, this position paper argues that model collapse has been warped from a nuanced multifaceted consideration into an oversimplified threat, and that the evidence suggests specific harms more likely under society's current trajectory have received disproportionately less attention",2025-04-03 19:28:09,31,11,MachineLearning,post,"[R] Position: Model Collapse Does Not Mean What You Think
- The proliferation of AI-generated content online has fueled concerns over __model collapse__, a degradation in future generative models' performance when trained on synthetic data generated by earlier models.
- We contend this widespread narrative fundamentally misunderstands the scientific evidence
- We highlight that research on model collapse actually encompasses eight distinct and at times conflicting definitions of model collapse, and argue that inconsistent terminology within and between papers has hindered building a comprehensive understanding of model collapse
- We posit what we believe are realistic conditions for studying model collapse and then conduct a rigorous assessment of the literature's methodologies through this lens
- Our analysis of research studies, weighted by how faithfully each study matches real-world conditions, leads us to conclude that certain predicted claims of model collapse rely on assumptions and conditions that poorly match real-world conditions,
- Altogether, this position paper argues that model collapse has been warped from a nuanced multifaceted consideration into an oversimplified threat, and that the evidence suggests specific harms more likely under society's current trajectory have received disproportionately less attention"
1jql3hx,[D] UAI 2025 Reviews Waiting Place,"A place to share your thoughts, prayers, and, most importantly (once the reviews are out, should be soon...), rants or maybe even some relieved comments. Good luck everyone!",2025-04-03 17:15:13,26,39,MachineLearning,post,"[D] UAI 2025 Reviews Waiting Place
A place to share your thoughts, prayers, and, most importantly (once the reviews are out, should be soon...), rants or maybe even some relieved comments. Good luck everyone!"
1jqp77y,"[R] For those of you who are familiar with Kolmogorov Arnold Networks and the Meijer-G function, is representing the B-Spline using a Meijer-G function possible?","As the title suggests, I wanted to know if a B-Spline for a given grid can be represented using a Meijer-G function? Or is there any way by which the exact parameters for the Meijer-G function can be found that can replicate the B-Spline of a given grid? I am trying to build a neural network as part of my research thesis that is inspired by the KAN, but instead uses the Meijer-G function as trainable activation functions. If there is a plausible way to represent the B-Spline using the Meijer function it would help me a lot in framing my proposition. Thanks in advance!",2025-04-03 19:52:57,9,2,MachineLearning,post,"[R] For those of you who are familiar with Kolmogorov Arnold Networks and the Meijer-G function, is representing the B-Spline using a Meijer-G function possible?
As the title suggests, I wanted to know if a B-Spline for a given grid can be represented using a Meijer-G function? Or is there any way by which the exact parameters for the Meijer-G function can be found that can replicate the B-Spline of a given grid? I am trying to build a neural network as part of my research thesis that is inspired by the KAN, but instead uses the Meijer-G function as trainable activation functions. If there is a plausible way to represent the B-Spline using the Meijer function it would help me a lot in framing my proposition. Thanks in advance!"
1jqns7l,[R] Speech to text summarisation - optimised model ideas,"Hi, I'm a cs major who choose speech to text summarisation as my honors topic because I wanted to pick something from machine learning field so that I could improve my understanding.

The primary goal is to implement the speech to text transcription model (summarisation one will be implemented next sem) but I also want to make some changes to the already existing model's architecture so that it'll be a little efficient(also identifying where current models lack like high latency, poor speaker diarization etc. is also another work to do) .

Although I have some experience in other ml topics this a complete new field for me and so I want some resources ( datasets and recent papers etc) which help me score some good marks at my honors review",2025-04-03 18:59:27,4,0,MachineLearning,post,"[R] Speech to text summarisation - optimised model ideas
Hi, I'm a cs major who choose speech to text summarisation as my honors topic because I wanted to pick something from machine learning field so that I could improve my understanding.

The primary goal is to implement the speech to text transcription model (summarisation one will be implemented next sem) but I also want to make some changes to the already existing model's architecture so that it'll be a little efficient(also identifying where current models lack like high latency, poor speaker diarization etc. is also another work to do) .

Although I have some experience in other ml topics this a complete new field for me and so I want some resources ( datasets and recent papers etc) which help me score some good marks at my honors review"
1jqyf03,[R]Struggling to Pick the Right XAI Method for CNN in Medical Imaging,"**Hey everyone!**  
I’m working on my thesis about using Explainable AI (XAI) for pneumonia detection with CNNs. The goal is to make model predictions more transparent and trustworthy—especially for clinicians—by showing *why* a chest X-ray is classified as pneumonia or not.

I’m currently exploring different XAI methods like Grad-CAM, LIME, and SHAP, but I’m struggling to decide which one best explains my model’s decisions.

Would love to hear your thoughts or experiences with XAI in medical imaging. Any suggestions or insights would be super helpful!",2025-04-04 02:06:15,0,12,MachineLearning,post,"[R]Struggling to Pick the Right XAI Method for CNN in Medical Imaging
**Hey everyone!**  
I’m working on my thesis about using Explainable AI (XAI) for pneumonia detection with CNNs. The goal is to make model predictions more transparent and trustworthy—especially for clinicians—by showing *why* a chest X-ray is classified as pneumonia or not.

I’m currently exploring different XAI methods like Grad-CAM, LIME, and SHAP, but I’m struggling to decide which one best explains my model’s decisions.

Would love to hear your thoughts or experiences with XAI in medical imaging. Any suggestions or insights would be super helpful!"
1jqn1xb,[P] Privately Hosted LLM (HIPAA Compliant),"Hey everyone, I need to parse text prompts from users and map them to a defined list of categories. We don't want to use a public API for data privacy reasons as well as having more control over the mapping. Also, this is healthcare related.

What are some resources I should use to start researching solutions for this? My immediate thought is to download the best general purpose open source LLM, throw it in an EC2 instance and do some prompt engineering to start with. I've built and deployed simpler ML models before but I've never deployed LLMs locally or in the cloud.

Any help is appreciated to get me started down this path. Thanks!",2025-04-03 18:31:03,2,4,MachineLearning,post,"[P] Privately Hosted LLM (HIPAA Compliant)
Hey everyone, I need to parse text prompts from users and map them to a defined list of categories. We don't want to use a public API for data privacy reasons as well as having more control over the mapping. Also, this is healthcare related.

What are some resources I should use to start researching solutions for this? My immediate thought is to download the best general purpose open source LLM, throw it in an EC2 instance and do some prompt engineering to start with. I've built and deployed simpler ML models before but I've never deployed LLMs locally or in the cloud.

Any help is appreciated to get me started down this path. Thanks!"
1jqevkl,[D] Anyone got reviews for the paper submitted to AIED 2025 conference,"Anyone got reviews for the paper submitted to AIED 2025 conference? I am yet to receive mine while few others have already got it. Have mailed chairs but doubt if I will get any reply. Anyone connected to AIED 2025, if you can reply here it would be super good.",2025-04-03 12:28:04,11,4,MachineLearning,post,"[D] Anyone got reviews for the paper submitted to AIED 2025 conference
Anyone got reviews for the paper submitted to AIED 2025 conference? I am yet to receive mine while few others have already got it. Have mailed chairs but doubt if I will get any reply. Anyone connected to AIED 2025, if you can reply here it would be super good."
1jqh8zw,[D] Fine-tuning a fine-tuned YOLO model?,"I have a semi annotated dataset(<1500 images), which I annotated using some automation. I also have a small fully annotated dataset(100-200 images derived from semi annotated dataset after I corrected incorrect bbox), and each image has \~100 bboxes(5 classes).

I am thinking of using YOLO11s or YOLO11m(not yet decided), for me the accuracy is more important than inference time.

So is it better to only fine-tune the pretrained YOLO11 model with the small fully annotated dataset or

First fine-tune the pretrained YOLO11 model on semi annotated dataset and then again fine-tune it on fully annotated dataset?",2025-04-03 14:34:48,6,5,MachineLearning,post,"[D] Fine-tuning a fine-tuned YOLO model?
I have a semi annotated dataset(<1500 images), which I annotated using some automation. I also have a small fully annotated dataset(100-200 images derived from semi annotated dataset after I corrected incorrect bbox), and each image has \~100 bboxes(5 classes).

I am thinking of using YOLO11s or YOLO11m(not yet decided), for me the accuracy is more important than inference time.

So is it better to only fine-tune the pretrained YOLO11 model with the small fully annotated dataset or

First fine-tune the pretrained YOLO11 model on semi annotated dataset and then again fine-tune it on fully annotated dataset?"
1jqettp,[D] Time series models with custom loss,"
Suppose I have a time-series prediction problem, where the loss between the model's prediction and the true outcome is some custom loss function l(x, y).

Is there some theory of how the standard ARMA / ARIMA models should be modified? For example, if the loss is not measuring the additive deviation, the ""error"" term in the MA part of ARMA may not be additive, but something else. Is it also not obvious what would be the generalized counterpoarts of the standard stationarity conditions in this setting.

I was looking for literature, but the only thing I found was a theory specially tailored towards Poisson time series. But nothing for more general cost functions.",2025-04-03 12:24:59,4,2,MachineLearning,post,"[D] Time series models with custom loss

Suppose I have a time-series prediction problem, where the loss between the model's prediction and the true outcome is some custom loss function l(x, y).

Is there some theory of how the standard ARMA / ARIMA models should be modified? For example, if the loss is not measuring the additive deviation, the ""error"" term in the MA part of ARMA may not be additive, but something else. Is it also not obvious what would be the generalized counterpoarts of the standard stationarity conditions in this setting.

I was looking for literature, but the only thing I found was a theory specially tailored towards Poisson time series. But nothing for more general cost functions."
1jq04ri,[D] Are you happy with the ICML discussion period?,"Are you happy with the ICML discussion period?

  
My reviewers just mentioned that they have acknowledged my rebuttals.

  
I'm not sure the ""Rebuttal Acknowledgement"" button really helped get the reviewers engaged.",2025-04-02 23:17:42,53,76,MachineLearning,post,"[D] Are you happy with the ICML discussion period?
Are you happy with the ICML discussion period?

  
My reviewers just mentioned that they have acknowledged my rebuttals.

  
I'm not sure the ""Rebuttal Acknowledgement"" button really helped get the reviewers engaged."
1jqdl35,[P] Looking for resources on simulating social phenomena with LLM,"I want to simulate social phenomena using LLM agents. However, since my major is in computer science, I have no background in social sciences.   
Are there any recommended resources or researchers working in this area? For example, something related to modeling changes in people's states or transformations in our world.

I think the list below is a good starting point. Let me know if you have anything even better!  
\- Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?  
\- AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society  
\- Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies  
\- Generative Agent Simulations of 1,000 People",2025-04-03 11:04:23,5,1,MachineLearning,post,"[P] Looking for resources on simulating social phenomena with LLM
I want to simulate social phenomena using LLM agents. However, since my major is in computer science, I have no background in social sciences.   
Are there any recommended resources or researchers working in this area? For example, something related to modeling changes in people's states or transformations in our world.

I think the list below is a good starting point. Let me know if you have anything even better!  
\- Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?  
\- AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society  
\- Using Large Language Models to Simulate Multiple Humans and Replicate Human Subject Studies  
\- Generative Agent Simulations of 1,000 People"
1jpwbag,[R] Neuron-based explanations of neural networks sacrifice completeness and interpretability (TMLR 2025),"**TL;DR:** The most important principal components provide more complete and interpretable explanations than the most important neurons.

This work has a fun interactive online demo to play around with:  
[https://ndey96.github.io/neuron-explanations-sacrifice/](https://ndey96.github.io/neuron-explanations-sacrifice/)

https://preview.redd.it/ndeycnxjvgse1.png?width=1132&format=png&auto=webp&s=a41da955dd93f287bf2cbf0a291489b22d9dc2f0

  
",2025-04-02 20:42:50,54,5,MachineLearning,post,"[R] Neuron-based explanations of neural networks sacrifice completeness and interpretability (TMLR 2025)
**TL;DR:** The most important principal components provide more complete and interpretable explanations than the most important neurons.

This work has a fun interactive online demo to play around with:  
[https://ndey96.github.io/neuron-explanations-sacrifice/](https://ndey96.github.io/neuron-explanations-sacrifice/)

https://preview.redd.it/ndeycnxjvgse1.png?width=1132&format=png&auto=webp&s=a41da955dd93f287bf2cbf0a291489b22d9dc2f0

  
"
1jplhtl,[R] Implemented 18 RL Algorithms in a Simpler Way,"I decided to create a comprehensive learning project in a Jupyter Notebook to implement RL Algorithms such as PPO, SAC, A3C and more. (Theory + Code).

Code, documentation, and example can all be found on GitHub:

[https://github.com/FareedKhan-dev/all-rl-algorithms](https://github.com/FareedKhan-dev/all-rl-algorithms)",2025-04-02 12:40:46,152,12,MachineLearning,post,"[R] Implemented 18 RL Algorithms in a Simpler Way
I decided to create a comprehensive learning project in a Jupyter Notebook to implement RL Algorithms such as PPO, SAC, A3C and more. (Theory + Code).

Code, documentation, and example can all be found on GitHub:

[https://github.com/FareedKhan-dev/all-rl-algorithms](https://github.com/FareedKhan-dev/all-rl-algorithms)"
1jq5uid,"[R] Patronus AI, Columbia University and Meta release BLUR benchmark for tip-of-the-tongue retrieval evaluation for agents",Hugging Face dataset: https://huggingface.co/datasets/PatronusAI/BLUR,2025-04-03 03:33:59,8,0,MachineLearning,post,"[R] Patronus AI, Columbia University and Meta release BLUR benchmark for tip-of-the-tongue retrieval evaluation for agents
Hugging Face dataset: https://huggingface.co/datasets/PatronusAI/BLUR"
1jpo78g,[D] Relevance of Minimum Description Length to understanding how Deep Learning really works,"There's a subfield of statistics called Minimum Description Length. Do you think it has a relevance to understanding not very well explained phenomena of why deep learning works, i.e. why overparameterized networks don't overfit, why double descent happens, why transformers works so well, and what really happens inside ofweights, etc. If so, what are the recent publications to read on?

P.S. I got interested since there's a link to a chapter of a book, related to this on the famous Shutskever reading list. ",2025-04-02 15:11:15,26,15,MachineLearning,post,"[D] Relevance of Minimum Description Length to understanding how Deep Learning really works
There's a subfield of statistics called Minimum Description Length. Do you think it has a relevance to understanding not very well explained phenomena of why deep learning works, i.e. why overparameterized networks don't overfit, why double descent happens, why transformers works so well, and what really happens inside ofweights, etc. If so, what are the recent publications to read on?

P.S. I got interested since there's a link to a chapter of a book, related to this on the famous Shutskever reading list. "
1jpthoa,"[Project] Open-source OCR system for creating educational ML datasets (math, multilingual, tables, diagrams)","Hi everyone,

I’ve open-sourced an OCR pipeline designed to extract structured, machine learning-ready data from complex educational documents. It’s built with a focus on academic content such as entrance exams, scientific PDFs, and textbooks — handling not just plain text but also math formulas, multilingual content, tables, and figures.

Core Capabilities
	•	Multilingual OCR (supports English, Korean, Japanese — easily extensible)
	•	Math recognition using MathPix API (LaTeX-style precision)
	•	Layout parsing with DocLayout-YOLO and OpenCV for detecting tables and diagrams
	•	Semantic postprocessing using GPT-4 / Gemini Pro Vision for summarization & tagging
	•	Structured output in JSON or Markdown for ML training, RAG pipelines, or LLM finetuning

Use Cases
	•	Creating high-quality datasets for training educational LLMs
	•	Preprocessing documents for retrieval-based tutoring systems
	•	Building RAG pipelines using real-world academic corpora
	•	Extracting and classifying visual/semantic structures in educational data

GitHub (Code & Examples)

Repo: https://github.com/ses4255/Versatile-OCR-Program

Would appreciate feedback, ideas, or even collaborators — especially if you’re working in document AI, education tech, or dataset curation.",2025-04-02 18:52:51,4,3,MachineLearning,post,"[Project] Open-source OCR system for creating educational ML datasets (math, multilingual, tables, diagrams)
Hi everyone,

I’ve open-sourced an OCR pipeline designed to extract structured, machine learning-ready data from complex educational documents. It’s built with a focus on academic content such as entrance exams, scientific PDFs, and textbooks — handling not just plain text but also math formulas, multilingual content, tables, and figures.

Core Capabilities
	•	Multilingual OCR (supports English, Korean, Japanese — easily extensible)
	•	Math recognition using MathPix API (LaTeX-style precision)
	•	Layout parsing with DocLayout-YOLO and OpenCV for detecting tables and diagrams
	•	Semantic postprocessing using GPT-4 / Gemini Pro Vision for summarization & tagging
	•	Structured output in JSON or Markdown for ML training, RAG pipelines, or LLM finetuning

Use Cases
	•	Creating high-quality datasets for training educational LLMs
	•	Preprocessing documents for retrieval-based tutoring systems
	•	Building RAG pipelines using real-world academic corpora
	•	Extracting and classifying visual/semantic structures in educational data

GitHub (Code & Examples)

Repo: https://github.com/ses4255/Versatile-OCR-Program

Would appreciate feedback, ideas, or even collaborators — especially if you’re working in document AI, education tech, or dataset curation."
1jp65e9,[R] NeuRaLaTeX: A machine learning library written in pure LaTeX,"Exicting times, SOTA wrt to Pytorch, TF and resent/transformer papers.",2025-04-01 22:36:34,152,6,MachineLearning,post,"[R] NeuRaLaTeX: A machine learning library written in pure LaTeX
Exicting times, SOTA wrt to Pytorch, TF and resent/transformer papers."
1joyu43,[R] The Future of Romance: Novel Techniques for Replacing your Boyfriend with Generative AI,I hope today is an okay day to post this here,2025-04-01 17:45:29,267,22,MachineLearning,post,"[R] The Future of Romance: Novel Techniques for Replacing your Boyfriend with Generative AI
I hope today is an okay day to post this here"
1jponcv,[P][R] Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus,"**Web Tool:** [https://citegeist.org/](https://citegeist.org/)

**Code** (for the local deployment)**:** [https://github.com/Geoff-Robin/CiteGeist](https://github.com/Geoff-Robin/CiteGeist)

**Paper:** [https://arxiv.org/pdf/2503.23229](https://arxiv.org/pdf/2503.23229)

**Abstract:**

>Large Language Models provide significant new opportunities for the generation of high-quality written works. However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles. In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs. For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering. To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers. To enable easy utilization in the scientific community, we release both, a website ([this https URL](https://citegeist.org/)), as well as an implementation harness that works with several different LLM implementations.

**Key features:**

>• Development of a dynamic retrieval and synthesis application for related work generation. 

>• Introduction of three key hyperparameters—breadth, depth, and diversity—to finetune the content and style of the result. 

>• Support for uploading full PDFs to enhance content-based retrieval. 

>• Employment of full paper texts through alternating between importance weighting and summarization techniques.

**Test:**

For some testing, I have chosen the paper [WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation](https://arxiv.org/pdf/2503.19065) \-- a kinda meta choice since it also explores automatic knowledge-based text generation. Its abstract was fed into the Citegeist web tool.

Tool output:

>\*\*Related Work\*\*  
  
Automated knowledge creation and collection have garnered increasing attention, particularly in the context of generating Wikipedia-style content. Several works have explored methods for automating the creation of comprehensive knowledge resources. For instance, Admati et al. (2018) introduced Wikibook-Bot, a system that automatically generates Wikibooks by organizing existing Wikipedia articles into a book format, using machine learning for article selection, chapter creation, and ordering \[Admati et al., 2018\]. Similarly, Li et al. (2021) tackled the challenge of generating up-to-date Wikipedia content for rapidly evolving fields, such as AI, by employing a two-stage approach involving extractive and abstractive summarization \[Li et al., 2021\]. Shao et al. (2024) focused on the pre-writing stage of article generation, introducing a system for synthesizing topic outlines through retrieval and multi-perspective question asking to improve the breadth and organization of generated articles \[Shao et al., 2024\]. Fan and Gardent (2022) addressed the challenges in generating factual, long-form text like Wikipedia articles by using a retrieval mechanism to gather relevant web evidence and a pre-trained encoder-decoder to generate biographies section by section with citations \[Fan and Gardent, 2022\]. While these approaches share the goal of automating content creation from existing knowledge sources, they primarily focus on text-only generation, whereas our work, WikiAutoGen, aims to generate new articles with both text and images, using a multi-perspective self-reflection mechanism to improve accuracy and coherence.  
  
A crucial aspect of generating high-quality Wikipedia content is ensuring factual accuracy and coherence. Chen et al. (2020) introduced WikiTableT, a dataset pairing Wikipedia sections with corresponding tabular data, highlighting challenges in coherence and factuality in data-to-text generation \[Chen et al., 2020\]. Our WikiAutoGen system addresses these issues through a multi-perspective self-reflection mechanism to improve the reliability and coherence of generated articles. Furthermore, Šakota et al. (2022) addressed the problem of missing short descriptions in Wikipedia articles, which hinders navigation and knowledge management, by automatically generating these descriptions using the Descartes model \[Šakota et al., 2022\]. While Descartes focuses on generating textual summaries, WikiAutoGen extends this by incorporating multimodal content, suggesting potential synergies in improving Wikipedia's accessibility and informativeness.  
  
The importance of multimodal content in enhancing informativeness and engagement has been recognized in recent research. Zhu et al. (2024) presented MuRAR, a framework for multimodal answer generation that enhances text answers with relevant images, tables, and videos \[Zhu et al., 2024\]. Their work, like WikiAutoGen, recognizes the limitations of text-only generation and aims to improve informativeness and user experience through multimodal content. Burns et al. (2023) introduced the WikiWeb2M dataset, a large-scale multimodal resource of Wikipedia webpages containing images, text, and structural information \[Burns et al., 2023\]. This dataset enables research on multimodal webpage understanding through tasks like page description generation, section summarization, and contextual image captioning. Another work by Burns et al. (2023) defines a suite of generative tasks for multi-level multimodal webpage understanding using the WikiWeb2M dataset \[Burns et al., 2023\]. These datasets and tasks are directly related to the goal of generating comprehensive Wikipedia-style articles, making them useful benchmarks for comparison.  
  
The evaluation of multimodal generation systems requires high-quality datasets and evaluation metrics. Wu et al. (2024) addressed the challenge of evaluating multimodal retrieval augmented generation (MMRAG) systems by proposing a synthetic data generation framework \[Wu et al., 2024\]. Their method of generating question-answer pairs from multimodal documents, with control over question styles and modalities, complements our focus on generating visually enriched Wikipedia-style articles.  
  
In contrast to existing approaches, our work introduces WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation that retrieves and integrates relevant images alongside text. To facilitate the evaluation of multimodal knowledge generation on more challenging topics, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations. This benchmark allows for a more comprehensive evaluation of systems like WikiAutoGen, which aim to generate more accurate, coherent, and visually enriched Wikipedia-style articles.

>References

>Shahar Admati, Lior Rokach, Bracha Shapira (2018). Wikibook-Bot - Automatic Generation of a Wikipedia Book. arXiv:1812.10937. [https://arxiv.org/abs/1812.10937](https://arxiv.org/abs/1812.10937)  
  
Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig (2024). Synthetic Multimodal Question Generation. arXiv:2407.02233. [https://arxiv.org/abs/2407.02233](https://arxiv.org/abs/2407.02233)  
  
Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, Yunyao Li (2024). MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering. arXiv:2408.08521. [https://arxiv.org/abs/2408.08521](https://arxiv.org/abs/2408.08521)  
  
Angela Fan, Claire Gardent (2022). Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies. arXiv:2204.05879. [https://arxiv.org/abs/2204.05879](https://arxiv.org/abs/2204.05879)  
  
Mingda Chen, Sam Wiseman, Kevin Gimpel (2020). WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections. arXiv:2012.14919. [https://arxiv.org/abs/2012.14919](https://arxiv.org/abs/2012.14919)  
  
Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo (2023). WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset. arXiv:2305.05432. [https://arxiv.org/abs/2305.05432](https://arxiv.org/abs/2305.05432)  
  
Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam (2024). Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. arXiv:2402.14207. [https://arxiv.org/abs/2402.14207](https://arxiv.org/abs/2402.14207)  
  
Irene Li, Alexander Fabbri, Rina Kawamura, Yixin Liu, Xiangru Tang, Jaesung Tae, Chang Shen, Sally Ma, Tomoe Mizutani, Dragomir Radev (2021). Surfer100: Generating Surveys From Web Resources, Wikipedia-style. arXiv:2112.06377. [https://arxiv.org/abs/2112.06377](https://arxiv.org/abs/2112.06377)  
  
Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo (2023). A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. arXiv:2305.03668. [https://arxiv.org/abs/2305.03668](https://arxiv.org/abs/2305.03668)

Overall, 3 out of 9 references suggested by Citegeist were actually present in the tested paper. And most of the rest weren't too far off. I think it's decent enough.",2025-04-02 15:33:03,4,1,MachineLearning,post,"[P][R] Citegeist: Automated Generation of Related Work Analysis on the arXiv Corpus
**Web Tool:** [https://citegeist.org/](https://citegeist.org/)

**Code** (for the local deployment)**:** [https://github.com/Geoff-Robin/CiteGeist](https://github.com/Geoff-Robin/CiteGeist)

**Paper:** [https://arxiv.org/pdf/2503.23229](https://arxiv.org/pdf/2503.23229)

**Abstract:**

>Large Language Models provide significant new opportunities for the generation of high-quality written works. However, their employment in the research community is inhibited by their tendency to hallucinate invalid sources and lack of direct access to a knowledge base of relevant scientific articles. In this work, we present Citegeist: An application pipeline using dynamic Retrieval Augmented Generation (RAG) on the arXiv Corpus to generate a related work section and other citation-backed outputs. For this purpose, we employ a mixture of embedding-based similarity matching, summarization, and multi-stage filtering. To adapt to the continuous growth of the document base, we also present an optimized way of incorporating new and modified papers. To enable easy utilization in the scientific community, we release both, a website ([this https URL](https://citegeist.org/)), as well as an implementation harness that works with several different LLM implementations.

**Key features:**

>• Development of a dynamic retrieval and synthesis application for related work generation. 

>• Introduction of three key hyperparameters—breadth, depth, and diversity—to finetune the content and style of the result. 

>• Support for uploading full PDFs to enhance content-based retrieval. 

>• Employment of full paper texts through alternating between importance weighting and summarization techniques.

**Test:**

For some testing, I have chosen the paper [WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation](https://arxiv.org/pdf/2503.19065) \-- a kinda meta choice since it also explores automatic knowledge-based text generation. Its abstract was fed into the Citegeist web tool.

Tool output:

>\*\*Related Work\*\*  
  
Automated knowledge creation and collection have garnered increasing attention, particularly in the context of generating Wikipedia-style content. Several works have explored methods for automating the creation of comprehensive knowledge resources. For instance, Admati et al. (2018) introduced Wikibook-Bot, a system that automatically generates Wikibooks by organizing existing Wikipedia articles into a book format, using machine learning for article selection, chapter creation, and ordering \[Admati et al., 2018\]. Similarly, Li et al. (2021) tackled the challenge of generating up-to-date Wikipedia content for rapidly evolving fields, such as AI, by employing a two-stage approach involving extractive and abstractive summarization \[Li et al., 2021\]. Shao et al. (2024) focused on the pre-writing stage of article generation, introducing a system for synthesizing topic outlines through retrieval and multi-perspective question asking to improve the breadth and organization of generated articles \[Shao et al., 2024\]. Fan and Gardent (2022) addressed the challenges in generating factual, long-form text like Wikipedia articles by using a retrieval mechanism to gather relevant web evidence and a pre-trained encoder-decoder to generate biographies section by section with citations \[Fan and Gardent, 2022\]. While these approaches share the goal of automating content creation from existing knowledge sources, they primarily focus on text-only generation, whereas our work, WikiAutoGen, aims to generate new articles with both text and images, using a multi-perspective self-reflection mechanism to improve accuracy and coherence.  
  
A crucial aspect of generating high-quality Wikipedia content is ensuring factual accuracy and coherence. Chen et al. (2020) introduced WikiTableT, a dataset pairing Wikipedia sections with corresponding tabular data, highlighting challenges in coherence and factuality in data-to-text generation \[Chen et al., 2020\]. Our WikiAutoGen system addresses these issues through a multi-perspective self-reflection mechanism to improve the reliability and coherence of generated articles. Furthermore, Šakota et al. (2022) addressed the problem of missing short descriptions in Wikipedia articles, which hinders navigation and knowledge management, by automatically generating these descriptions using the Descartes model \[Šakota et al., 2022\]. While Descartes focuses on generating textual summaries, WikiAutoGen extends this by incorporating multimodal content, suggesting potential synergies in improving Wikipedia's accessibility and informativeness.  
  
The importance of multimodal content in enhancing informativeness and engagement has been recognized in recent research. Zhu et al. (2024) presented MuRAR, a framework for multimodal answer generation that enhances text answers with relevant images, tables, and videos \[Zhu et al., 2024\]. Their work, like WikiAutoGen, recognizes the limitations of text-only generation and aims to improve informativeness and user experience through multimodal content. Burns et al. (2023) introduced the WikiWeb2M dataset, a large-scale multimodal resource of Wikipedia webpages containing images, text, and structural information \[Burns et al., 2023\]. This dataset enables research on multimodal webpage understanding through tasks like page description generation, section summarization, and contextual image captioning. Another work by Burns et al. (2023) defines a suite of generative tasks for multi-level multimodal webpage understanding using the WikiWeb2M dataset \[Burns et al., 2023\]. These datasets and tasks are directly related to the goal of generating comprehensive Wikipedia-style articles, making them useful benchmarks for comparison.  
  
The evaluation of multimodal generation systems requires high-quality datasets and evaluation metrics. Wu et al. (2024) addressed the challenge of evaluating multimodal retrieval augmented generation (MMRAG) systems by proposing a synthetic data generation framework \[Wu et al., 2024\]. Their method of generating question-answer pairs from multimodal documents, with control over question styles and modalities, complements our focus on generating visually enriched Wikipedia-style articles.  
  
In contrast to existing approaches, our work introduces WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation that retrieves and integrates relevant images alongside text. To facilitate the evaluation of multimodal knowledge generation on more challenging topics, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations. This benchmark allows for a more comprehensive evaluation of systems like WikiAutoGen, which aim to generate more accurate, coherent, and visually enriched Wikipedia-style articles.

>References

>Shahar Admati, Lior Rokach, Bracha Shapira (2018). Wikibook-Bot - Automatic Generation of a Wikipedia Book. arXiv:1812.10937. [https://arxiv.org/abs/1812.10937](https://arxiv.org/abs/1812.10937)  
  
Ian Wu, Sravan Jayanthi, Vijay Viswanathan, Simon Rosenberg, Sina Pakazad, Tongshuang Wu, Graham Neubig (2024). Synthetic Multimodal Question Generation. arXiv:2407.02233. [https://arxiv.org/abs/2407.02233](https://arxiv.org/abs/2407.02233)  
  
Zhengyuan Zhu, Daniel Lee, Hong Zhang, Sai Sree Harsha, Loic Feujio, Akash Maharaj, Yunyao Li (2024). MuRAR: A Simple and Effective Multimodal Retrieval and Answer Refinement Framework for Multimodal Question Answering. arXiv:2408.08521. [https://arxiv.org/abs/2408.08521](https://arxiv.org/abs/2408.08521)  
  
Angela Fan, Claire Gardent (2022). Generating Full Length Wikipedia Biographies: The Impact of Gender Bias on the Retrieval-Based Generation of Women Biographies. arXiv:2204.05879. [https://arxiv.org/abs/2204.05879](https://arxiv.org/abs/2204.05879)  
  
Mingda Chen, Sam Wiseman, Kevin Gimpel (2020). WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections. arXiv:2012.14919. [https://arxiv.org/abs/2012.14919](https://arxiv.org/abs/2012.14919)  
  
Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo (2023). WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset. arXiv:2305.05432. [https://arxiv.org/abs/2305.05432](https://arxiv.org/abs/2305.05432)  
  
Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, Monica S. Lam (2024). Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. arXiv:2402.14207. [https://arxiv.org/abs/2402.14207](https://arxiv.org/abs/2402.14207)  
  
Irene Li, Alexander Fabbri, Rina Kawamura, Yixin Liu, Xiangru Tang, Jaesung Tae, Chang Shen, Sally Ma, Tomoe Mizutani, Dragomir Radev (2021). Surfer100: Generating Surveys From Web Resources, Wikipedia-style. arXiv:2112.06377. [https://arxiv.org/abs/2112.06377](https://arxiv.org/abs/2112.06377)  
  
Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, Mandy Guo (2023). A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding. arXiv:2305.03668. [https://arxiv.org/abs/2305.03668](https://arxiv.org/abs/2305.03668)

Overall, 3 out of 9 references suggested by Citegeist were actually present in the tested paper. And most of the rest weren't too far off. I think it's decent enough."
1jpbgzn,[D] What are the current challenges in deepfake detection (image)?,"Hey guys, I need some help figuring out the research gap in my deepfake detection literature review.

I’ve already written about the challenges of dataset generalization and cited papers that address this issue. I also compared different detection methods for images vs. videos. But I realized I never actually identified a clear research gap—like, what specific problem still needs solving?

Deepfake detection is super common, and I feel like I’ve covered most of the major issues. Now, I’m stuck because I don’t know what problem to focus on.

For those familiar with the field, what do you think are the biggest current challenges in deepfake detection (especially for images)? Any insights would be really helpful!",2025-04-02 02:28:45,11,2,MachineLearning,post,"[D] What are the current challenges in deepfake detection (image)?
Hey guys, I need some help figuring out the research gap in my deepfake detection literature review.

I’ve already written about the challenges of dataset generalization and cited papers that address this issue. I also compared different detection methods for images vs. videos. But I realized I never actually identified a clear research gap—like, what specific problem still needs solving?

Deepfake detection is super common, and I feel like I’ve covered most of the major issues. Now, I’m stuck because I don’t know what problem to focus on.

For those familiar with the field, what do you think are the biggest current challenges in deepfake detection (especially for images)? Any insights would be really helpful!"
1jose8v,[R] Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad,"Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad  
Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev - ETH Zurich, INSAIT, Sofia University ""St. Kliment Ohridski""  
*Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, o3-mini, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly, achieving less than 5% on average. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.*  
arXiv:2503.21934 \[cs.CL\]: [https://arxiv.org/abs/2503.21934v1](https://arxiv.org/abs/2503.21934v1)

https://preview.redd.it/nyjvsp0lb7se1.jpg?width=1291&format=pjpg&auto=webp&s=96500fd5c539a3bca4ee96a8ae2fc39b6757e476

",2025-04-01 12:34:02,110,22,MachineLearning,post,"[R] Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad
Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad  
Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev - ETH Zurich, INSAIT, Sofia University ""St. Kliment Ohridski""  
*Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, o3-mini, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly, achieving less than 5% on average. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.*  
arXiv:2503.21934 \[cs.CL\]: [https://arxiv.org/abs/2503.21934v1](https://arxiv.org/abs/2503.21934v1)

https://preview.redd.it/nyjvsp0lb7se1.jpg?width=1291&format=pjpg&auto=webp&s=96500fd5c539a3bca4ee96a8ae2fc39b6757e476

"
1jpimql,[P] Handling Missing Values in Dataset,"I'm using [this](https://data.cms.gov/provider-summary-by-type-of-service/medicare-physician-other-practitioners/medicare-physician-other-practitioners-by-provider) dataset for a regression project, and the goal is to predict the beneficiary risk score(Bene\_Avg\_Risk\_Scre). Now, to protect beneficiary identities and safeguard this information, CMS has redacted all data elements from this file where the data element represents fewer than 11 beneficiaries. Due to this, there are plenty of features with lots of missing values as shown below in the image.

Basically, if the data element is represented by lesser than 11 beneficiaries, they've redacted that cell. So all non-null entries in that column are >= 11, and all missing values supposedly had < 11 before redaction(This is my understanding so far). One imputation technique I could think of was assuming a discrete uniform distribution for the variables, ranging from 1 to 10 and imputing with the mean of said distribution(5 or 6). But obviously this is not a good idea because I do not take into account any skewness / the fact that the data might have been biased to either smaller/larger numbers. How do I impute these columns in such a case? I do not want to drop these columns. Any help will be appreciated, TIA!

[Features with Missing Values](https://preview.redd.it/taw7pfddfdse1.png?width=666&format=png&auto=webp&s=93a6b7423e1beb49df65b2a12740b8dfe73db4f5)",2025-04-02 09:06:54,2,0,MachineLearning,post,"[P] Handling Missing Values in Dataset
I'm using [this](https://data.cms.gov/provider-summary-by-type-of-service/medicare-physician-other-practitioners/medicare-physician-other-practitioners-by-provider) dataset for a regression project, and the goal is to predict the beneficiary risk score(Bene\_Avg\_Risk\_Scre). Now, to protect beneficiary identities and safeguard this information, CMS has redacted all data elements from this file where the data element represents fewer than 11 beneficiaries. Due to this, there are plenty of features with lots of missing values as shown below in the image.

Basically, if the data element is represented by lesser than 11 beneficiaries, they've redacted that cell. So all non-null entries in that column are >= 11, and all missing values supposedly had < 11 before redaction(This is my understanding so far). One imputation technique I could think of was assuming a discrete uniform distribution for the variables, ranging from 1 to 10 and imputing with the mean of said distribution(5 or 6). But obviously this is not a good idea because I do not take into account any skewness / the fact that the data might have been biased to either smaller/larger numbers. How do I impute these columns in such a case? I do not want to drop these columns. Any help will be appreciated, TIA!

[Features with Missing Values](https://preview.redd.it/taw7pfddfdse1.png?width=666&format=png&auto=webp&s=93a6b7423e1beb49df65b2a12740b8dfe73db4f5)"
1jpe138,[R] Is iterative re-training in semi-supervised segmentation a good idea?,"I’m working on a **medical image segmentation** project and would love to hear your thoughts on a couple of decisions I’m facing.

To give some context: I started with a small set of labeled CT images and a large set of unlabeled ones. I used a **semi-supervised segmentation model** to generate pseudo-labels for the unlabeled data. But instead of doing it in a single pass, I took an **iterative approach** — after each cycle, I manually refined a few of the auto-generated segmentations, retrained the model, and repeated this process several times. Over multiple iterations, the quality of the segmentations improved significantly.

**First question:**  
Is this kind of **iterative re-training in semi-supervised learning (SSL)** actually considered a good idea? Or is there a risk of overfitting / model drift / confirmation bias because I keep training on my own model's pseudo-labels?

**Second question:**  
Now that I have a decent, refined labeled dataset from this iterative process, should I:

1. **Keep using the semi-supervised model** (the one trained over several iterations) for segmenting new, unseen images?
2. **Train a fully supervised segmentation model** using the final refined labels and use that for inference?

I’ve read mixed opinions on whether SSL models generalize well enough to be used directly vs. whether it’s better to retrain a clean supervised model once you’ve built a solid labeled dataset.

If anyone has experience with this type of workflow in segmentation tasks — or knows of any relevant papers discussing this trade-off — I’d love to hear your thoughts!

**PS: I can technically test both options and compare them — but to do that properly, I’d need to manually label at least 20 more images to get statistically meaningful results, which is quite time-consuming. So I’d really appreciate any advice before going down that path.**",2025-04-02 04:30:11,3,7,MachineLearning,post,"[R] Is iterative re-training in semi-supervised segmentation a good idea?
I’m working on a **medical image segmentation** project and would love to hear your thoughts on a couple of decisions I’m facing.

To give some context: I started with a small set of labeled CT images and a large set of unlabeled ones. I used a **semi-supervised segmentation model** to generate pseudo-labels for the unlabeled data. But instead of doing it in a single pass, I took an **iterative approach** — after each cycle, I manually refined a few of the auto-generated segmentations, retrained the model, and repeated this process several times. Over multiple iterations, the quality of the segmentations improved significantly.

**First question:**  
Is this kind of **iterative re-training in semi-supervised learning (SSL)** actually considered a good idea? Or is there a risk of overfitting / model drift / confirmation bias because I keep training on my own model's pseudo-labels?

**Second question:**  
Now that I have a decent, refined labeled dataset from this iterative process, should I:

1. **Keep using the semi-supervised model** (the one trained over several iterations) for segmenting new, unseen images?
2. **Train a fully supervised segmentation model** using the final refined labels and use that for inference?

I’ve read mixed opinions on whether SSL models generalize well enough to be used directly vs. whether it’s better to retrain a clean supervised model once you’ve built a solid labeled dataset.

If anyone has experience with this type of workflow in segmentation tasks — or knows of any relevant papers discussing this trade-off — I’d love to hear your thoughts!

**PS: I can technically test both options and compare them — but to do that properly, I’d need to manually label at least 20 more images to get statistically meaningful results, which is quite time-consuming. So I’d really appreciate any advice before going down that path.**"
1jot2zr,[D][P] Turning Knowledge Graphs into Memory with Ontologies?,"Most AI models rely on external data that is either in a knowledge graph, vector store or a combination of both - but they mostly regurgitate the already available datasets — but memory doesn’t work that way. The brain uses symbolic models to power the mental architecture that governs how we think, reason, and behave

We've added ontologies to cognee, our AI memory tool, which uses RDF + OWL to match external system rules to LLM generated Graphs in order to ground them.

Our assumption is that we will need dozens of small, validated ontologies to ground the memory systems, across different models. 

We might have ontologies for modelling timegraphs or complex rulesets for hypergraphs.

And in the end you get to see and explore a nice looking graph.

Here is a short [tutorial ](https://www.youtube.com/watch?v=BHs6FU0Evp0&t=101s)to set up ontologies with cognee:

Here is our [repository](https://github.com/topoteretes/cognee)

  
Would love to get your feedback on our approach

",2025-04-01 13:18:05,37,20,MachineLearning,post,"[D][P] Turning Knowledge Graphs into Memory with Ontologies?
Most AI models rely on external data that is either in a knowledge graph, vector store or a combination of both - but they mostly regurgitate the already available datasets — but memory doesn’t work that way. The brain uses symbolic models to power the mental architecture that governs how we think, reason, and behave

We've added ontologies to cognee, our AI memory tool, which uses RDF + OWL to match external system rules to LLM generated Graphs in order to ground them.

Our assumption is that we will need dozens of small, validated ontologies to ground the memory systems, across different models. 

We might have ontologies for modelling timegraphs or complex rulesets for hypergraphs.

And in the end you get to see and explore a nice looking graph.

Here is a short [tutorial ](https://www.youtube.com/watch?v=BHs6FU0Evp0&t=101s)to set up ontologies with cognee:

Here is our [repository](https://github.com/topoteretes/cognee)

  
Would love to get your feedback on our approach

"
1jp21ld,[D] What are the hardest LLM tasks to evaluate in your experience?,"I am trying to figure out which LLM tasks are the hardest to evaluate; especially ones where public benchmarks don’t help much.

Any niche use cases come to mind?  
(e.g. NER for clinical notes, QA over financial news, etc.)

Would love to hear what you have struggled with.",2025-04-01 19:54:07,3,23,MachineLearning,post,"[D] What are the hardest LLM tasks to evaluate in your experience?
I am trying to figure out which LLM tasks are the hardest to evaluate; especially ones where public benchmarks don’t help much.

Any niche use cases come to mind?  
(e.g. NER for clinical notes, QA over financial news, etc.)

Would love to hear what you have struggled with."
1jpd67i,[D] How do you see the research/academic climate given the current state of the world?,"Suppose the current climate in the US, and the current world view of the US, continues to stagnate/degrade. How do you think this will impact the larger scientific community? Whether it be research producers, grant funding, conference venues, poaching of talent, etc.",2025-04-02 03:52:54,0,9,MachineLearning,post,"[D] How do you see the research/academic climate given the current state of the world?
Suppose the current climate in the US, and the current world view of the US, continues to stagnate/degrade. How do you think this will impact the larger scientific community? Whether it be research producers, grant funding, conference venues, poaching of talent, etc."
1jozejh,[D] Dubious Validation Accuracy on Different Dataset Splits,"Hi all, I have been working on a hydrological forecasting model for some time now, with the goal of making the model robust enough to inform operations at my company, specifically for several years into the future. 

For this reason, most of my time spent designing and training the model, I have been using a time-based split of the data to simulate the potential of the model being used for a long time. This training process often saw overfitting at around 6 epochs; the best model producing a MAE of 0.06. 

However, I am now being asked to train the final production model, and I want to use all of the available data. For this, I use a standard random 80-20 split including the years I previously held out. Importantly, this model is training on less data than the prior models. But now, the model seems to be capable of much lower error, around 0.04 in most cases. It also has never overfit with the same hyperparameters I used for the previous models.

I'm concerned that this production model isn't actually capable of making robust predictions for future conditions, and the random split is actually allowing it to memorize the current river morphology conditions, rather than generally understand the flow and the potential of other conditions.

How could I analyze the potential of this model on conditions that we haven't seen? Should I return to the old approach of using the time-based split? Should I try a k-fold cross-validation with time splits?

Any help is appreciated.

Two notes: I am on another team analyzing the long term flow of the river, and there is a long term trend that we can observe, but we are not sure of the actual shape of the curve given the next 10+ years. (Hydrology is evil). And, because of this, I tried at one point using a positional encoding (rotary) that corresponded to the day of the current observation since the first observation in the dataset (Jan 1 2008 = 0; Jan 1 2009 = 365). This was in hopes of the model discovering the trend itself. I attempted using this in both the encoder and decoder, with no success. ",2025-04-01 18:07:52,2,1,MachineLearning,post,"[D] Dubious Validation Accuracy on Different Dataset Splits
Hi all, I have been working on a hydrological forecasting model for some time now, with the goal of making the model robust enough to inform operations at my company, specifically for several years into the future. 

For this reason, most of my time spent designing and training the model, I have been using a time-based split of the data to simulate the potential of the model being used for a long time. This training process often saw overfitting at around 6 epochs; the best model producing a MAE of 0.06. 

However, I am now being asked to train the final production model, and I want to use all of the available data. For this, I use a standard random 80-20 split including the years I previously held out. Importantly, this model is training on less data than the prior models. But now, the model seems to be capable of much lower error, around 0.04 in most cases. It also has never overfit with the same hyperparameters I used for the previous models.

I'm concerned that this production model isn't actually capable of making robust predictions for future conditions, and the random split is actually allowing it to memorize the current river morphology conditions, rather than generally understand the flow and the potential of other conditions.

How could I analyze the potential of this model on conditions that we haven't seen? Should I return to the old approach of using the time-based split? Should I try a k-fold cross-validation with time splits?

Any help is appreciated.

Two notes: I am on another team analyzing the long term flow of the river, and there is a long term trend that we can observe, but we are not sure of the actual shape of the curve given the next 10+ years. (Hydrology is evil). And, because of this, I tried at one point using a positional encoding (rotary) that corresponded to the day of the current observation since the first observation in the dataset (Jan 1 2008 = 0; Jan 1 2009 = 365). This was in hopes of the model discovering the trend itself. I attempted using this in both the encoder and decoder, with no success. "
1jof0f2,[Project] Tensara: Codeforces/Kaggle for GPU programming,"A few friends and I recently built [tensara.org](https://tensara.org/) – a competitive GPU kernel optimization platform where you can submit and benchmark kernels (in FLOPS) for common deep learning workloads (GEMM, Conv2D, etc) in CUDA/Triton.

We [launched \~1 month ago](https://x.com/msarthak29/status/1894859767807455695), and we've gotten 6k+ submissions on our platform since. We just released a [bunch of updates](https://x.com/tensarahq/status/1906591084093874433) that we wanted to share:

* Triton support is live!
* 30+ problems waiting to be solved
* Profile pages to show off your submission activity
* Ratings that track skill/activity
* Rankings to fully embrace the competitive spirit
* A [CLI tool](https://github.com/tensara/tensara-cli) in Rust to submit solutions

We're fully [open-source too](https://github.com/tensara/tensara), try it out and let us know what you think!",2025-03-31 23:41:12,55,9,MachineLearning,post,"[Project] Tensara: Codeforces/Kaggle for GPU programming
A few friends and I recently built [tensara.org](https://tensara.org/) – a competitive GPU kernel optimization platform where you can submit and benchmark kernels (in FLOPS) for common deep learning workloads (GEMM, Conv2D, etc) in CUDA/Triton.

We [launched \~1 month ago](https://x.com/msarthak29/status/1894859767807455695), and we've gotten 6k+ submissions on our platform since. We just released a [bunch of updates](https://x.com/tensarahq/status/1906591084093874433) that we wanted to share:

* Triton support is live!
* 30+ problems waiting to be solved
* Profile pages to show off your submission activity
* Ratings that track skill/activity
* Rankings to fully embrace the competitive spirit
* A [CLI tool](https://github.com/tensara/tensara-cli) in Rust to submit solutions

We're fully [open-source too](https://github.com/tensara/tensara), try it out and let us know what you think!"
1jowuwz,[R] Query Generation with Execution-Guided Selection for Improved Text-to-SQL Accuracy,"I was intrigued by this execution-guided approach to SQL generation that uses database query results to improve accuracy. The key insight is simple but powerful: by executing candidate SQL queries against the actual database and analyzing the results, models can learn from their mistakes and generate better SQL.

The method works in two ways:
* **During training**: Models are shown not just SQL queries but also their execution results
* **During inference**: Multiple candidate queries are generated, executed, and the best one is selected using minimum Bayes risk (MBR) decoding
* **Utility functions** determine the ""best"" query based on execution success, row counts, and result similarity
* **Performance gains** are substantial: 10.6% improvement for GPT-3.5 and 5.4% for GPT-4 on the Spider benchmark
* Works with both **closed-source LLMs** (GPT models) and **open-source models** (CodeLlama)
* Requires **no architectural changes** to existing models

I think this approach could become standard practice for SQL generation systems. The ability to incorporate execution feedback addresses a fundamental limitation in current text-to-SQL systems that rely solely on textual prompts. This could make natural language database interfaces much more reliable in practical applications.

I think the computational overhead is a real concern, though. Executing multiple queries introduces latency that might be problematic for real-time applications. The privacy implications also need careful consideration - you don't want incorrect queries accidentally returning sensitive data.

TLDR: By executing candidate SQL queries and using their results as feedback, this approach improves SQL generation accuracy by 5-10% across different models. It's a practical enhancement that could make natural language database interfaces significantly more reliable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/query-conquer-execution-guided-sql-generation). Paper [here](https://arxiv.org/abs/2503.24364).",2025-04-01 16:24:28,2,1,MachineLearning,post,"[R] Query Generation with Execution-Guided Selection for Improved Text-to-SQL Accuracy
I was intrigued by this execution-guided approach to SQL generation that uses database query results to improve accuracy. The key insight is simple but powerful: by executing candidate SQL queries against the actual database and analyzing the results, models can learn from their mistakes and generate better SQL.

The method works in two ways:
* **During training**: Models are shown not just SQL queries but also their execution results
* **During inference**: Multiple candidate queries are generated, executed, and the best one is selected using minimum Bayes risk (MBR) decoding
* **Utility functions** determine the ""best"" query based on execution success, row counts, and result similarity
* **Performance gains** are substantial: 10.6% improvement for GPT-3.5 and 5.4% for GPT-4 on the Spider benchmark
* Works with both **closed-source LLMs** (GPT models) and **open-source models** (CodeLlama)
* Requires **no architectural changes** to existing models

I think this approach could become standard practice for SQL generation systems. The ability to incorporate execution feedback addresses a fundamental limitation in current text-to-SQL systems that rely solely on textual prompts. This could make natural language database interfaces much more reliable in practical applications.

I think the computational overhead is a real concern, though. Executing multiple queries introduces latency that might be problematic for real-time applications. The privacy implications also need careful consideration - you don't want incorrect queries accidentally returning sensitive data.

TLDR: By executing candidate SQL queries and using their results as feedback, this approach improves SQL generation accuracy by 5-10% across different models. It's a practical enhancement that could make natural language database interfaces significantly more reliable.

[Full summary is here](https://aimodels.fyi/papers/arxiv/query-conquer-execution-guided-sql-generation). Paper [here](https://arxiv.org/abs/2503.24364)."
1jomxcm,[Project] AxiomGPT – programming with LLMs by defining Oracles in natural language,"Hello there,

I’ve been working on something called AxiomGPT, for a while, which is a model of latent-space programming that treats language not just as instruction, but as invocation.

Instead of writing traditional functions, you define Oracles using natural language..
 tiny semantic contracts like:

(defn fibber (Oracle ""Return the nth Fibonacci number""))

(fibber 123)
; => 22698374052006863956975682

Oracles can be procedural, persona-based, conceptual, or abstract.

They’re not executed, but remembered, manifested and reconstructed by the model through learned latent behavior.

Highlights:

You can define entities like (defn clarke ...) or (defn tspsolver ...)

Oracles can be composed, piped, even treated like lambda functions.

Ughhh, and no, you don't have to program them in LISP, but it helps!

They work with real algorithms, recursive calls, map/reduce, and code in any language

Entire functions and their behaviors can live inside a single token

It's programmable in English, by design

We’ve written up a full Codex, with theory, usage, quotes, even philosophical parallels to quantum computing.

If you are into AI cognition, symbolic programming, or latent computing, it’s well worth checking out and weird ride.

Easy to try it yourself in minutes for fun and profit!

Explore it here: [https://x.com/chrisbe1968/status/1906875616290365941]

Very happy to answer any questions and hear your thoughts!
",2025-04-01 06:09:43,11,5,MachineLearning,post,"[Project] AxiomGPT – programming with LLMs by defining Oracles in natural language
Hello there,

I’ve been working on something called AxiomGPT, for a while, which is a model of latent-space programming that treats language not just as instruction, but as invocation.

Instead of writing traditional functions, you define Oracles using natural language..
 tiny semantic contracts like:

(defn fibber (Oracle ""Return the nth Fibonacci number""))

(fibber 123)
; => 22698374052006863956975682

Oracles can be procedural, persona-based, conceptual, or abstract.

They’re not executed, but remembered, manifested and reconstructed by the model through learned latent behavior.

Highlights:

You can define entities like (defn clarke ...) or (defn tspsolver ...)

Oracles can be composed, piped, even treated like lambda functions.

Ughhh, and no, you don't have to program them in LISP, but it helps!

They work with real algorithms, recursive calls, map/reduce, and code in any language

Entire functions and their behaviors can live inside a single token

It's programmable in English, by design

We’ve written up a full Codex, with theory, usage, quotes, even philosophical parallels to quantum computing.

If you are into AI cognition, symbolic programming, or latent computing, it’s well worth checking out and weird ride.

Easy to try it yourself in minutes for fun and profit!

Explore it here: [https://x.com/chrisbe1968/status/1906875616290365941]

Very happy to answer any questions and hear your thoughts!
"
1joxq8q,[D] Simple Questions Thread,"Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!",2025-04-01 17:00:42,1,1,MachineLearning,post,"[D] Simple Questions Thread
Please post your questions here instead of creating a new thread. Encourage others who create new posts for questions to post here instead!

Thread will stay alive until next one so keep posting after the date in the title.

Thanks to everyone for answering questions in the previous thread!"
1jopjw2,[D] Any open source library similar to this?,"[https://aws.amazon.com/blogs/machine-learning/snapper-provides-machine-learning-assisted-labeling-for-pixel-perfect-image-object-detection/](https://aws.amazon.com/blogs/machine-learning/snapper-provides-machine-learning-assisted-labeling-for-pixel-perfect-image-object-detection/)

This will make it easier for annotating a dataset which is niche.",2025-04-01 09:05:28,4,2,MachineLearning,post,"[D] Any open source library similar to this?
[https://aws.amazon.com/blogs/machine-learning/snapper-provides-machine-learning-assisted-labeling-for-pixel-perfect-image-object-detection/](https://aws.amazon.com/blogs/machine-learning/snapper-provides-machine-learning-assisted-labeling-for-pixel-perfect-image-object-detection/)

This will make it easier for annotating a dataset which is niche."
1jo8hyp,[R] Latent Verification for ~10% Absolute Factual Accuracy Improvement,"Let me preface by saying I'm a little nervous / embarrass posting this here. I'm just some self-taught dude that's been dabbling in ML since 2016. My implementation is probably incredibly crude and amateur, but I found it really rewarding regardless.

The TransMLA paper blew my mind when it came out.

Since then I've been playing around with manipulating pre-trained LLMs. I'm nowhere near as smart as the people behind transMLA or probably any of you, but I hope you still find this interesting.

here's the repo to the implementation for my architectural modification. It adds self-verification capabilities to LLMs (currently implemented in Qwen2.5 7B: [https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent\_Verification](https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent_Verification)).

It works by adding verification adapters (lightweight modules) every few layers.

These modules analyze the hidden states passing through its layer, computes a confidence score indicating how reliable the states are, applies weighted correction based on the inverse of that confidence score, and returns the corrected state back to the model's processing flow.

Then the cross-layer verifier compares representation across different layers to ensure consistency in the model's internal reasoning.

It's pretty cool. You can actually see the verification happening in the PCA projection within the \`results\` directory.

Anyway, hope y'all enjoy this. Looking forward to any feedback or ideas for improvement!

Repo: [https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs](https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs)",2025-03-31 19:16:08,26,9,MachineLearning,post,"[R] Latent Verification for ~10% Absolute Factual Accuracy Improvement
Let me preface by saying I'm a little nervous / embarrass posting this here. I'm just some self-taught dude that's been dabbling in ML since 2016. My implementation is probably incredibly crude and amateur, but I found it really rewarding regardless.

The TransMLA paper blew my mind when it came out.

Since then I've been playing around with manipulating pre-trained LLMs. I'm nowhere near as smart as the people behind transMLA or probably any of you, but I hope you still find this interesting.

here's the repo to the implementation for my architectural modification. It adds self-verification capabilities to LLMs (currently implemented in Qwen2.5 7B: [https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent\_Verification](https://huggingface.co/jacobpwarren/Qwen2.5-7B-Latent_Verification)).

It works by adding verification adapters (lightweight modules) every few layers.

These modules analyze the hidden states passing through its layer, computes a confidence score indicating how reliable the states are, applies weighted correction based on the inverse of that confidence score, and returns the corrected state back to the model's processing flow.

Then the cross-layer verifier compares representation across different layers to ensure consistency in the model's internal reasoning.

It's pretty cool. You can actually see the verification happening in the PCA projection within the \`results\` directory.

Anyway, hope y'all enjoy this. Looking forward to any feedback or ideas for improvement!

Repo: [https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs](https://github.com/jacobwarren/Latent-Space-Verification-for-Self-Correcting-LLMs)"
1jo48j9,[P] Developing a open-source (Retrieval Augmented Generation) framework written in C++ with python bindings for high performance,"Been exploring ways to optimize *Retrieval-Augmented Generation* (RAG) lately, and it’s clear that there’s always more ground to cover when it comes to balancing performance, speed, and resource efficiency in dynamic environments.

So, we decided to build an open-source framework designed to push those boundaries,  handling retrieval tasks faster, scaling efficiently, and integrating with key tools in the ecosystem.

We’re still in early development, but initial benchmarks are already showing some promising results. In certain cases, it’s matching or even surpassing well-known solutions like LangChain and LlamaIndex in performance.

[Comparisson for CPU usage over time](https://preview.redd.it/zsdeodwl91se1.jpg?width=2038&format=pjpg&auto=webp&s=25b2531bedbbee72646bb876d2e40516df982a1f)

[Comparisson for PDF extration and chunking](https://preview.redd.it/8urmicwl91se1.jpg?width=2038&format=pjpg&auto=webp&s=5b3f67249cee86a01c44547e39dcfdd777306f5b)

It integrates smoothly with tools like **TensorRT**, **FAISS**, **vLLM** and others. And our roadmap is packed with further optimizations, tools integrations and updates we’re excited to roll out.

If that sounds like something you’d like to explore, check out the GitHub repo:[ **https://github.com/pureai-ecosystem/purecpp**](https://github.com/pureai-ecosystem/purecpp).   
Contributions are welcome, whether through ideas, code, or simply sharing feedback. And if you find it useful, dropping a star on GitHub would mean a lot!",2025-03-31 16:16:29,39,2,MachineLearning,post,"[P] Developing a open-source (Retrieval Augmented Generation) framework written in C++ with python bindings for high performance
Been exploring ways to optimize *Retrieval-Augmented Generation* (RAG) lately, and it’s clear that there’s always more ground to cover when it comes to balancing performance, speed, and resource efficiency in dynamic environments.

So, we decided to build an open-source framework designed to push those boundaries,  handling retrieval tasks faster, scaling efficiently, and integrating with key tools in the ecosystem.

We’re still in early development, but initial benchmarks are already showing some promising results. In certain cases, it’s matching or even surpassing well-known solutions like LangChain and LlamaIndex in performance.

[Comparisson for CPU usage over time](https://preview.redd.it/zsdeodwl91se1.jpg?width=2038&format=pjpg&auto=webp&s=25b2531bedbbee72646bb876d2e40516df982a1f)

[Comparisson for PDF extration and chunking](https://preview.redd.it/8urmicwl91se1.jpg?width=2038&format=pjpg&auto=webp&s=5b3f67249cee86a01c44547e39dcfdd777306f5b)

It integrates smoothly with tools like **TensorRT**, **FAISS**, **vLLM** and others. And our roadmap is packed with further optimizations, tools integrations and updates we’re excited to roll out.

If that sounds like something you’d like to explore, check out the GitHub repo:[ **https://github.com/pureai-ecosystem/purecpp**](https://github.com/pureai-ecosystem/purecpp).   
Contributions are welcome, whether through ideas, code, or simply sharing feedback. And if you find it useful, dropping a star on GitHub would mean a lot!"
1jol6ez,[D] IJCNN 2025 results seems vague,"My IJCNN paper is rejected (fair enough). However the reviewer comments are very good usually atleast one reviewer criticize the work to be rejected. Moreover individual reviewer score is not shared which is not the case of top conferences. And this statement at the end of the email :


Thank you again for your submission, but stay tuned, a selection of papers will soon be invited to participate in additional initiatives related to IJCNN 2025.


Thoughts?",2025-04-01 04:32:55,2,13,MachineLearning,post,"[D] IJCNN 2025 results seems vague
My IJCNN paper is rejected (fair enough). However the reviewer comments are very good usually atleast one reviewer criticize the work to be rejected. Moreover individual reviewer score is not shared which is not the case of top conferences. And this statement at the end of the email :


Thank you again for your submission, but stay tuned, a selection of papers will soon be invited to participate in additional initiatives related to IJCNN 2025.


Thoughts?"
1jokue7,IJCNN Acceptance Notification [N],"Hello , did anybody get their acceptance notification for [IJCNN](https://2025.ijcnn.org) 2025. Today was supposed to be the paper notification date. I submitted a paper and haven't gotten any response yet.",2025-04-01 04:15:33,3,7,MachineLearning,post,"IJCNN Acceptance Notification [N]
Hello , did anybody get their acceptance notification for [IJCNN](https://2025.ijcnn.org) 2025. Today was supposed to be the paper notification date. I submitted a paper and haven't gotten any response yet."
1jo04ad,[R] Trajectory-Guided Video Motion Segmentation Using DINO Features and SAM2 Prompting,"SAM-Motion introduces a novel approach to video object segmentation by focusing on motion patterns rather than object categories. The key innovation is a motion pattern encoding technique that leverages trajectory information to identify and segment moving objects of any type in videos.

The technical approach consists of:
* **Motion Pattern Encoding**: Tracks point trajectories across video frames using RAFT for optical flow estimation
* **Per-trajectory Motion Prediction**: Determines if trajectories belong to moving objects by comparing against camera motion
* **Motion Decoder**: Generates precise segmentation masks by combining motion information with SAM architecture
* Works without category-specific training, making it generalizable to any moving object

Key results:
* State-of-the-art performance on DAVIS, FBMS, and MoCA datasets
* Successfully segments diverse motion types: rigid (vehicles), articulated (humans), and non-rigid (fluids)
* Enables applications like selective motion freezing and interactive editing
* Outperforms existing methods in both accuracy and generalization ability

I think this approach represents a significant paradigm shift in how we tackle video understanding. By focusing on motion patterns rather than pre-defined categories, SAM-Motion offers much greater flexibility for real-world applications. The trajectory-based method seems particularly well-suited for scenarios where object appearance varies widely but motion characteristics remain distinct.

I think the most promising aspect is how this bridges the gap between motion analysis and object segmentation. Traditional methods excel at one or the other, but SAM-Motion effectively combines both paradigms. This could be particularly valuable for robotics and autonomous systems that need to identify and track moving objects in dynamic environments.

That said, the dependence on high-quality trajectory estimation could be limiting in challenging conditions like poor lighting or extremely fast motion. I'd be interested to see how robust this approach is in more adverse real-world scenarios.

TLDR: SAM-Motion segments any moving object in videos by encoding motion patterns from trajectory information, achieving SOTA results without category-specific training, and enabling new video editing capabilities.

[Full summary is here](https://aimodels.fyi/papers/arxiv/segment-any-motion-videos). Paper [here](https://arxiv.org/abs/2503.22268).",2025-03-31 12:34:22,18,1,MachineLearning,post,"[R] Trajectory-Guided Video Motion Segmentation Using DINO Features and SAM2 Prompting
SAM-Motion introduces a novel approach to video object segmentation by focusing on motion patterns rather than object categories. The key innovation is a motion pattern encoding technique that leverages trajectory information to identify and segment moving objects of any type in videos.

The technical approach consists of:
* **Motion Pattern Encoding**: Tracks point trajectories across video frames using RAFT for optical flow estimation
* **Per-trajectory Motion Prediction**: Determines if trajectories belong to moving objects by comparing against camera motion
* **Motion Decoder**: Generates precise segmentation masks by combining motion information with SAM architecture
* Works without category-specific training, making it generalizable to any moving object

Key results:
* State-of-the-art performance on DAVIS, FBMS, and MoCA datasets
* Successfully segments diverse motion types: rigid (vehicles), articulated (humans), and non-rigid (fluids)
* Enables applications like selective motion freezing and interactive editing
* Outperforms existing methods in both accuracy and generalization ability

I think this approach represents a significant paradigm shift in how we tackle video understanding. By focusing on motion patterns rather than pre-defined categories, SAM-Motion offers much greater flexibility for real-world applications. The trajectory-based method seems particularly well-suited for scenarios where object appearance varies widely but motion characteristics remain distinct.

I think the most promising aspect is how this bridges the gap between motion analysis and object segmentation. Traditional methods excel at one or the other, but SAM-Motion effectively combines both paradigms. This could be particularly valuable for robotics and autonomous systems that need to identify and track moving objects in dynamic environments.

That said, the dependence on high-quality trajectory estimation could be limiting in challenging conditions like poor lighting or extremely fast motion. I'd be interested to see how robust this approach is in more adverse real-world scenarios.

TLDR: SAM-Motion segments any moving object in videos by encoding motion patterns from trajectory information, achieving SOTA results without category-specific training, and enabling new video editing capabilities.

[Full summary is here](https://aimodels.fyi/papers/arxiv/segment-any-motion-videos). Paper [here](https://arxiv.org/abs/2503.22268)."
1joncpd,[D] Multi-GPU Thread,"I've just bought parts for my first PC build. I was deadset in January on getting an rtx 5090 and attempted almost every drop to no avail. Unfortunately with the tariffs, the price is now out of my budget, so I decided to go with a 7900xtx. I bought a mobo that has 2 pcie 5.0 x16 lanes, so I can utilize two GPUs at x8 lanes.

My main question is, can you mix GPUs? I was torn between the 9070xt or the 7900xtx since the 9070xt only has 16gb of VRAM while the 7900xtx has 24gb. I opted for more VRAM even though it has marginally lower boost clock speeds. Would it be possible to get both cards? If not, dual 7900xtxs could work, but it would be nice if I could allocate the 9070xt for stuff such as gaming and then both cards if I want parallel processing of different ML workloads.

From my understanding, the VRAM isn't necessarily additive, but I'm also confused since others claim their dual 7900xtx setups allow them to work with larger LLMs.

What are the limitations for dual GPU setups and is it possible to use different cards? I'm definitely assuming you can't mix both AMD and Nvidia as the drivers and structure are extremely different (or maybe I'm mistaken there too and there's some software magic to let you mix).

I'm new to PC building, but have a few years experience tinkering with and training AI/ML models.",2025-04-01 06:35:47,0,5,MachineLearning,post,"[D] Multi-GPU Thread
I've just bought parts for my first PC build. I was deadset in January on getting an rtx 5090 and attempted almost every drop to no avail. Unfortunately with the tariffs, the price is now out of my budget, so I decided to go with a 7900xtx. I bought a mobo that has 2 pcie 5.0 x16 lanes, so I can utilize two GPUs at x8 lanes.

My main question is, can you mix GPUs? I was torn between the 9070xt or the 7900xtx since the 9070xt only has 16gb of VRAM while the 7900xtx has 24gb. I opted for more VRAM even though it has marginally lower boost clock speeds. Would it be possible to get both cards? If not, dual 7900xtxs could work, but it would be nice if I could allocate the 9070xt for stuff such as gaming and then both cards if I want parallel processing of different ML workloads.

From my understanding, the VRAM isn't necessarily additive, but I'm also confused since others claim their dual 7900xtx setups allow them to work with larger LLMs.

What are the limitations for dual GPU setups and is it possible to use different cards? I'm definitely assuming you can't mix both AMD and Nvidia as the drivers and structure are extremely different (or maybe I'm mistaken there too and there's some software magic to let you mix).

I'm new to PC building, but have a few years experience tinkering with and training AI/ML models."
1jo7l3f,[P] Best Approach to Building an Efficient Search Tool for a Metadata Dictionary in Excel,"I am working with a metadata dictionary stored in Excel, which contains information about database fields across multiple tables. The dataset includes the following columns:

Physical Table Name

Database Name

Physical Column Name (e.g., hlp_mgr_12_full_nm)

Logical Column Name (e.g., Home Loan Processor Manager 12 Name)

Definition (e.g., Name of the 12th manager in the loan processing team)

Primary/Foreign Key Indicator (Rows where a column is a primary or foreign key are marked as True)


Problem Statement

I want to build a search engine that allows users to enter a query and get the most relevant columns from the dictionary, ranked by relevance. The challenge is that:

1. Exact matches aren’t always available – Users might search for ""loan number,"" but the metadata might store it as ""Servicing Loan Account Number"" (srvcing_loan_acc_num).


2. Acronyms and abbreviations exist – Physical column names often use acronyms (hlp_mgr_12_full_nm), while logical names are in full form (Home Loan Processor Manager 12 Name). The search should understand these mappings.


3. Users should be able to filter by table/database – The user may want to search only within a specific table or database. This filtering should be applied before the ranking process.


4. Primary/Foreign Key Retrieval – For any table returned in the search results, I need to automatically list its primary and foreign keys in a separate column. Since a table can have multiple keys, they should be concatenated in a single cell (comma-separated).


5. The search should work well even in a restrictive environment – I am working in a VDI environment where I can’t install large NLP models (e.g., sentence-transformers). Solutions that are lightweight and work locally are preferred.



Current Approaches I Am Exploring

So far, I have considered the following:

1. TF-IDF + Fuzzy Matching:

Precompute TF-IDF embeddings for the metadata dictionary.

Use cosine similarity to compare search queries against the metadata.

Combine this with fuzzy string matching (fuzz.partial_ratio) to improve ranking.



2. Acronym Expansion & Normalization:

Maintain a dictionary of common acronyms (e.g., hlp -> home loan processor, mgr -> manager).

Expand query terms before searching.



3. Exact Table/Database Filtering:

Apply exact match filtering on table and database names first before performing text matching.



4. Concatenation of Primary/Foreign Keys:

Extract all primary/foreign keys for each table in the results and concatenate them into a single output column.




Looking for Better Approaches

While these approaches work reasonably well, I am looking for alternative solutions beyond NLP that might be faster, more efficient, and simpler to implement in a restricted VDI environment.

Would a different ranking strategy work better?

Is there a database indexing technique that could improve search speed?

Are there other lightweight similarity approaches I haven’t considered?


Would love to hear from others who have solved similar metadata search challenges! Any insights or suggestions are greatly appreciated.",2025-03-31 18:38:53,3,0,MachineLearning,post,"[P] Best Approach to Building an Efficient Search Tool for a Metadata Dictionary in Excel
I am working with a metadata dictionary stored in Excel, which contains information about database fields across multiple tables. The dataset includes the following columns:

Physical Table Name

Database Name

Physical Column Name (e.g., hlp_mgr_12_full_nm)

Logical Column Name (e.g., Home Loan Processor Manager 12 Name)

Definition (e.g., Name of the 12th manager in the loan processing team)

Primary/Foreign Key Indicator (Rows where a column is a primary or foreign key are marked as True)


Problem Statement

I want to build a search engine that allows users to enter a query and get the most relevant columns from the dictionary, ranked by relevance. The challenge is that:

1. Exact matches aren’t always available – Users might search for ""loan number,"" but the metadata might store it as ""Servicing Loan Account Number"" (srvcing_loan_acc_num).


2. Acronyms and abbreviations exist – Physical column names often use acronyms (hlp_mgr_12_full_nm), while logical names are in full form (Home Loan Processor Manager 12 Name). The search should understand these mappings.


3. Users should be able to filter by table/database – The user may want to search only within a specific table or database. This filtering should be applied before the ranking process.


4. Primary/Foreign Key Retrieval – For any table returned in the search results, I need to automatically list its primary and foreign keys in a separate column. Since a table can have multiple keys, they should be concatenated in a single cell (comma-separated).


5. The search should work well even in a restrictive environment – I am working in a VDI environment where I can’t install large NLP models (e.g., sentence-transformers). Solutions that are lightweight and work locally are preferred.



Current Approaches I Am Exploring

So far, I have considered the following:

1. TF-IDF + Fuzzy Matching:

Precompute TF-IDF embeddings for the metadata dictionary.

Use cosine similarity to compare search queries against the metadata.

Combine this with fuzzy string matching (fuzz.partial_ratio) to improve ranking.



2. Acronym Expansion & Normalization:

Maintain a dictionary of common acronyms (e.g., hlp -> home loan processor, mgr -> manager).

Expand query terms before searching.



3. Exact Table/Database Filtering:

Apply exact match filtering on table and database names first before performing text matching.



4. Concatenation of Primary/Foreign Keys:

Extract all primary/foreign keys for each table in the results and concatenate them into a single output column.




Looking for Better Approaches

While these approaches work reasonably well, I am looking for alternative solutions beyond NLP that might be faster, more efficient, and simpler to implement in a restricted VDI environment.

Would a different ranking strategy work better?

Is there a database indexing technique that could improve search speed?

Are there other lightweight similarity approaches I haven’t considered?


Would love to hear from others who have solved similar metadata search challenges! Any insights or suggestions are greatly appreciated."
1jo2il4,[R] DeepFake video detection: Insights into model generalisation — A Systematic review,"I'm excited to share that my paper, **“DeepFake Video Detection: Insights into Model Generalisation - A Systematic Review,”** has been published in an Elsevier Q2 Open Access Journal. This work examines the current landscape of deep learning models used for detecting deepfakes, with a special focus on how well these models can generalize across different datasets and scenarios—a critical factor in their real-world application.

Key highlights from the study include:

- **Model Generalisation:** The research identifies key challenges in achieving robust performance when detection models encounter new, unseen data. We discuss strategies to enhance model adaptability, crucial for keeping pace with evolving deepfake techniques.
- **Methodological Advances:** The paper reviews various architectural innovations and algorithmic strategies that show promise in improving detection accuracy and efficiency.
- **Cross-Dataset Performance:** A significant portion of the paper is dedicated to analyzing how these models perform across different datasets, a factor critical to their practical deployment. The study suggests improvements in training practices to better prepare models for a diverse range of inputs.

📄 [Read the full paper here.] https://www.sciencedirect.com/science/article/pii/S2543925125000075

I’d love to engage with the community here and hear your thoughts or questions about the research. How do you see AI and deep learning contributing to media security, and what are your thoughts on overcoming the challenges posed by deepfake technology?",2025-03-31 14:55:30,7,0,MachineLearning,post,"[R] DeepFake video detection: Insights into model generalisation — A Systematic review
I'm excited to share that my paper, **“DeepFake Video Detection: Insights into Model Generalisation - A Systematic Review,”** has been published in an Elsevier Q2 Open Access Journal. This work examines the current landscape of deep learning models used for detecting deepfakes, with a special focus on how well these models can generalize across different datasets and scenarios—a critical factor in their real-world application.

Key highlights from the study include:

- **Model Generalisation:** The research identifies key challenges in achieving robust performance when detection models encounter new, unseen data. We discuss strategies to enhance model adaptability, crucial for keeping pace with evolving deepfake techniques.
- **Methodological Advances:** The paper reviews various architectural innovations and algorithmic strategies that show promise in improving detection accuracy and efficiency.
- **Cross-Dataset Performance:** A significant portion of the paper is dedicated to analyzing how these models perform across different datasets, a factor critical to their practical deployment. The study suggests improvements in training practices to better prepare models for a diverse range of inputs.

📄 [Read the full paper here.] https://www.sciencedirect.com/science/article/pii/S2543925125000075

I’d love to engage with the community here and hear your thoughts or questions about the research. How do you see AI and deep learning contributing to media security, and what are your thoughts on overcoming the challenges posed by deepfake technology?"
1joby4h,[P] [D] Having trouble enhancing GNN + LSTM for 3D data forecasting,"Hi everyone! I’m working on a forecasting task involving 3D data with shape \[T, H, W\], where each frame corresponds to a daily snapshot. I’m trying to model both spatial and temporal dependencies, but I’m running into some issues and would love some advice on improving the model’s performance.

**Setup**

* I flatten each \[H, W\] frame into \[N\], where N is the number of valid spatial locations.
* The full dataset becomes a \[T, N\] time series.
* I split the data chronologically into train, val, and test sets. So, no shuffling when splitting my data

**Graph Construction**

* For each sequence (e.g., 7 days), I construct a semi-dynamic (I am not sure what to call it) sequence of graphs Gₜ.
* Node features: \[value, h, w\], where the ""value"" changes daily.
* Edges: Static across the sequence based on:
   * Euclidean distance threshold
   * Pearson correlation computed over the sequence
* Edge features: Direction (angle to north) and distance
* Loss: MAE (shown below)

https://preview.redd.it/qf160okfu2se1.png?width=1979&format=png&auto=webp&s=1a447b88192feae1ca19b6be1befc666aea1a71e

**Model**

* Spatial Encoder: 4-layer GNN (edge update → edge aggregation → node update)
   * Recently added skip connections, self-attention, and increased hidden units
* Temporal Encoder: 2-layer LSTM
* Prediction Head: Feedforward layer to predict values for the next 3 time steps

**Current Behavior**

* Initially, GNN layers were barely learning. LSTM and FF layers dominated.
* After adding skip connections and self-attention, GNN behavior improved somewhat, but overall loss is still high
* Training is slow, so it's hard to iterate quickly
* I'm currently prototyping using just 3 batches for training/validation to track behavior more easily. I have around 500 batches in total.

**Parameter Update Magnitudes**  
Tracking L2 norm of weight changes across layers:

https://preview.redd.it/x8smklgku2se1.png?width=1920&format=png&auto=webp&s=07153d7b052c89f0f256e2e13f06f2431df4ee24

I’m currently trying to figure out how to break out of this learning plateau. The model starts converging quickly but then flattens out (around MAE ≈ 5), even with a scheduled learning rate and weight decay in place.

Could this be a case of overcomplicating the architecture? Would switching from MAE to a different loss function help with optimization stability or gradient flow?

Also, if anyone has advice on better ways to integrate spatial learning early on (e.g., via pretraining or regularization) or general tips for speeding up convergence in GNN+LSTM pipelines, I’d love to hear it!",2025-03-31 21:35:48,2,4,MachineLearning,post,"[P] [D] Having trouble enhancing GNN + LSTM for 3D data forecasting
Hi everyone! I’m working on a forecasting task involving 3D data with shape \[T, H, W\], where each frame corresponds to a daily snapshot. I’m trying to model both spatial and temporal dependencies, but I’m running into some issues and would love some advice on improving the model’s performance.

**Setup**

* I flatten each \[H, W\] frame into \[N\], where N is the number of valid spatial locations.
* The full dataset becomes a \[T, N\] time series.
* I split the data chronologically into train, val, and test sets. So, no shuffling when splitting my data

**Graph Construction**

* For each sequence (e.g., 7 days), I construct a semi-dynamic (I am not sure what to call it) sequence of graphs Gₜ.
* Node features: \[value, h, w\], where the ""value"" changes daily.
* Edges: Static across the sequence based on:
   * Euclidean distance threshold
   * Pearson correlation computed over the sequence
* Edge features: Direction (angle to north) and distance
* Loss: MAE (shown below)

https://preview.redd.it/qf160okfu2se1.png?width=1979&format=png&auto=webp&s=1a447b88192feae1ca19b6be1befc666aea1a71e

**Model**

* Spatial Encoder: 4-layer GNN (edge update → edge aggregation → node update)
   * Recently added skip connections, self-attention, and increased hidden units
* Temporal Encoder: 2-layer LSTM
* Prediction Head: Feedforward layer to predict values for the next 3 time steps

**Current Behavior**

* Initially, GNN layers were barely learning. LSTM and FF layers dominated.
* After adding skip connections and self-attention, GNN behavior improved somewhat, but overall loss is still high
* Training is slow, so it's hard to iterate quickly
* I'm currently prototyping using just 3 batches for training/validation to track behavior more easily. I have around 500 batches in total.

**Parameter Update Magnitudes**  
Tracking L2 norm of weight changes across layers:

https://preview.redd.it/x8smklgku2se1.png?width=1920&format=png&auto=webp&s=07153d7b052c89f0f256e2e13f06f2431df4ee24

I’m currently trying to figure out how to break out of this learning plateau. The model starts converging quickly but then flattens out (around MAE ≈ 5), even with a scheduled learning rate and weight decay in place.

Could this be a case of overcomplicating the architecture? Would switching from MAE to a different loss function help with optimization stability or gradient flow?

Also, if anyone has advice on better ways to integrate spatial learning early on (e.g., via pretraining or regularization) or general tips for speeding up convergence in GNN+LSTM pipelines, I’d love to hear it!"
1joh6di,[R] IEEE Access publishing,Im looking to make a paper into a new metric to evaluate prompt engineering(pls don't hound me for this) for code generation. Do you guys think it has a good chance to get published in IEEE Access. Btw im a HS Senior looking to boost my college app. thanks for the help!,2025-04-01 01:15:21,0,4,MachineLearning,post,"[R] IEEE Access publishing
Im looking to make a paper into a new metric to evaluate prompt engineering(pls don't hound me for this) for code generation. Do you guys think it has a good chance to get published in IEEE Access. Btw im a HS Senior looking to boost my college app. thanks for the help!"
1jo7w6z,[D] distillation with different number of tokens,"Hi folks,
I've been reading some distillation literature for image encoders, particular vit and variants.

Often when distilling a larger model with a bigger embedding dimension than the student model, we use an up-projection linear layer that is thrown away after distillation.

What do you do when you have different number of tokens? This can arise if you're using different patch sizes or image resolutions or just different pooling techniques.

I havent been able to find literature that does this so wanted to know if there were some common approaches I'm missing 

Thanks!",2025-03-31 18:51:45,0,2,MachineLearning,post,"[D] distillation with different number of tokens
Hi folks,
I've been reading some distillation literature for image encoders, particular vit and variants.

Often when distilling a larger model with a bigger embedding dimension than the student model, we use an up-projection linear layer that is thrown away after distillation.

What do you do when you have different number of tokens? This can arise if you're using different patch sizes or image resolutions or just different pooling techniques.

I havent been able to find literature that does this so wanted to know if there were some common approaches I'm missing 

Thanks!"
1jnxkga,[P] Curated List of Awesome Time Series Papers – Open Source Resource on GitHub,"Hey everyone 

If you're into time series analysis like I am, I wanted to share a GitHub repo I’ve been working on:  
👉 [Awesome Time Series Papers](https://github.com/hushuguo/awesome-time-series-papers)

It’s a curated collection of influential and recent research papers related to time series forecasting, classification, anomaly detection, representation learning, and more. 📚

The goal is to make it easier for practitioners and researchers to explore key developments in this field without digging through endless conference proceedings.

Topics covered:

* Forecasting (classical + deep learning)
* Anomaly detection
* Representation learning
* Time series classification
* Benchmarks and datasets
* Reviews and surveys

I’d love to get feedback or suggestions—if you have a favorite paper that’s missing, PRs and issues are welcome 🙌

Hope it helps someone here!",2025-03-31 09:18:46,5,0,MachineLearning,post,"[P] Curated List of Awesome Time Series Papers – Open Source Resource on GitHub
Hey everyone 

If you're into time series analysis like I am, I wanted to share a GitHub repo I’ve been working on:  
👉 [Awesome Time Series Papers](https://github.com/hushuguo/awesome-time-series-papers)

It’s a curated collection of influential and recent research papers related to time series forecasting, classification, anomaly detection, representation learning, and more. 📚

The goal is to make it easier for practitioners and researchers to explore key developments in this field without digging through endless conference proceedings.

Topics covered:

* Forecasting (classical + deep learning)
* Anomaly detection
* Representation learning
* Time series classification
* Benchmarks and datasets
* Reviews and surveys

I’d love to get feedback or suggestions—if you have a favorite paper that’s missing, PRs and issues are welcome 🙌

Hope it helps someone here!"
1jnjfaq,[D] Why is table extraction still not solved by modern multimodal models?,"There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/krox7ytlhvre1.png?width=1650&format=png&auto=webp&s=5daa7f68f4acc55f4bdac3b2defa21b9ebfae0d9

",2025-03-30 20:47:05,41,45,MachineLearning,post,"[D] Why is table extraction still not solved by modern multimodal models?
There is a lot of hype around multimodal models, such as Qwen 2.5 VL or Omni, GOT, SmolDocling, etc. I would like to know if others made a similar experience in practice: While they can do impressive things, they still struggle with table extraction, in cases which are straight-forward for humans.

Attached is a simple example, all I need is a reconstruction of the table as a flat CSV, preserving empty all empty cells correctly. Which open source model is able to do that?

https://preview.redd.it/krox7ytlhvre1.png?width=1650&format=png&auto=webp&s=5daa7f68f4acc55f4bdac3b2defa21b9ebfae0d9

"
1jnyr2m,[D][R]Question about LLM VS prophet on Time series forcasting Task,"**Background:**

The company has financial data related to income and expenses, categorized into five types. For each category, there are approximately 60 data points spanning from 2020 to 2024. The data exhibits reasonable periodicity, with visible year-over-year increases and decreases. Due to the small sample size, the consideration is to use simple models or zero-shot forecasting models for prediction.

**Current Status:**

Currently, the company is using Facebook's Prophet statistical machine learning model, which has yielded satisfactory results. There's an ongoing effort to explore time series foundation models for zero-shot forecasting. Initial attempts with Tsinghua's Timer and Amazon's Chronos models have shown poor performance, often degenerating into near-mean predictions and failing to capture trends.

**Question:**

The question is whether anyone has experience with similar tasks and can recommend models that would perform well with such a small sample size. Additionally, are there any other time series foundation models worth trying?",2025-03-31 10:53:24,0,4,MachineLearning,post,"[D][R]Question about LLM VS prophet on Time series forcasting Task
**Background:**

The company has financial data related to income and expenses, categorized into five types. For each category, there are approximately 60 data points spanning from 2020 to 2024. The data exhibits reasonable periodicity, with visible year-over-year increases and decreases. Due to the small sample size, the consideration is to use simple models or zero-shot forecasting models for prediction.

**Current Status:**

Currently, the company is using Facebook's Prophet statistical machine learning model, which has yielded satisfactory results. There's an ongoing effort to explore time series foundation models for zero-shot forecasting. Initial attempts with Tsinghua's Timer and Amazon's Chronos models have shown poor performance, often degenerating into near-mean predictions and failing to capture trends.

**Question:**

The question is whether anyone has experience with similar tasks and can recommend models that would perform well with such a small sample size. Additionally, are there any other time series foundation models worth trying?"
1jneuix,[Discussion] Linear Regression performs better than LGBM or XGBoost on Time Series,"Hello, I'm developing a model to hourly forecast weather. They're more than 100000+ temperature points. I used shifting rolling and ewm, each of them from 1 to 24 and weekly and monthly.  
Linear regression mae result is 0.30-0.31 while XGBoost performs 0.32-0.34 and LGBM performs 0.334. I've tried many parameters or asked chatgpt with providing the code but I don't know If I am doing something really wrong or it is totally normal situation.",2025-03-30 17:26:24,20,14,MachineLearning,post,"[Discussion] Linear Regression performs better than LGBM or XGBoost on Time Series
Hello, I'm developing a model to hourly forecast weather. They're more than 100000+ temperature points. I used shifting rolling and ewm, each of them from 1 to 24 and weekly and monthly.  
Linear regression mae result is 0.30-0.31 while XGBoost performs 0.32-0.34 and LGBM performs 0.334. I've tried many parameters or asked chatgpt with providing the code but I don't know If I am doing something really wrong or it is totally normal situation."
1jny30f,[D] CLI for merging repos LLM Context,"Hey I created a simple tool to merge repos into a single file so that I can give context to LLMs (especially web based)

It prefixes each file with its relative path, applies configurable probabilistic line skipping, and filters to include only human-readable code.

**\*How can we further reduce the file size while preserving context for LLMs?\***

Currently I just skip lines based on probability

EDIT : [Code](https://pastebin.com/2kfPe3zP)",2025-03-31 09:59:42,0,5,MachineLearning,post,"[D] CLI for merging repos LLM Context
Hey I created a simple tool to merge repos into a single file so that I can give context to LLMs (especially web based)

It prefixes each file with its relative path, applies configurable probabilistic line skipping, and filters to include only human-readable code.

**\*How can we further reduce the file size while preserving context for LLMs?\***

Currently I just skip lines based on probability

EDIT : [Code](https://pastebin.com/2kfPe3zP)"
1jn0ha9,[R] [D] My (Mostly Failed) Attempt to Improve Transformers by Enriching Embeddings with the Last Hidden State – Why It Didn't Scale,"Hi guys!

I [recently posted](https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/) on this sub about what I believed was a sub-optimal feature of Decoder Transformers: namely the fact that the last hidden state, which has the potential to carry a lot of information (32 bits \* embedding dim), is collapsed into a single token (assuming temperature is 0), that can only carry log2(vocab\_size) bits of information.

I tested a new architecture where the last hidden state of the transformer is used to enrich the embedding of the token that was generated using it (it = the last hidden state).

And, would you believe it? It failed. 

The worst thing about it is that it worked well enough for very small (100K params) transformers to give me hope and feed my self delusional grandiosity. I had even given this architecture a name. But when I scaled it up (a whopping 1M params!!), the compute overhead stopped being worth the improvement.

The high-level idea of why it failed is that every hidden state of every previous token, up to the penultimate one (the input of the last decoder block) are available when predicting the next token, thanks to the token-mixing property of the attention mechanism. Only the last couple of hidden states (the input of the last decoder block's FFN, and final linear layer + softmax) are unavailable, as there are no token-mixing steps left. So this hidden state injection idea is merely about not discarding the work done by the last couple layers, which is not that important when there are a lot of decoder layers (the marginal importance of each layer decreases).

Anyway, I wrote a [5,000 words post](https://www.eloidereynal.com/p/a-partially-failed-attempt-at-improving?r=4ksqg3&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false) about why it failed, with a bit of nice math and some cattle pictures, just in case you like cows. 

Honestly, the post is quite long and technical, but you might find one or two interesting things, especially if you like to read about the failures of other people.",2025-03-30 01:29:36,163,17,MachineLearning,post,"[R] [D] My (Mostly Failed) Attempt to Improve Transformers by Enriching Embeddings with the Last Hidden State – Why It Didn't Scale
Hi guys!

I [recently posted](https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/) on this sub about what I believed was a sub-optimal feature of Decoder Transformers: namely the fact that the last hidden state, which has the potential to carry a lot of information (32 bits \* embedding dim), is collapsed into a single token (assuming temperature is 0), that can only carry log2(vocab\_size) bits of information.

I tested a new architecture where the last hidden state of the transformer is used to enrich the embedding of the token that was generated using it (it = the last hidden state).

And, would you believe it? It failed. 

The worst thing about it is that it worked well enough for very small (100K params) transformers to give me hope and feed my self delusional grandiosity. I had even given this architecture a name. But when I scaled it up (a whopping 1M params!!), the compute overhead stopped being worth the improvement.

The high-level idea of why it failed is that every hidden state of every previous token, up to the penultimate one (the input of the last decoder block) are available when predicting the next token, thanks to the token-mixing property of the attention mechanism. Only the last couple of hidden states (the input of the last decoder block's FFN, and final linear layer + softmax) are unavailable, as there are no token-mixing steps left. So this hidden state injection idea is merely about not discarding the work done by the last couple layers, which is not that important when there are a lot of decoder layers (the marginal importance of each layer decreases).

Anyway, I wrote a [5,000 words post](https://www.eloidereynal.com/p/a-partially-failed-attempt-at-improving?r=4ksqg3&utm_campaign=post&utm_medium=web&showWelcomeOnShare=false) about why it failed, with a bit of nice math and some cattle pictures, just in case you like cows. 

Honestly, the post is quite long and technical, but you might find one or two interesting things, especially if you like to read about the failures of other people."
1jngh6e,[P] Agent - A Local Computer-Use Operator for macOS,"We've just open-sourced Agent, our framework for running computer-use workflows across multiple apps in isolated macOS/Linux sandboxes. 

**Grab the code at** [**https://github.com/trycua/cua**](https://github.com/trycua/cua)

After launching Computer a few weeks ago, we realized many of you wanted to run complex workflows that span multiple applications. Agent builds on Computer to make this possible. It works with local Ollama models (if you're privacy-minded) or cloud providers like OpenAI, Anthropic, and others.

**Why we built this:**

We kept hitting the same problems when building multi-app AI agents - they'd break in unpredictable ways, work inconsistently across environments, or just fail with complex workflows. So we built Agent to solve these headaches:

•⁠ ⁠It handles complex workflows across multiple apps without falling apart

•⁠ ⁠You can use your preferred model (local or cloud) - we're not locking you into one provider

•⁠ ⁠You can swap between different agent loop implementations depending on what you're building

•⁠ ⁠You get clean, structured responses that work well with other tools

**The code is pretty straightforward:**

`async with Computer() as macos_computer:`

`agent = ComputerAgent(`

`computer=macos_computer,`

`loop=AgentLoop.OPENAI,`

`model=LLM(provider=LLMProvider.OPENAI)`

`)`

`tasks = [`

`""Look for a repository named trycua/cua on GitHub."",`

`""Check the open issues, open the most recent one and read it."",`

`""Clone the repository if it doesn't exist yet.""`

`]`

`for i, task in enumerate(tasks):`

`print(f""\nTask {i+1}/{len(tasks)}: {task}"")`

`async for result in agent.run(task):`

`print(result)`

`print(f""\nFinished task {i+1}!"")`

**Some cool things you can do with it:**

•⁠ ⁠Mix and match agent loops - OpenAI for some tasks, Claude for others, or try our experimental OmniParser

•⁠ ⁠Run it with various models - works great with OpenAI's computer\_use\_preview, but also with Claude and others

•⁠ ⁠Get detailed logs of what your agent is thinking/doing (super helpful for debugging)

•⁠ ⁠All the sandboxing from Computer means your main system stays protected

**Getting started is easy:**

`pip install ""cua-agent[all]""`

\# Or if you only need specific providers:

`pip install ""cua-agent[openai]"" # Just OpenAI`

`pip install ""cua-agent[anthropic]"" # Just Anthropic`

`pip install ""cua-agent[omni]"" # Our experimental OmniParser`

We've been dogfooding this internally for weeks now, and it's been a game-changer for automating our workflows. 

Would love to hear your thoughts ! :)",2025-03-30 18:38:56,6,2,MachineLearning,post,"[P] Agent - A Local Computer-Use Operator for macOS
We've just open-sourced Agent, our framework for running computer-use workflows across multiple apps in isolated macOS/Linux sandboxes. 

**Grab the code at** [**https://github.com/trycua/cua**](https://github.com/trycua/cua)

After launching Computer a few weeks ago, we realized many of you wanted to run complex workflows that span multiple applications. Agent builds on Computer to make this possible. It works with local Ollama models (if you're privacy-minded) or cloud providers like OpenAI, Anthropic, and others.

**Why we built this:**

We kept hitting the same problems when building multi-app AI agents - they'd break in unpredictable ways, work inconsistently across environments, or just fail with complex workflows. So we built Agent to solve these headaches:

•⁠ ⁠It handles complex workflows across multiple apps without falling apart

•⁠ ⁠You can use your preferred model (local or cloud) - we're not locking you into one provider

•⁠ ⁠You can swap between different agent loop implementations depending on what you're building

•⁠ ⁠You get clean, structured responses that work well with other tools

**The code is pretty straightforward:**

`async with Computer() as macos_computer:`

`agent = ComputerAgent(`

`computer=macos_computer,`

`loop=AgentLoop.OPENAI,`

`model=LLM(provider=LLMProvider.OPENAI)`

`)`

`tasks = [`

`""Look for a repository named trycua/cua on GitHub."",`

`""Check the open issues, open the most recent one and read it."",`

`""Clone the repository if it doesn't exist yet.""`

`]`

`for i, task in enumerate(tasks):`

`print(f""\nTask {i+1}/{len(tasks)}: {task}"")`

`async for result in agent.run(task):`

`print(result)`

`print(f""\nFinished task {i+1}!"")`

**Some cool things you can do with it:**

•⁠ ⁠Mix and match agent loops - OpenAI for some tasks, Claude for others, or try our experimental OmniParser

•⁠ ⁠Run it with various models - works great with OpenAI's computer\_use\_preview, but also with Claude and others

•⁠ ⁠Get detailed logs of what your agent is thinking/doing (super helpful for debugging)

•⁠ ⁠All the sandboxing from Computer means your main system stays protected

**Getting started is easy:**

`pip install ""cua-agent[all]""`

\# Or if you only need specific providers:

`pip install ""cua-agent[openai]"" # Just OpenAI`

`pip install ""cua-agent[anthropic]"" # Just Anthropic`

`pip install ""cua-agent[omni]"" # Our experimental OmniParser`

We've been dogfooding this internally for weeks now, and it's been a game-changer for automating our workflows. 

Would love to hear your thoughts ! :)"
1jn7jvg,[R] Text based backprop: Optimizing generative AI by backpropagating language model feedback,"> Recent breakthroughs in artifcial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily handcrafted by domain experts and tweaked through heuristics rather than being automatically optimized, presenting a substantial challenge to accelerating progress. The development of artifcial neural networks faced a similar challenge until backpropagation and automatic diferentiation transformed the feld by making optimization turnkey. Analogously, here we introduce TextGrad, a versatile framework that performs optimization by backpropagating LLM-generated feedback to improve AI systems. By leveraging natural language feedback to critique and suggest improvements to any part of a system—from prompts to outputs such as molecules or treatment plans—TextGrad enables the automatic optimization of generative AI systems across diverse tasks. We demonstrate TextGrad’s generality and efectiveness through studies in solving PhD-level science problems, optimizing plans for radiotherapy treatments, designing molecules with specifc properties, coding, and optimizing agentic systems. TextGrad empowers scientists and engineers to easily develop impactful generative AI systems.

Interesting paper published on Nature on using text based backprop for LLM optimization. Might have some potential but still not a perfect optimization technique. 

Edit

Paper link: https://www.researchgate.net/publication/389991515_Optimizing_generative_AI_by_backpropagating_language_model_feedback",2025-03-30 09:50:09,22,8,MachineLearning,post,"[R] Text based backprop: Optimizing generative AI by backpropagating language model feedback
> Recent breakthroughs in artifcial intelligence (AI) are increasingly driven by systems orchestrating multiple large language models (LLMs) and other specialized tools, such as search engines and simulators. So far, these systems are primarily handcrafted by domain experts and tweaked through heuristics rather than being automatically optimized, presenting a substantial challenge to accelerating progress. The development of artifcial neural networks faced a similar challenge until backpropagation and automatic diferentiation transformed the feld by making optimization turnkey. Analogously, here we introduce TextGrad, a versatile framework that performs optimization by backpropagating LLM-generated feedback to improve AI systems. By leveraging natural language feedback to critique and suggest improvements to any part of a system—from prompts to outputs such as molecules or treatment plans—TextGrad enables the automatic optimization of generative AI systems across diverse tasks. We demonstrate TextGrad’s generality and efectiveness through studies in solving PhD-level science problems, optimizing plans for radiotherapy treatments, designing molecules with specifc properties, coding, and optimizing agentic systems. TextGrad empowers scientists and engineers to easily develop impactful generative AI systems.

Interesting paper published on Nature on using text based backprop for LLM optimization. Might have some potential but still not a perfect optimization technique. 

Edit

Paper link: https://www.researchgate.net/publication/389991515_Optimizing_generative_AI_by_backpropagating_language_model_feedback"
1jnqe2p,[R] [P] [D] Short Time Fourier Transform based Kolmogorov-Arnold Network called(STFT-KAN),"Recently, the Kolmogorov-Arnold Network (KAN) has been used in many deep learning applications to improve accuracy and interpretability over classical MLPs. However, the problem with KAN lies in complexity control. While we can increase the number of parameters by augmenting spline degrees or stacking more layers, the challenge arises when we aim to maintain the same number of parameters or fewer than a simple linear layer. In this context, we propose a new Kolmogorov-Arnold Network called STFT-KAN, which provides increased control over complexity and parametrization based on the Short Time Fourier Transform principle, without relying on complex nonlinear transformations, while maintaining comparable performance. I am sharing with you the GitHub repository for STFT-KAN, along with a simple benchmark using the MNIST 

dataset.Github: 🚀 [https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN](https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN)

We are waiting for your feedback!.",2025-03-31 02:02:55,1,0,MachineLearning,post,"[R] [P] [D] Short Time Fourier Transform based Kolmogorov-Arnold Network called(STFT-KAN)
Recently, the Kolmogorov-Arnold Network (KAN) has been used in many deep learning applications to improve accuracy and interpretability over classical MLPs. However, the problem with KAN lies in complexity control. While we can increase the number of parameters by augmenting spline degrees or stacking more layers, the challenge arises when we aim to maintain the same number of parameters or fewer than a simple linear layer. In this context, we propose a new Kolmogorov-Arnold Network called STFT-KAN, which provides increased control over complexity and parametrization based on the Short Time Fourier Transform principle, without relying on complex nonlinear transformations, while maintaining comparable performance. I am sharing with you the GitHub repository for STFT-KAN, along with a simple benchmark using the MNIST 

dataset.Github: 🚀 [https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN](https://github.com/said-ohamouddou/STFT-KAN-liteDGCNN)

We are waiting for your feedback!."
1jn6ttj,[R] Lumina-Image 2.0: Efficient Text-to-Image Generation via Unified Architecture and Progressive Training,"Just came across Lumina-Image 2.0, which introduces a unified transformer-based architecture for multiple image generation tasks and a novel sampling technique they call Multiple Sampling with Iterative Refinement (MSIR).

The key idea is replacing specialized architectures with a single model that handles text-to-image generation, image editing, inpainting, and outpainting through a transformer that treats images as sequences of tokens (similar to how LLMs handle text).

Key technical points:
- **MSIR sampling**: Generates multiple candidate images simultaneously (8-32) then selectively refines the most promising ones, improving quality without increasing computation
- **Unified architecture**: Single model handles multiple tasks using task-specific embedding tokens
- **Parallel decoding with deep fusion**: Processes multiple tokens in parallel then fuses results, significantly speeding up inference
- **Results**: 4.11 FID on COCO dataset, outperforming previous SOTA while using 38% less compute for training
- **Scaling efficiency**: 8B parameter model shows substantial improvements over 3B version while maintaining fast inference

I think this approach represents an important shift in image generation architecture. Moving away from specialized diffusion models toward unified transformer-based approaches could significantly simplify deployment and maintenance of AI image systems. The MSIR technique is particularly interesting as it provides a clever way to improve sample quality without the computational penalty of naive approaches.

The 38% reduction in training computation is noteworthy given the increasing concerns about AI's environmental impact. If we can get better models with less compute, that's a win for both performance and sustainability.

I'm curious to see if this unified architecture approach can extend beyond images to efficiently handle video or 3D generation tasks. The paper suggests this direction might be viable.

TLDR: Lumina-Image 2.0 achieves SOTA image generation across multiple tasks using a single transformer-based model instead of specialized architectures. Its novel sampling approach (MSIR) generates multiple candidates and refines the best ones, improving quality while reducing computational costs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/lumina-image-20-unified-efficient-image-generative). Paper [here](https://arxiv.org/abs/2503.21758).",2025-03-30 08:54:57,15,0,MachineLearning,post,"[R] Lumina-Image 2.0: Efficient Text-to-Image Generation via Unified Architecture and Progressive Training
Just came across Lumina-Image 2.0, which introduces a unified transformer-based architecture for multiple image generation tasks and a novel sampling technique they call Multiple Sampling with Iterative Refinement (MSIR).

The key idea is replacing specialized architectures with a single model that handles text-to-image generation, image editing, inpainting, and outpainting through a transformer that treats images as sequences of tokens (similar to how LLMs handle text).

Key technical points:
- **MSIR sampling**: Generates multiple candidate images simultaneously (8-32) then selectively refines the most promising ones, improving quality without increasing computation
- **Unified architecture**: Single model handles multiple tasks using task-specific embedding tokens
- **Parallel decoding with deep fusion**: Processes multiple tokens in parallel then fuses results, significantly speeding up inference
- **Results**: 4.11 FID on COCO dataset, outperforming previous SOTA while using 38% less compute for training
- **Scaling efficiency**: 8B parameter model shows substantial improvements over 3B version while maintaining fast inference

I think this approach represents an important shift in image generation architecture. Moving away from specialized diffusion models toward unified transformer-based approaches could significantly simplify deployment and maintenance of AI image systems. The MSIR technique is particularly interesting as it provides a clever way to improve sample quality without the computational penalty of naive approaches.

The 38% reduction in training computation is noteworthy given the increasing concerns about AI's environmental impact. If we can get better models with less compute, that's a win for both performance and sustainability.

I'm curious to see if this unified architecture approach can extend beyond images to efficiently handle video or 3D generation tasks. The paper suggests this direction might be viable.

TLDR: Lumina-Image 2.0 achieves SOTA image generation across multiple tasks using a single transformer-based model instead of specialized architectures. Its novel sampling approach (MSIR) generates multiple candidates and refines the best ones, improving quality while reducing computational costs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/lumina-image-20-unified-efficient-image-generative). Paper [here](https://arxiv.org/abs/2503.21758)."
1jnm8dj,[Discussion] Rethinking Advanced AI Benchmarks: Why Autonomous Homesteads Should Be a Real-World Testing Ground,"Good day Reddit Community,

I have spent a considerable amount of time working on AI projects like vector neural networks, that treat scalars as 2-D vectors, and spatial probability networks where vectors get dynamically routed across multitudes of nodes. I have been keeping up with our pursuit of more advanced and intelligent neural networks, and our approach toward Advanced AI. I hear about Advanced AI benchmarks that look similar to IQ tests, and that test the complexity of the mental model that AIs can build internally. Super-intelligent AIs are poised to tackle real-world problems, such as preventing aging and curing diseases. All of this is great, but most of it does not seem focused on basic human needs. It seems like jumping into the deep end of the pool before actually learning how to swim. They seem more focused on giving us what we desire than what we truly need deep down as a society.
Our society has been built on scarcity. It drives supply and demand and our economies. It can be a force for good, but at the same time, a force for inequality.

When we empower our AI models and AI agents to conquer our most difficult open problems, are they also solving the longest rooted ones, the ones that have been dug the deepest?
Are we focused on truly reducing scarcity and moving toward abundance? We have been conditioned to live in a scarcity economy for so long, are we just prolonging it by focusing on AI and AGI benchmarks that are ethereal and abstract? Or are we focused on first providing for our basic needs, then building off of that. Are we following the path of least resistance or following the best path?

We have open-source libraries where the distributed community can create better and more powerful AI models, but do we have an embodied GitHub, one focused on embodied AI that can attend to our physical needs?
Should we be focused on AGI that does work and physical labor, rather than one that relies on the human race to do the work and physical labor while AI is stuck in intellectual pursuits? Does it result in a race to the bottom, or a race to the top, for the well-being of the human race.

## The Case for Autonomous Homesteads

I envision autonomous, self-sustaining homesteads as testing grounds for AGI. Not just as another benchmark, but as a way to ground artificial intelligence in the real, physical needs of human beings. 
These homesteads should be decentralized, distributed, and open source.

Think about what this would require:

- Systems that can actually see and understand their environment through multiple senses
- Real physical control of things like water systems, energy management, and growing food
- The ability to plan for long-term changes, like weather and seasons
- Natural ways to communicate with humans about what's happening
- Learning to make safe decisions in an environment where mistakes have real consequences
- Adapting to constant change in messy, real-world conditions

This isn’t about creating another smart home or narrow automation system. It’s about developing embodied intelligence that can maintain a habitat, adapt to change, and collaborate with humans.

## How Would This Actually Work?

From a technical perspective, I imagine integrating several key components:

- Edge computing systems running multiple AI agents that work together to handle different aspects of the homestead
- Vision systems that can actually understand what they're seeing in the environment
- Language models that can translate between human needs and system actions
- Learning systems that share knowledge between different homesteads
- Robust ways to collect and use sensor data

Each homestead becomes a living testbed—a node in a distributed benchmark ecosystem, testing intelligence with respect to survival, sustainability, and sovereignty.
It's like a 'Survivor' for AI.

## Why This Matters for AGI Research

When I think about why this approach is important, several key points come to mind:

1. Instead of testing our AI systems on abstract problems, we'd be testing them against real physics, biology, and human needs
2. The physical world creates natural boundaries - you can't work around the fact that plants need water to grow
3. Success requires bringing together all the pieces - perception, planning, and action
4. Nature provides the ultimate testing ground - seasons change, things break down, new challenges constantly emerge
5. We'd be building systems that could actually help with food security, energy independence, and sustainable living
6. Safety constraints emerge naturally from working with real physical systems

## The Embodied GitHub (Open Infrastructure for All)

I believe we need something like a GitHub but for physical systems. Imagine:
- Open blueprints for building these homesteads
- Shareable AI systems for controlling different aspects
- Standard ways to connect sensors and systems
- Designs that anyone could reproduce and improve
- A community working together on both the software and hardware

This would help create a global movement of AI-aligned, physically grounded infrastructure development.

## The Real Challenges We Need to Solve

I see several key technical hurdles we need to overcome:
1. Making these systems work with limited computing resources
2. Bringing together data from many different sensors reliably
3. Planning for an uncertain future
4. Testing new approaches safely in the real world
5. Getting multiple AI systems to work together effectively

## A Starting Point

I think we could begin with something as simple as a robotic garden pod that manages its own irrigation, monitors plant health, utilizes solar power, and can communicate with humans about its activities. Even this small system would push our current capabilities in meaningful ways.

## Questions for Discussion

1. What existing open-source frameworks could serve as the base for this kind of project?
2. Are you working on (or aware of) similar efforts that combine AI, robotics, and sustainability?
3. How would you approach designing a first prototype of an autonomous homestead node?
4. How might we structure this as a shared AGI benchmark across research groups?

If our AGI can't grow food, clean water, or maintain shelter - can we really call it general intelligence? Maybe it's time our benchmarks reflected the world we actually want to build.",2025-03-30 22:48:26,0,0,MachineLearning,post,"[Discussion] Rethinking Advanced AI Benchmarks: Why Autonomous Homesteads Should Be a Real-World Testing Ground
Good day Reddit Community,

I have spent a considerable amount of time working on AI projects like vector neural networks, that treat scalars as 2-D vectors, and spatial probability networks where vectors get dynamically routed across multitudes of nodes. I have been keeping up with our pursuit of more advanced and intelligent neural networks, and our approach toward Advanced AI. I hear about Advanced AI benchmarks that look similar to IQ tests, and that test the complexity of the mental model that AIs can build internally. Super-intelligent AIs are poised to tackle real-world problems, such as preventing aging and curing diseases. All of this is great, but most of it does not seem focused on basic human needs. It seems like jumping into the deep end of the pool before actually learning how to swim. They seem more focused on giving us what we desire than what we truly need deep down as a society.
Our society has been built on scarcity. It drives supply and demand and our economies. It can be a force for good, but at the same time, a force for inequality.

When we empower our AI models and AI agents to conquer our most difficult open problems, are they also solving the longest rooted ones, the ones that have been dug the deepest?
Are we focused on truly reducing scarcity and moving toward abundance? We have been conditioned to live in a scarcity economy for so long, are we just prolonging it by focusing on AI and AGI benchmarks that are ethereal and abstract? Or are we focused on first providing for our basic needs, then building off of that. Are we following the path of least resistance or following the best path?

We have open-source libraries where the distributed community can create better and more powerful AI models, but do we have an embodied GitHub, one focused on embodied AI that can attend to our physical needs?
Should we be focused on AGI that does work and physical labor, rather than one that relies on the human race to do the work and physical labor while AI is stuck in intellectual pursuits? Does it result in a race to the bottom, or a race to the top, for the well-being of the human race.

## The Case for Autonomous Homesteads

I envision autonomous, self-sustaining homesteads as testing grounds for AGI. Not just as another benchmark, but as a way to ground artificial intelligence in the real, physical needs of human beings. 
These homesteads should be decentralized, distributed, and open source.

Think about what this would require:

- Systems that can actually see and understand their environment through multiple senses
- Real physical control of things like water systems, energy management, and growing food
- The ability to plan for long-term changes, like weather and seasons
- Natural ways to communicate with humans about what's happening
- Learning to make safe decisions in an environment where mistakes have real consequences
- Adapting to constant change in messy, real-world conditions

This isn’t about creating another smart home or narrow automation system. It’s about developing embodied intelligence that can maintain a habitat, adapt to change, and collaborate with humans.

## How Would This Actually Work?

From a technical perspective, I imagine integrating several key components:

- Edge computing systems running multiple AI agents that work together to handle different aspects of the homestead
- Vision systems that can actually understand what they're seeing in the environment
- Language models that can translate between human needs and system actions
- Learning systems that share knowledge between different homesteads
- Robust ways to collect and use sensor data

Each homestead becomes a living testbed—a node in a distributed benchmark ecosystem, testing intelligence with respect to survival, sustainability, and sovereignty.
It's like a 'Survivor' for AI.

## Why This Matters for AGI Research

When I think about why this approach is important, several key points come to mind:

1. Instead of testing our AI systems on abstract problems, we'd be testing them against real physics, biology, and human needs
2. The physical world creates natural boundaries - you can't work around the fact that plants need water to grow
3. Success requires bringing together all the pieces - perception, planning, and action
4. Nature provides the ultimate testing ground - seasons change, things break down, new challenges constantly emerge
5. We'd be building systems that could actually help with food security, energy independence, and sustainable living
6. Safety constraints emerge naturally from working with real physical systems

## The Embodied GitHub (Open Infrastructure for All)

I believe we need something like a GitHub but for physical systems. Imagine:
- Open blueprints for building these homesteads
- Shareable AI systems for controlling different aspects
- Standard ways to connect sensors and systems
- Designs that anyone could reproduce and improve
- A community working together on both the software and hardware

This would help create a global movement of AI-aligned, physically grounded infrastructure development.

## The Real Challenges We Need to Solve

I see several key technical hurdles we need to overcome:
1. Making these systems work with limited computing resources
2. Bringing together data from many different sensors reliably
3. Planning for an uncertain future
4. Testing new approaches safely in the real world
5. Getting multiple AI systems to work together effectively

## A Starting Point

I think we could begin with something as simple as a robotic garden pod that manages its own irrigation, monitors plant health, utilizes solar power, and can communicate with humans about its activities. Even this small system would push our current capabilities in meaningful ways.

## Questions for Discussion

1. What existing open-source frameworks could serve as the base for this kind of project?
2. Are you working on (or aware of) similar efforts that combine AI, robotics, and sustainability?
3. How would you approach designing a first prototype of an autonomous homestead node?
4. How might we structure this as a shared AGI benchmark across research groups?

If our AGI can't grow food, clean water, or maintain shelter - can we really call it general intelligence? Maybe it's time our benchmarks reflected the world we actually want to build."
1jnc4vq,[D] Minimising focal loss but log loss exceeds base rate,"Hey guys, I'm working on a model for churn prevention. The gist of it is this:

*Predict how likely somebody is to transact tomorrow given their last 30 days of behaviour. Plot a line of these next-day predictions over a 14-day time span. The gradient of this line is a measure of the risk of a customer churning.*

My company does not have a definition of churn - static markers like *customer has not transacted in the last 14 days* are too coarse.  The idea is to identify a negative shift in the latent representation of a user's engagement with the platform by proxy of their likelihood to transact over time.

The real distribution of data is 20:1 in favour of a user not transacting on any given day (\~120k total samples). So, naively guessing a 0.05% chance of transacting gives you a model with accuracy of 95% (how good right?...), log loss of \~1.6, undefined precision and 0 recall. So, not a useful model.

I am trying to train an LSTM. If I minimise binary log loss it converges to 0 straight away - as expected. If I minimise focal loss with a positive weight of \~10, I get \~90% accuracy, \~12% precision, \~50% recall and log loss of \~0.3. So the model learned something, but the probabilities are uncalibrated. I cannot get the log loss below the base rate of \~1.6... The difficult thing about this problem is there isn't a good way of being able to tell if this next-day prediction model suffices as a latent encoder of a customer's engagement.

I haven't tried negative subsampling yet as the data pipeline is more complex. Also, users will often have long periods of inactivity so there may often be no engagement for a large proportion of any given sequence (i.e. sample). I've considered condensing each sample to only include rows (i.e. days) on which a user was engaged and adding some indicator feature, *number\_of\_days\_since\_last\_engaged* to capture the temporal difference. Anyway, I'm a bit stuck atm so figured I'd reach out and see if anyone had any thoughts. Cheers",2025-03-30 15:14:12,2,1,MachineLearning,post,"[D] Minimising focal loss but log loss exceeds base rate
Hey guys, I'm working on a model for churn prevention. The gist of it is this:

*Predict how likely somebody is to transact tomorrow given their last 30 days of behaviour. Plot a line of these next-day predictions over a 14-day time span. The gradient of this line is a measure of the risk of a customer churning.*

My company does not have a definition of churn - static markers like *customer has not transacted in the last 14 days* are too coarse.  The idea is to identify a negative shift in the latent representation of a user's engagement with the platform by proxy of their likelihood to transact over time.

The real distribution of data is 20:1 in favour of a user not transacting on any given day (\~120k total samples). So, naively guessing a 0.05% chance of transacting gives you a model with accuracy of 95% (how good right?...), log loss of \~1.6, undefined precision and 0 recall. So, not a useful model.

I am trying to train an LSTM. If I minimise binary log loss it converges to 0 straight away - as expected. If I minimise focal loss with a positive weight of \~10, I get \~90% accuracy, \~12% precision, \~50% recall and log loss of \~0.3. So the model learned something, but the probabilities are uncalibrated. I cannot get the log loss below the base rate of \~1.6... The difficult thing about this problem is there isn't a good way of being able to tell if this next-day prediction model suffices as a latent encoder of a customer's engagement.

I haven't tried negative subsampling yet as the data pipeline is more complex. Also, users will often have long periods of inactivity so there may often be no engagement for a large proportion of any given sequence (i.e. sample). I've considered condensing each sample to only include rows (i.e. days) on which a user was engaged and adding some indicator feature, *number\_of\_days\_since\_last\_engaged* to capture the temporal difference. Anyway, I'm a bit stuck atm so figured I'd reach out and see if anyone had any thoughts. Cheers"
1jnfr0s,[R] GANs evaluation metrixs,"Hello guys, i am im the process of choosing my bachelors thesis. One idea i had was to focus on compering different methods of evaluating GANs. As a experiment i thought of artificially adding artefacts to generated images and then checking the impact, that different artefacts can have on different evaluation scores. Do you think that this idea makes sense and is appropriate for a bachelors thesis? If you see any issues and problems with this topic, please let me know. Thanks for help!",2025-03-30 18:06:11,0,3,MachineLearning,post,"[R] GANs evaluation metrixs
Hello guys, i am im the process of choosing my bachelors thesis. One idea i had was to focus on compering different methods of evaluating GANs. As a experiment i thought of artificially adding artefacts to generated images and then checking the impact, that different artefacts can have on different evaluation scores. Do you think that this idea makes sense and is appropriate for a bachelors thesis? If you see any issues and problems with this topic, please let me know. Thanks for help!"
1jn11wq,[N] [P] Transformer model made with PHP,"**New Release**

Rindow Neural Networks Version 2.2 has been released.

This release includes samples of transformer models.

We have published **a tutorial on creating transformer models** supported in the new version.

* [Neural Machine Translation with Transformer Models in PHP](https://rindow.github.io/neuralnetworks/tutorials/neural-machine-translation-with-transformer.html)

Rindow Neural Networks is a high-level neural network library for PHP.

It enables powerful machine learning in PHP.

* [Rindow Neural Networks](https://rindow.github.io/neuralnetworks/)

**Overview**

* Rindow Neural Networks is a high-level neural network library for PHP. It enables powerful machine learning in PHP.
* You can build machine learning models such as DNN, CNN, RNN, (multi-head) attention, etc.
* You can leverage your knowledge of Python and Keras.
* Popular computer vision and natural language processing samples are available.
* By calling high-speed calculation libraries, you can process data at speeds comparable to the CPU version of TensorFlow.
* No dedicated machine learning environment is required. It can run on an inexpensive laptop.
* NVIDIA GPU is not required. You can utilize the GPU of your laptop.

**What Rindow Neural Networks is not:**

* It is not an inference-only library.
* It is not a PHP binding for other machine learning frameworks.
* It is not a library for calling AI web services.",2025-03-30 01:58:42,11,14,MachineLearning,post,"[N] [P] Transformer model made with PHP
**New Release**

Rindow Neural Networks Version 2.2 has been released.

This release includes samples of transformer models.

We have published **a tutorial on creating transformer models** supported in the new version.

* [Neural Machine Translation with Transformer Models in PHP](https://rindow.github.io/neuralnetworks/tutorials/neural-machine-translation-with-transformer.html)

Rindow Neural Networks is a high-level neural network library for PHP.

It enables powerful machine learning in PHP.

* [Rindow Neural Networks](https://rindow.github.io/neuralnetworks/)

**Overview**

* Rindow Neural Networks is a high-level neural network library for PHP. It enables powerful machine learning in PHP.
* You can build machine learning models such as DNN, CNN, RNN, (multi-head) attention, etc.
* You can leverage your knowledge of Python and Keras.
* Popular computer vision and natural language processing samples are available.
* By calling high-speed calculation libraries, you can process data at speeds comparable to the CPU version of TensorFlow.
* No dedicated machine learning environment is required. It can run on an inexpensive laptop.
* NVIDIA GPU is not required. You can utilize the GPU of your laptop.

**What Rindow Neural Networks is not:**

* It is not an inference-only library.
* It is not a PHP binding for other machine learning frameworks.
* It is not a library for calling AI web services."
1jmhoq6,[R] Anthropic: On the Biology of a Large Language Model,"In this paper, we focus on applying attribution graphs to study a particular language model – Claude 3.5 Haiku, released in October 2024, which serves as Anthropic’s lightweight production model as of this writing. We investigate a wide range of phenomena. Many of these have been explored before (see [§ 16 Related Work](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#related-work)), but our methods are able to offer additional insight, in the context of a frontier model:

* [**Introductory Example: Multi-step Reasoning.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-tracing) We present a simple example where the model performs “two-hop” reasoning “in its head” to identify that “the capital of the state containing Dallas” is “Austin.” We can see and manipulate an internal step where the model represents “Texas”.
* [**Planning in Poems.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-poems) We discover that the model plans its outputs ahead of time when writing lines of poetry. Before beginning to write each line, the model identifies potential rhyming words that could appear at the end. These preselected rhyming options then shape how the model constructs the entire line.
* [**Multilingual Circuits.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-multilingual) We find the model uses a mixture of language-specific and abstract, language-independent circuits. The language-independent circuits are more prominent in Claude 3.5 Haiku than in a smaller, less capable model.
* [**Addition.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-addition) We highlight cases where the same addition circuitry generalizes between very different contexts.
* [**Medical** ](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-medical)[**Diagnoses**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-medical)**.** We show an example in which the model identifies candidate diagnoses based on reported symptoms, and uses these to inform follow-up questions about additional symptoms that could corroborate the diagnosis – all “in its head,” without writing down its steps.
* [**Entity Recognition and Hallucinations.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-hallucinations) We uncover circuit mechanisms that allow the model to distinguish between familiar and unfamiliar entities, which determine whether it elects to answer a factual question or profess ignorance. “Misfires” of this circuit can cause hallucinations.
* [**Refusal of Harmful Requests.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-refusals) We find evidence that the model constructs a general-purpose “harmful requests” feature during finetuning, aggregated from features representing *specific* harmful requests learned during pretraining.
* [**An Analysis of a Jailbreak.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-jailbreak) We investigate an attack which works by first tricking the model into starting to give dangerous instructions “without realizing it,” after which it continues to do so due to pressure to adhere to syntactic and grammatical rules.
* [**Chain-of-thought Faithfulness.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot) We explore the faithfulness of chain-of-thought reasoning to the model’s actual mechanisms. We are able to distinguish between cases where the model genuinely performs the steps it says it is performing, cases where it makes up its reasoning without regard for truth, and cases where it *works backwards* from a human-provided clue so that its “reasoning” will end up at the human-suggested answer.
* [**A Model with a Hidden Goal.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-misaligned) We also apply our method to a variant of the model that has been finetuned to pursue a secret goal: exploiting “bugs” in its training process. While the model avoids revealing its goal when asked, our method identifies mechanisms involved in pursuing the goal. Interestingly, these mechanisms are embedded within the model’s representation of its “Assistant” persona.

  
The above excerpt is from a research by Anthropic. Super interesting stuff, basically a step closer to interpretability that doesn’t just treat the model as a black box. If you're into model interpretability, safety, or inner monologue tracing. Would love to hear thoughts. 

  
Paper link: [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)",2025-03-29 09:18:04,218,56,MachineLearning,post,"[R] Anthropic: On the Biology of a Large Language Model
In this paper, we focus on applying attribution graphs to study a particular language model – Claude 3.5 Haiku, released in October 2024, which serves as Anthropic’s lightweight production model as of this writing. We investigate a wide range of phenomena. Many of these have been explored before (see [§ 16 Related Work](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#related-work)), but our methods are able to offer additional insight, in the context of a frontier model:

* [**Introductory Example: Multi-step Reasoning.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-tracing) We present a simple example where the model performs “two-hop” reasoning “in its head” to identify that “the capital of the state containing Dallas” is “Austin.” We can see and manipulate an internal step where the model represents “Texas”.
* [**Planning in Poems.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-poems) We discover that the model plans its outputs ahead of time when writing lines of poetry. Before beginning to write each line, the model identifies potential rhyming words that could appear at the end. These preselected rhyming options then shape how the model constructs the entire line.
* [**Multilingual Circuits.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-multilingual) We find the model uses a mixture of language-specific and abstract, language-independent circuits. The language-independent circuits are more prominent in Claude 3.5 Haiku than in a smaller, less capable model.
* [**Addition.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-addition) We highlight cases where the same addition circuitry generalizes between very different contexts.
* [**Medical** ](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-medical)[**Diagnoses**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-medical)**.** We show an example in which the model identifies candidate diagnoses based on reported symptoms, and uses these to inform follow-up questions about additional symptoms that could corroborate the diagnosis – all “in its head,” without writing down its steps.
* [**Entity Recognition and Hallucinations.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-hallucinations) We uncover circuit mechanisms that allow the model to distinguish between familiar and unfamiliar entities, which determine whether it elects to answer a factual question or profess ignorance. “Misfires” of this circuit can cause hallucinations.
* [**Refusal of Harmful Requests.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-refusals) We find evidence that the model constructs a general-purpose “harmful requests” feature during finetuning, aggregated from features representing *specific* harmful requests learned during pretraining.
* [**An Analysis of a Jailbreak.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-jailbreak) We investigate an attack which works by first tricking the model into starting to give dangerous instructions “without realizing it,” after which it continues to do so due to pressure to adhere to syntactic and grammatical rules.
* [**Chain-of-thought Faithfulness.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-cot) We explore the faithfulness of chain-of-thought reasoning to the model’s actual mechanisms. We are able to distinguish between cases where the model genuinely performs the steps it says it is performing, cases where it makes up its reasoning without regard for truth, and cases where it *works backwards* from a human-provided clue so that its “reasoning” will end up at the human-suggested answer.
* [**A Model with a Hidden Goal.**](https://transformer-circuits.pub/2025/attribution-graphs/biology.html#dives-misaligned) We also apply our method to a variant of the model that has been finetuned to pursue a secret goal: exploiting “bugs” in its training process. While the model avoids revealing its goal when asked, our method identifies mechanisms involved in pursuing the goal. Interestingly, these mechanisms are embedded within the model’s representation of its “Assistant” persona.

  
The above excerpt is from a research by Anthropic. Super interesting stuff, basically a step closer to interpretability that doesn’t just treat the model as a black box. If you're into model interpretability, safety, or inner monologue tracing. Would love to hear thoughts. 

  
Paper link: [On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html)"
1jnalfy,[R] FrigoRelu - Straight-through ReLU,"    from torch import Tensor
    import torch
    import torch.nn as nn

    class FrigoRelu (nn.Module):

        def __init__ (self, alpha = 0.1):
            super(FrigoRelu, self).__init__()
            self.alpha = alpha

        def forward (self, x: Tensor) -> Tensor:
            hard = torch.relu(x.detach())
            soft = torch.where(x >= 0, x, x * self.alpha)
            return hard - soft.detach() + soft

I have figured out I can change ReLU in a similar manner to straight-through estimators. Forward pass proceeds as usual with hard ReLU, whereas the backward pass behaves like LeakyReLU for gradient propagation. It is a dogshit simple idea and somehow the existing literature missed it. I have found only one article where they use the same trick except with GELU instead of LeakyReLU: https://www.biorxiv.org/content/10.1101/2024.08.22.609123v2

I had an earlier attempt at MNIST which had issues with ReLU, likely dead convolutions that hindered learning and accuracy. This was enabled by too high initial learning rate (1e-0), and too few parameters which was deliberate (300). The model produced 54.1%, 32.1% (canceled), 45.3%, 55.8%, and 95.5% accuracies after 100k iterations. This model was the primary reason I transitioned to SeLU + AvgPool2d, and then to other architectures that did not have issues with learning and accuracy.

So now I brought back that old model, and plugged in FrigoRelu with alpha=0.1 parameter. The end result was 91.0%, 89.1%, 89.1%, and 90.9% with only 5k iterations. Better, faster, and more stable learning with higher accuracies on average, so it is clear improvement compared to the old model. For comparison the SELU model produced 93.7%, 92.7%, 94.9% and 95.0% accuracies but with 100k iterations. I am going to run 4x100k iterations on FrigoReLU so I can compare them on an even playing field.

Until then enjoy FrigoRelu, and please provide some feedback if you do.",2025-03-30 13:39:48,1,0,MachineLearning,post,"[R] FrigoRelu - Straight-through ReLU
    from torch import Tensor
    import torch
    import torch.nn as nn

    class FrigoRelu (nn.Module):

        def __init__ (self, alpha = 0.1):
            super(FrigoRelu, self).__init__()
            self.alpha = alpha

        def forward (self, x: Tensor) -> Tensor:
            hard = torch.relu(x.detach())
            soft = torch.where(x >= 0, x, x * self.alpha)
            return hard - soft.detach() + soft

I have figured out I can change ReLU in a similar manner to straight-through estimators. Forward pass proceeds as usual with hard ReLU, whereas the backward pass behaves like LeakyReLU for gradient propagation. It is a dogshit simple idea and somehow the existing literature missed it. I have found only one article where they use the same trick except with GELU instead of LeakyReLU: https://www.biorxiv.org/content/10.1101/2024.08.22.609123v2

I had an earlier attempt at MNIST which had issues with ReLU, likely dead convolutions that hindered learning and accuracy. This was enabled by too high initial learning rate (1e-0), and too few parameters which was deliberate (300). The model produced 54.1%, 32.1% (canceled), 45.3%, 55.8%, and 95.5% accuracies after 100k iterations. This model was the primary reason I transitioned to SeLU + AvgPool2d, and then to other architectures that did not have issues with learning and accuracy.

So now I brought back that old model, and plugged in FrigoRelu with alpha=0.1 parameter. The end result was 91.0%, 89.1%, 89.1%, and 90.9% with only 5k iterations. Better, faster, and more stable learning with higher accuracies on average, so it is clear improvement compared to the old model. For comparison the SELU model produced 93.7%, 92.7%, 94.9% and 95.0% accuracies but with 100k iterations. I am going to run 4x100k iterations on FrigoReLU so I can compare them on an even playing field.

Until then enjoy FrigoRelu, and please provide some feedback if you do."
1jmjstd,[R] DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products,"[https://openreview.net/forum?id=nvb60szj5C](https://openreview.net/forum?id=nvb60szj5C)

*Code:* [https://x.com/julien\_siems/status/1909487370656764208](https://x.com/julien_siems/status/1909487370656764208)

*Twitter / X:* [https://x.com/julien\_siems/status/1905628609714286687](https://x.com/julien_siems/status/1905628609714286687)

*Authors:* Julien Siems\*, Timur Carstensen\*, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi\* (\*equal contribution)

*Abstract:* Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple (nh) steps per token. This naturally leads to diagonal plus rank-state-transition matrices, formed as products of nh generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet by proving that it can solve dihedral group word problems in just two layers.

https://preview.redd.it/k4mq9js81mre1.png?width=2792&format=png&auto=webp&s=b0e42d71fee09cc6867e667f72b80fc24772ba3c",2025-03-29 11:59:23,19,0,MachineLearning,post,"[R] DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products
[https://openreview.net/forum?id=nvb60szj5C](https://openreview.net/forum?id=nvb60szj5C)

*Code:* [https://x.com/julien\_siems/status/1909487370656764208](https://x.com/julien_siems/status/1909487370656764208)

*Twitter / X:* [https://x.com/julien\_siems/status/1905628609714286687](https://x.com/julien_siems/status/1905628609714286687)

*Authors:* Julien Siems\*, Timur Carstensen\*, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi\* (\*equal contribution)

*Abstract:* Linear Recurrent Neural Networks (linear RNNs) have emerged as competitive alternatives to Transformers for sequence modeling, offering efficient training and linear-time inference. However, existing architectures face a fundamental trade-off between expressivity and efficiency, dictated by the structure of their state-transition matrices. While diagonal matrices used in architectures like Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limited expressivity. To address this, recent architectures such as (Gated) DeltaNet and RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneous token-channel mixing, which overcomes some expressivity limitations with only a slight decrease in training efficiency. Building on the interpretation of DeltaNet's recurrence as performing one step of online gradient descent per token on an associative recall loss, we introduce DeltaProduct, which instead takes multiple (nh) steps per token. This naturally leads to diagonal plus rank-state-transition matrices, formed as products of nh generalized Householder transformations, providing a tunable mechanism to balance expressivity and efficiency and a stable recurrence. Through extensive experiments, we demonstrate that DeltaProduct achieves superior state-tracking and language modeling capabilities while exhibiting significantly improved length extrapolation compared to DeltaNet. Additionally, we also strengthen the theoretical foundation of DeltaNet by proving that it can solve dihedral group word problems in just two layers.

https://preview.redd.it/k4mq9js81mre1.png?width=2792&format=png&auto=webp&s=b0e42d71fee09cc6867e667f72b80fc24772ba3c"
1jmlko7,"[D] What is your cloud setup specs, and how did you setup the environment?","Hi there! 

I am planning to setup a cloud environment to run models for research. I have beeb using local GPUs for a while for small pojects, but I would like to at least practice with cloud infrastructure, and I am currently interested in using Google TPU. I would like to know is there any better providers, and if anyone here is using cloud services, how did they get started and set up the environment? I would appreciate tutorials on getting started with setting up cloud VMs, as I already know there are quite a lot of online websites for running notebook style environments but I am more interested in using the whole machine with SSH. 
Thank you, and have a great day everyone! ",2025-03-29 13:50:22,8,9,MachineLearning,post,"[D] What is your cloud setup specs, and how did you setup the environment?
Hi there! 

I am planning to setup a cloud environment to run models for research. I have beeb using local GPUs for a while for small pojects, but I would like to at least practice with cloud infrastructure, and I am currently interested in using Google TPU. I would like to know is there any better providers, and if anyone here is using cloud services, how did they get started and set up the environment? I would appreciate tutorials on getting started with setting up cloud VMs, as I already know there are quite a lot of online websites for running notebook style environments but I am more interested in using the whole machine with SSH. 
Thank you, and have a great day everyone! "
1jmgv3r,[R] Enhancing GUI Agent Reasoning Through Rule-Based Reinforcement Learning,"I've been exploring UI-R1, a new approach that combines rule-based reinforcement learning with large language models to improve GUI agents. The key innovation here is using reinforcement learning to help these agents adapt and learn from their mistakes when navigating interfaces, rather than relying solely on fixed patterns.

**Technical approach:**
* Integrates a specialized R1 reinforcement learning system with LLMs for GUI navigation
* Creates a perception module that processes interface elements, an action prediction module, and a rule-based RL system
* Uses contrastive learning to differentiate between effective and ineffective actions
* Implements a ""self-correction"" mechanism that generalizes lessons from errors to similar scenarios
* Maintains a rule database that prioritizes actions that succeeded in similar contexts

**Key results:**
* 17.85% performance improvement over baseline GUI action prediction models
* 8.47% higher performance on complex multi-step tasks
* More effective learning from negative feedback (mistakes)
* Reduced need for extensive training data
* Superior adaptation to previously unseen interfaces
* Tested on the Mind2Web benchmark across various website tasks

I think this approach could fundamentally change how we build AI assistants that interact with digital interfaces. The ability to learn from mistakes and adapt to new interfaces addresses one of the major limitations in current GUI agents. This could lead to more robust automated testing tools, better accessibility solutions for users with disabilities, and more capable digital assistants that can handle unfamiliar websites or applications with minimal human intervention.

What's particularly interesting is how they've streamlined the reinforcement learning approach to be more efficient than traditional RL methods. The rule-based system means improvements can happen without the computational expense typically associated with RL training, which makes this more practical for real-world deployment.

**TLDR:** UI-R1 combines LLMs with rule-based reinforcement learning to create GUI agents that learn from their mistakes and adapt to new interfaces, showing significant performance improvements over baseline models across various web navigation tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/ui-r1-enhancing-action-prediction-gui-agents). Paper [here](https://arxiv.org/abs/2503.21620).",2025-03-29 08:14:33,12,1,MachineLearning,post,"[R] Enhancing GUI Agent Reasoning Through Rule-Based Reinforcement Learning
I've been exploring UI-R1, a new approach that combines rule-based reinforcement learning with large language models to improve GUI agents. The key innovation here is using reinforcement learning to help these agents adapt and learn from their mistakes when navigating interfaces, rather than relying solely on fixed patterns.

**Technical approach:**
* Integrates a specialized R1 reinforcement learning system with LLMs for GUI navigation
* Creates a perception module that processes interface elements, an action prediction module, and a rule-based RL system
* Uses contrastive learning to differentiate between effective and ineffective actions
* Implements a ""self-correction"" mechanism that generalizes lessons from errors to similar scenarios
* Maintains a rule database that prioritizes actions that succeeded in similar contexts

**Key results:**
* 17.85% performance improvement over baseline GUI action prediction models
* 8.47% higher performance on complex multi-step tasks
* More effective learning from negative feedback (mistakes)
* Reduced need for extensive training data
* Superior adaptation to previously unseen interfaces
* Tested on the Mind2Web benchmark across various website tasks

I think this approach could fundamentally change how we build AI assistants that interact with digital interfaces. The ability to learn from mistakes and adapt to new interfaces addresses one of the major limitations in current GUI agents. This could lead to more robust automated testing tools, better accessibility solutions for users with disabilities, and more capable digital assistants that can handle unfamiliar websites or applications with minimal human intervention.

What's particularly interesting is how they've streamlined the reinforcement learning approach to be more efficient than traditional RL methods. The rule-based system means improvements can happen without the computational expense typically associated with RL training, which makes this more practical for real-world deployment.

**TLDR:** UI-R1 combines LLMs with rule-based reinforcement learning to create GUI agents that learn from their mistakes and adapt to new interfaces, showing significant performance improvements over baseline models across various web navigation tasks.

[Full summary is here](https://aimodels.fyi/papers/arxiv/ui-r1-enhancing-action-prediction-gui-agents). Paper [here](https://arxiv.org/abs/2503.21620)."
1jmsjfx,[R] Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification on health datasets,,2025-03-29 19:16:38,0,2,MachineLearning,post,"[R] Synergistic eigenanalysis of covariance and Hessian matrices for enhanced binary classification on health datasets
"
1jmf17a,[D] Difficulty understanding how DPO is different in VLMs!,"Hi, I recently tried to learn about DPO on Visual Language Models and there’s just not enough resources to help me understand the difference in implementation. I see we are using the image embeddings but anyway using alignment only in language component which boils it down to doing the same thing in LLMs. If there is no vision guidance, then how will it learn vision cues to new image and question while answering it post preference alignment- it might generate text in a better way but where are we guaranteed that it will give visually grounded outputs as well if the language component is only used in DPO. 
Anyone who has tried this- can you please educate me on what I am missing out here?",2025-03-29 06:04:57,7,4,MachineLearning,post,"[D] Difficulty understanding how DPO is different in VLMs!
Hi, I recently tried to learn about DPO on Visual Language Models and there’s just not enough resources to help me understand the difference in implementation. I see we are using the image embeddings but anyway using alignment only in language component which boils it down to doing the same thing in LLMs. If there is no vision guidance, then how will it learn vision cues to new image and question while answering it post preference alignment- it might generate text in a better way but where are we guaranteed that it will give visually grounded outputs as well if the language component is only used in DPO. 
Anyone who has tried this- can you please educate me on what I am missing out here?"
1jmfg7h,[D] General questions regarding rebuttal phase (ACL ARR Feb 2025),"Hi all, it's my second time submitting to ACL-related conference, but I am still pretty confused about the rebuttal phase.

I recognize that we could not really modify the original manuscript, there's simply no such option. If there are some suggested changes, do we just say that we acknowledge them, and we will make such changes (if we agree those suggestions) in the revised version? Or, you guys actually revise the whole thing and place it in the response? The amount of time needed will be substantially different if we actually rewrite the whole thing.

This might be a silly question, but I want know how detailed we should be in the response.",2025-03-29 06:32:48,4,6,MachineLearning,post,"[D] General questions regarding rebuttal phase (ACL ARR Feb 2025)
Hi all, it's my second time submitting to ACL-related conference, but I am still pretty confused about the rebuttal phase.

I recognize that we could not really modify the original manuscript, there's simply no such option. If there are some suggested changes, do we just say that we acknowledge them, and we will make such changes (if we agree those suggestions) in the revised version? Or, you guys actually revise the whole thing and place it in the response? The amount of time needed will be substantially different if we actually rewrite the whole thing.

This might be a silly question, but I want know how detailed we should be in the response."
1jlt27q,[D] How Do You Make Your Published Plots Look So Good?,"I'm noticing that some of the graphics and plots for the papers I am reviewing look *really* good. How do you make them look so good? Are you using any special python libraries that I don't know about? I know some of you are using Adobe Illustrator and going over the plots/figures, but is there anything else I'm missing?",2025-03-28 12:40:42,116,35,MachineLearning,post,"[D] How Do You Make Your Published Plots Look So Good?
I'm noticing that some of the graphics and plots for the papers I am reviewing look *really* good. How do you make them look so good? Are you using any special python libraries that I don't know about? I know some of you are using Adobe Illustrator and going over the plots/figures, but is there anything else I'm missing?"
1jm050g,[D] Do you think that self-distillation really works?,"The gains from self-distillation in image classification problems have not been substantial, as published in empirical papers. Mostly they get at max 1% improvement in test accuracy, with the usual order being 0.2-0.5%. Is there a strong reason to believe it really works, other than a ""dark matter"" fairytale?",2025-03-28 18:11:57,17,13,MachineLearning,post,"[D] Do you think that self-distillation really works?
The gains from self-distillation in image classification problems have not been substantial, as published in empirical papers. Mostly they get at max 1% improvement in test accuracy, with the usual order being 0.2-0.5%. Is there a strong reason to believe it really works, other than a ""dark matter"" fairytale?"
